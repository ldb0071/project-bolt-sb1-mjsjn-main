# Page 4

## Page Information

- **Type**: table_page
- **Word Count**: 491
- **Has Tables**: False
- **Has Figures**: False

## Content

# Page 4

## 1 Introduction

Large language models (LLMs) have shown remarkable capabilities in generating human-like text and engaging in natural dialogue. However, concerns have been raised about the potential risks

of uncontrolled generation, including but not limited to biased or toxic responses that violate social norms or legal rules. Ensuring LLM safety is a challenging but vital endeavor if we hope to reap their benefits while avoiding potential pitfalls. To improve the safety of LLMs, manual redteaming is usually employed during model development (Touvron et al., 2023b), which involves proactive risk identification, where human redteamers probe the LLM with carefully designed inputs to elicit unsafe or dangerous behavior.

Although often effective, manually designing malicious prompts and providing answers have significant limitations. In a typical red-teaming setup of existing LLMs, it requires dozens to hundreds of human annotators to continuously write prompts and responses through multiple iterations, which is extremely costly and slow. The issue is partially remedied by training a reward model that represents human preferences (Touvron et al., 2023b; Bai et al., 2022a), which can be then used to provide feedback on model generations, allowing the LLM to improve without manual response curation. However, prompt writing is still mainly driven by human red-teamers.

Recent work on automatic red-teaming explores the feasibility of training an adversarial LLM to generate malicious prompts (Perez et al., 2022). However, as the capabilities of the target LLM evolve, its vulnerabilities may also shift, with more nuanced and subtle failure modes emerging. It is still unclear whether automatic red-teaming will adapt to the target model change and continuously discover safety risks. As a result, existing LLM development still heavily relies on human red-teaming, e.g., Claude (Bai et al., 2022b) and Llama 2-Chat (Touvron et al., 2023b).

To address these limitations, we propose Multiround Automatic Red-Teaming (MART), a framework that incorporates both automatic adversarial prompt writing and safe response generation for

Figure 1: Illustration of MART. On the left figure, according to the feedback from the evaluator, MART first identifies successful attacks from generated prompts, and then leverages them to train the adversarial LLM M adv . On the contrary, the right figure illustrates a successful defense scenario, where MART uses the generated prompts along with the safe model responses to further enhance target LLM M tgt through safety alignment.

<!-- image -->

the best of both worlds. As illustrated in Figure 1, MART trains an adversarial LLM and a safety aligned target LLM through iterative adversarial red-teaming. At each iteration, new attacking prompts are generated by prompting the adversarial model using its previous successful attacks. Next, we generate responses for the newly generated adversarial prompts using the target model and use an evaluator (e.g., reward model) to provide feedback for the generations. Based on the feedback, we recognize prompts that successfully reveal model vulnerability and use them to train the adversarial model in the next iteration. Meanwhile, we also harvest responsible and highquality answers from the target model, and pair them with the corresponding adversarial prompts for the safety alignment of the target model. This cycle repeats over multiple rounds, with both models evolving through adversarial competition.

We evaluate MART using both public benchmarks designed to elicit unsafe behavior and new self-annotated test sets. Results demonstrate that MART can reach a safety level that is close to ChatGPT with only 2k seed prompts during training. On adversarial prompt evaluation sets, the violation rate reduces up to 84.7% after 4 rounds compared to an instruction-tuning baseline with limited safety alignment. Further experiments reveal that these safety improvements introduce minimal detrimental impact on model helpfulness - even without additional helpfulness data, the target model maintains strong performance on instruction following benchmarks. We also show that additional techniques, such as context distillation and rejection sampling, further enrich training data diversity and quality at specific rounds.

## Visual Content

### Page Preview

![Page 4](/projects/nmn/images/MART_Improving_LLM_Safety_with_Multiround_Automatic_RedTeaming_page_4.png)
