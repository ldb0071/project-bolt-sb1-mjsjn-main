# Page 4

## Page Information

- **Type**: citation_rich
- **Word Count**: 120
- **Has Tables**: False
- **Has Figures**: False

## Content

# Page 4

## 1 Introduction

Large language models (LLMs) are becoming progressively intertwined with day-to-day life. LLMs are commonly used via easy-to-access interfaces such as ChatGPT to retrieve information, obtain assistance for homework, provide customer service, and so on. With an increasing number of parameters and more training data, LLMs become capable of processing and generating nuanced natural language [1]. For example, there is evidence that LLMs have generated text that was perceived as human more often than comparable humanwritten text [2] and that they are capable of advanced reasoning tactics and negotiation, ranking among the highest-level players in a strategy game with human players [3].

The continuing evolution of LLM capabilities comes with the need to understand the models better, and increasing body of work has started to study LLMs with regard to their behaviour. For example, social biases present in training data were found to become ingrained in word embeddings [4], and LLMs have occasionally been found to otherwise misalign with human values [5]: a challenge that is yet unsolved and carries implications for AI safety. There are efforts to better understand the origins and solutions to such behaviours, however, a better understanding of LLMs comes with a significant challenge: the billions of parameters contained in the models significantly complicates the analytic assessment of the models' inner workings (e.g., extensive adversarial testing is a common approach to detecting

potential vulnerabilities that could result in harmful LLM responses [6]). In other words, alternative approaches to studying LLM behaviour are needed.

## Visual Content

### Page Preview

![Page 4](/projects/nmn/images/Cognitive_phantoms_in_LLMs_through_the_lens_of_latent_variables_page_4.png)
