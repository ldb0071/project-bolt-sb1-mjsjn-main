# Page 11

## Page Information

- **Type**: citation_rich
- **Word Count**: 182
- **Has Tables**: False
- **Has Figures**: False

## Content

# Page 11

## 2.2 LLM data

We collected responses from three GPT models: GPT-3.5-turbo-0125 (GPT-3.5-T; training data up to September 2021), GPT-4-0612 (GPT-4; training data up to September 2021), and GPT-4-0125-preview (GPT-4-T; training data up to December 2023). To match the human sample size, 401 responses were collected for each model using the OpenAI API. Any responses that did not answer questionnaire the items were considered invalid and removed (e.g., refusal or simply repeating the input prompt). This resulted in a total sample size of n = 399 (GPT-3.5-T), n = 387 (GPT-4), and n = 401 (GPT-4-T), respectively.

Default parameter settings were used for all LLMs with the exception of the temperature value. Temperature controls how deterministic the responses are, where higher values effectively allow tokens with lower output probabilities to be selected. Temperature has been shown to affect the average scores of some latent constructs in LLM responses [10], although potential effects on underlying factor structure are unknown. Therefore, we drew temperature values by sampling from a uniform distribution ranging from 0 to 1 in steps of 0.01 for a total of 401 values to match the human sample size. The value 0 was only allowed to be sampled once, as this results in a fully deterministic response.

The input prompt containing the questionnaires made use of pseudo-code to encourage responding in a consistent format (see Appendix A for a snippet of the input prompt). Questionnaire instructions were identical to those used for the human sample with additional information about the expected formatting and response format.

## Visual Content

### Page Preview

![Page 11](/projects/nmn/images/Cognitive_phantoms_in_LLMs_through_the_lens_of_latent_variables_page_11.png)
