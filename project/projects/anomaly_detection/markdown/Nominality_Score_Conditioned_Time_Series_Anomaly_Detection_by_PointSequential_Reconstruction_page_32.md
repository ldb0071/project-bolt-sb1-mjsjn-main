## F Model and Parameter Selection Heuristics

We believe that the most important idea in model selection is to achieve a balance between effectively utilizing both point and sequence-based models and carefully selecting the appropriate parameters. As an extreme example, we optimize our algorithm on the Mackey-Glass anomaly benchmark (MGAB) [47], which is a univariate time series dataset, and find that sequence-based models can significantly outperform point-based models when using their reconstruction errors as A ( · ) . As in Fig. 6, we see that M seq can correctly identify the anomalies, whereas M pt did not learn at all. This is reasonable, as point-based models only consider a single time point. In general, the lower the dimensionality of the dataset, the harder it gets for a point-based model to learn statistically meaningful representations. Moreover, looking at the high-dimensional datasets reported in Table 3, the induced anomaly score remains important for some datasets: For the MSL dataset, F1 ∗ improves 0.1 compared to only using point-based reconstruction. In Fig. 7b, when using a soft gate function, F1 ∗ improves 0.047 compared to using the point-based anomaly score. Since the amount of improvement depends on the statistical structure of the test data, it is still useful to consider using both M seq and M pt in general.

The difficulty in choosing the parameters for unsupervised time series anomaly detection stems from the absence of anomalies in the training dataset. The selection of soft or hard gate functions, the value for d , and the ratio for θ N largely depend on how we presume the anomaly will occur based on domain knowledge. For instance, d controls the distance that anomaly scores may propagate. This value should be higher if we presume the average anomaly length is long and vice versa. If

Figure 6: (a) best F1 scores and (b) AUCs using point and sequence-based models as the anomaly score on the Mackey-Glass anomaly benchmark.

<!-- image -->

the anomaly is expected to occur abruptly and significantly, there would be a clear gap between the distribution of nominality scores for normal and anomaly data (Fig. 8a). In this case, a hard gate function should be chosen as it allows anomaly scores to propagate through time points without reduction, as long as an anomaly time point has a nominality score lower than θ N . Conversely, if the anomaly occurs progressively, the distribution of nominality scores is likely to overlap (Fig. 8b). Here, a soft gate function will be more appropriate to prevent excessive accumulation of anomaly scores on normal time points, reducing false-positives. A dataset could contain both abrupt and progressive anomalies. However, based on Table 3, it is evident that using a soft gate function generally yields better performance compared to a hard gate function. This suggests that the distribution of nominality scores is predominantly overlapped, which is also evident in Fig. 4 and Fig. 7a.