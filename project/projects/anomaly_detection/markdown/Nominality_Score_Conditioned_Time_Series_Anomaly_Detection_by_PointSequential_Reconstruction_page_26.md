## C Data Preprocessing and Training Details

Table 4 shows the hyperparameters used for implementing NPSR on the experimented datasets. To ensure fair comparison, the same preprocessing method is applied to all algorithms for the same dataset. The search for hyperparameters is done manually, starting from some reasonable value (e.g. a learning rate of 10 -4 ). The authors believe that there is still room for improvement by fine-tuning these hyperparameters. To speed up training, we load all training inputs and outputs, and testing inputs onto the GPU before training. We use a local GPU, which can be either GeForce RTX 3070 (8GB), 3080 (12GB) or 3090 (24GB). For an individual experiment using a single dataset and training method, the training time ranges from approximately 2 minutes to 12 hours. For single-entity datasets and multi-entity datasets that use the combined training method, we run the experiments for at least 3 times and confirm that the results are stable given different random seeds. For multi-entity datasets with entities trained individually, the results are averaged across all entities. Generally, datasets with single entities train faster than those with multiple entities. There are some additional remarks regarding the preprocessing of the datasets.

For SWaT, we use SWaT\_Dataset\_Attack\_v0.csv and SWaT\_Dataset\_Normal\_v1.csv from the folder SWaT.A1 & A2\_Dec 2015 (manually converted from *.xlsx ). We corrected some original flaws in the dataset (e.g. redundant blank spaces in some labels), and set the 5th and 10th columns to all 0. For WADI, we use the 2017 year dataset. Columns with excessive NaNs (more than half of the entire length) are deleted. Other NaNs are forward-filled. After deleting all the columns with excessive NaNs, the 86th column is further set to all 0. For PSM, we forward-fill all NaNs. For MSL and SMD, two additional blank channels are added to make the number of channels divisible by the number of heads. For trimSyn, we separated the dataset into training ( t ∈ { 0 , ..., 9999 } ) and testing ( t ∈ { 10000 , ..., 19999 } ) data (cf. [24]). For the testing data, we extracted segments within the time intervals t ∈ { 10000 , ..., 11719 }∪{ 11900 , ..., 12849 }∪{ 14630 , ..., 17699 }∪{ 17880 , ..., 18529 }∪ { 18710 , ..., 19999 } and concatenated them to form a single-entity test dataset. We also inserted additional segment IDs using one-hot encoding to enable segment identification. This process resulted in a test dataset with a single anomaly segment ( t ∈ { 12670 , ..., 12849 } ) and a corresponding anomaly rate of 2 . 34% . In the case of the training data, we duplicated the original segment five times, concatenated them, and added the necessary segment IDs.

Table 4: Implementation details. (c) stands for the combined training method (cf. section 4.2)