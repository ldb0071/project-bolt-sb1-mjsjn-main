Page 18

In this section, we introduce the details of the four M-LLMs investigated in the study. We focus on resolving the performance discrepancy between those models and their text-only counterparts. The purpose is to ensure reproducibility and justify direct comparisons between visual and text inputs.

GPT-4o mini Launched in July 2024, GPT-4o mini (OpenAI, 2024) is a cost-efficient, smaller version of GPT-4o, designed to replace GPT-3.5, exceeding its performance at a lower cost. It excels in mathematical and coding tasks, achieving 87.0% on MGSM (measuring math reasoning) and 87.2% on HumanEval (measuring coding performance). The model features a 128,000-token context window, knowledge up to October 2023, and support for text and vision in the API.

The architecture of GPT-4o is not disclosed. The GPT-4o-mini variant we used in this work is gpt-4o-2024-08-06 . Since we are sending text queries with and without images to GPT-4o, an important thing to consider is that by adding the image, the language part of the model does not degrade, or the OpenAI reverse proxy does not send the query to a different backend. To the best of our knowledge, there is no prior validation study on this. We perform experiments by adding a small, white, 10 x 10 pixels image to the text queries and run the 5-shot CoT MMLU-Pro (Wang et al., 2024). The score without image is 61.54 . The score with the image is 61.49 . The scores do not reject the hypothesis that the same model is behind different modalities.

Qwen-VL-Chat Developed by Alibaba Cloud, Qwen-VL-Chat (Bai et al., 2023) stands out as a high-performing large vision language model designed for text-image dialogue tasks. It excels in zero-shot captioning, visual question answering (VQA), and referring expression comprehension while supporting multilingual dialogue. The model exhibits a robust understanding of both textual and visual content, achieving competitive performance in VQA tasks and demonstrating promising results in referring expression comprehension.

Qwen-VL-Chat is open-sourced. We use the model last updated on Jan 25, 2024. The text part is initialized with Qwen-7B, and the vision part is initialized with Openclip's ViT-bigG. We note that the model's MMLU performance is a lot worse than the text-only variant. Qwen-7B has an MMLU score of 58.2 , while Qwen-VL-Chat has an MMLU score of 50.7 . However, it is explained in the paper that the Qwen-7B used for initializing the text part is an intermediate version, whose MMLU score is 49.9 . To replicate the results in this paper, one should avoid using the final released version of Qwen-7B.

Gemini-1.5-Flash Gemini-1.5-Flash is the fastest model in the Gemini family, optimized for highvolume, high-frequency tasks and offering a cost-effective solution with a long context window. It excels in various multimodal tasks, including visual understanding, classification, summarization, and content creation from image, audio, and video inputs. It achieves comparable quality to other Gemini Pro models at a significantly reduced cost.

Gemini-1.5-Flash is proprietary. We use the model variant gemini-1.5-flash-002 . Similar to GPT-4o, we validate the model by MMLU-Pro with a trivial image. The score without image is 59.12 . The score with the image is 59.23 . The scores do not reject the hypothesis that the same model is behind different prompts.

Intern-VLM InternVL 2 (Chen et al., 2024) is an open-source multimodal large language model (MLLM) designed to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. It features a strong vision encoder using InternViT with continuous learning, dynamic high-resolution processing supporting up to 4K input, and a high-quality bilingual dataset. The model achieves state-of-the-art results in 8 of 18 benchmarks, surpassing the performance of some commercial models on tasks like chart understanding (Shi et al., 2024).

InternVL 2 is open-sourced, and we use the variant InternVL2-Llama3-76B last updated on July 15, 2024. The language part is initialized with Hermes-2-Theta-LLaMA3-70B, and the vision part is initialized with InternViT-6B-448px-V1-5. It is noteworthy that Hermes-2-Theta-LLaMA3-70B has a much worse MMLU-Pro score than the official LlaMA-3-70B-Instruct by Meta. The Hermes