Page 16

LLMs' time series understanding vary significantly across different model architectures.

Our experiments reveal substantial variations in performance and behavior across different models when analyzing time series data. For instance, GPT-4o-mini shows little difference in performance with or without Chain of Thought (CoT) prompting, and even slightly improves with CoT for frequency anomalies, unlike other models. Qwen demonstrates poor performance with text prompts but reasonable performance with vision prompts and is most negatively affected by CoT. Gemini, similar to GPT-4o-mini, struggles with visual frequency anomalies. InternVL2 shows a smaller gap between vision and text performance, suggesting a more balanced approach. These diverse results indicate that the LLMs' capabilities in time series analysis are highly dependent on the specific model architecture and training approach, rather than being uniform across all LLMs.

## 6 CONCLUSION

In this paper, we conducted a comprehensive investigation into Large Language Models' (LLMs) understanding of time series data and their anomaly detection capabilities. Our findings challenge several assumptions prevalent in current literature, highlighting the need for rigorous empirical validation of hypotheses about LLM behavior. Our key findings include LLM's visual advantage, limited reasoning, non-human-like processing, and model-specific capabilities. These insights have important implications for the design of future LLMs and the development of anomaly detection systems. For instance, our results suggest that LLMs do not effectively detect visual frequency anomalies, so vision-LLM-based anomaly detection systems shall leverage Fourier analysis before feeding the data to the LLM To improve performance. Our model-specific findings suggest that model selection and possible ensemble methods are crucial for designing LLM-based anomaly detection systems.

Our work underscores the importance of controlled studies in validating hypotheses about LLM behavior, cautioning against relying solely on intuition or speculation. Future research should continue to empirically test assumptions about LLMs' capabilities and limitations in processing complex data types like time series.

## REFERENCES

Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S'ebastien Bubeck, Qin Cai, Martin Cai, Caio C'esar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Yen-Chun Chen, YiLing Chen, Parul Chopra, Xiyang Dai, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Victor Fragoso, Dan Iter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin, Zeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang, Yu Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your phone. (arXiv:2404.14219), may 2024. doi: 10.48550/arXiv.2404.14219. URL http://arxiv.org/abs/2404.14219 . arXiv:2404.14219 [cs].

Julien Audibert, Pietro Michiardi, Fr'ed'eric Guyard, S'ebastien Marti, and Maria A. Zuluaga. Do deep neural networks contribute to multivariate time series anomaly detection? Pattern Recognition , 132:108945, dec 2022. ISSN 00313203. doi: 10.1016/j.patcog.2022.108945. arXiv:2204.01637 [cs].

Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. (arXiv:2308.12966), oct 2023. URL http://arxiv.org/ abs/2308.12966 . arXiv:2308.12966 [cs].