Page 4

We apply the above prompting strategies to design the trial examples for the five types of anomalies and evaluate the ability of the LLMs in identifying and explaining such anomalies in time series. The bottom part in Figure 1 shows the overall performance of LLaMA-3 and GPT-4 on these trial examples. Additional examples of the responses from LLaMA-3 and GPT-4 can be found in Appendix A.4.

Generally, we observe that LLaMA-3 does not show significant improvement with different prompt designs and example cases, while GPT-4 demonstrates impressive results with any kind of prompts. For each case, we conduct five trials, and a detailed analysis of the responses from GPT-4 and LLaMA-3 reveals the following: GPT-4's responses are more consistent, suggesting that GPT-4 genuinely understands the prompts and examples. These instructions 'activate" GPT-4 to consider different perspectives and provide correct results. In contrast, LLaMA-3 more likely provides varied responses to the same prompt. For more obvious anomalies, such as global point anomalies and trend anomalies (see Figure 7 in Appendix), LLaMA-3 provides more stable results with correctly identified anomalies and explanations. In summary, we observe more emergent abilities [Wei et al., 2022a] in GPT-4, where simple instructions can activate its capability of time series anomaly detection, leading to more accurate identification and explanation of time series anomalies. Although LLaMA-3 does not exhibit these abilities to the same extent (potentially due to its smaller parameter size compared to GPT-4), it still shows some capabilities in grasping the overall shape of time series.

## 4.3 LLMagainst Anomaly Detection Baselines

Given the impressive performance of GPT-4 on all trial examples with different prompts, we now evaluate how GPT-4 performs time series anomaly detection compared to classic anomaly detection baseline methods.

Datasets & Evaluation Metrics We evaluate the performance on four common time series anomaly detection datasets [Paparrizos et al., 2022]: YAHOO, ECG, SVDB, and IOPS, which include anomalies in monitoring services and ECG recordings. In our study, we carefully curate the datasets to encompass a broad spectrum of patterns. In each dataset, we select 100 distinct time series segments with length 1,080 that demonstrate maximum variability. We utilize the initial 50% of each time series as training data. More details can be found in Appendix A.1. We use F-score and Range-F [Paparrizos et al., 2022] to evaluate the performance. Range-F is an extension of the F-score, where a detection is considered accurate if the identified anomaly falls within the same window as the actual anomaly; in this case, we set the window size to 5.

Baseline Methods In our comparison, we evaluate a range of time series anomaly detection methods. This includes traditional approaches such as Isolation Forest (IForest) [Liu et al., 2008], Matrix Profile (MP) [Yeh et al., 2016], and Autoencoder [Sakurada and Yairi, 2014]. Additionally, we explore forecasting-based methods, namely LSTM [Malhotra et al., 2015], Prophet [Taylor and Letham, 2018], Informer [Zhou et al., 2021], DLinear [Zeng et al., 2023] and TimesNet [Wu et al., 2022]. For these forecasting methods, anomalies are defined as observations deviating from the forecasted values by more than a 3Ïƒ (three standard deviations) threshold. More implementation details can be found in Appendix A.2.

LLMSettings We structure the time series segments using multi-modal prompts analogous to the example depicted in Figure 1, then feed the prompts to GPT-4 through the OpenAI API services 3 . Additionally, we craft a specific prompt designed to parse the output into a desired JSON format. This format encompasses two key components: a list of indices identifying the anomalous points, and a textual explanation that elucidates the rationale behind the identification of these anomalies. More details can be found in Appendix A.6.

Comparison Results Table 1 provides a comparative analysis of GPT-4 against baseline methods in anomaly detection tasks. Generally, we observe that most baseline methods perform well on

Table 1: Comparison between GPT-4 and Classic Time Series Anomaly Detectors with rank.

| Method   | YAHOO     | YAHOO     | ECG       | ECG       | SVDB      | SVDB      | IOPS      | IOPS      | avg-Rank   |
|----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|------------|
|          | F-score   | Range-F   | F-score   | Range-F   | F-score   | Range-F   | F-score   | Range-F   | avg-Rank   |
| IForest  | 0.0271(7) | 0.1066(8) | 0.1602(9) | 0.1780(9) | 0.0839(5) | 0.0886(9) | 0.0754(4) | 0.1125(5) | 9(7,9)     |
| MP       | 0.0630(4) | 0.1719(4) | 0.1654(8) | 0.1808(8) | 0.1170(4) | 0.1303(4) | 0.0064(9) | 0.0160(9) | 7(7,7)     |
| AE       | 0.0212(8) | 0.1078(7) | 0.3199(2) | 0.3580(2) | 0.4352(1) | 0.4626(1) | 0.0667(6) | 0.0985(7) | 4(2,4)     |
| LSTM     | 0.1466(1) | 0.2144(1) | 0.1737(7) | 0.2012(7) | 0.0773(9) | 0.0900(8) | 0.1062(2) | 0.1395(3) | 6(5,6)     |
| Prophet  | 0.0455(5) | 0.1324(5) | 0.3852(1) | 0.4556(1) | 0.1745(3) | 0.2107(3) | 0.0456(8) | 0.1324(4) | 2(2,1)     |
| Informer | 0.0304(6) | 0.1161(6) | 0.1842(6) | 0.2141(6) | 0.0784(8) | 0.0916(7) | 0.0474(7) | 0.0707(8) | 8(9,8)     |
| DLinear  | 0.1051(3) | 0.1760(3) | 0.1867(4) | 0.2208(4) | 0.0801(7) | 0.0954(5) | 0.0676(5) | 0.0995(6) | 5(5,5)     |
| TimesNet | 0.1457(2) | 0.2112(2) | 0.1867(4) | 0.2178(5) | 0.0809(6) | 0.0943(6) | 0.1443(1) | 0.1889(1) | 1(1,2)     |
| GPT-4    | 0.0204(9) | 0.0936(9) | 0.2911(3) | 0.3258(3) | 0.2681(2) | 0.2945(2) | 0.1020(3) | 0.1414(2) | 3(2,3)     |

specific datasets. For example, LSTM performs well on the YAHOO and IOPS datasets but poorly on the ECG and SVDB datasets. Conversely, the Autoencoder performs well on the ECG and SVDB datasets but poorly on YAHOO and IOPS. Compared to these baseline methods, GPT-4 shows notable achievements in F-score and Range-F metrics across the ECG, SVDB, and IOPS datasets, with an average rank of 3. Despite its performance on YAHOO, GPT-4 is the most stable model across the ECG, SVDB, and IOPS datasets, even when compared to TimesNet and Prophet. This demonstrates the potential of GPT-4 as a generalized time series anomaly detector.

Hallucination in Indices Although the F-score and Range-F results from GPT-4 appear promising, our analysis reveals that the model occasionally generates indices outside the expected time series segments, such as 1,200 for a series of length 1,000. As shown in Table 2, these hallucinations occur in approximately 21% to 29% of time series segments across different datasets. In datasets like YAHOO and IOPS, with a lower anomaly proportion (about 1% to 4% of segment points), the median number of hallucinated points is relatively low (2 or 3). However, the average number of hallucinations is significantly higher, indicating that some segments experience many spurious predictions. This issue