Page 23

Figure 18: Examples for a) good, b) bad, and c) hallucinated explanation by GPT-4 on YAHOO, ECG, and SVDB datasets.

<!-- image -->

<!-- image -->

<!-- image -->

Figure 19: The distribution over F-score and Range-F on the four datasets by GPT-4.

<!-- image -->

cosine learning rate scheduler, a warmup ratio of 0.05, a max gradient norm of 0.3, fp16 set to True , and group\_by\_length set to True . We trained for 1 epoch, as we observed a drop in model performance with additional epochs. During inference, we set the max\_new\_tokens parameter to 512 due to computational limitations (processing 100 samples with this parameter set to 2048 takes approximately 5 hours). We used the default generation strategy, which is greedy search. The instruction dataset is configured to contain 1000 samples generated by TTGenerator, with a mix of time series lengths of 180, 360, and 720, and includes a variety of five types of time series anomalies or no anomaly. By default, we use the general instruction prompt to formalize the text for fine-tuning LLaMA3.

## A.7 Computation Resources

The experiments are conducted on two NVIDIA H100 PCIe 80G GPUs. Fine-tuning LLaMA-3-8B on 2000 datasets with the time series length as 360 takes approximately 2 hours and requires about 130

Sudden changes and spikes in the series indicate anomalies. Specifically, the dramatic increase and decrease around indexes 110, 111, and 165-999, deviating significantly from the general trend, suggest external factors or errors in data collection/recording.

## (c) ECG - Hallucination

The values drastically decrease to 53235 and then increase to 7117031, which is inconsistent with the overall trend and magnitude of the data.