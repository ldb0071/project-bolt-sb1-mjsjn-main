We suggest using these simple heuristic values as a strong baseline for future studies to compare against. In light of recent publications highlighting the limitations of using the point-adjusted F1 score ([11, 30, 31, 42]), and yet the large amount of work still using it, we also report the results using point-adjustment in Appendix D.

For multi-entity datasets, we observe that the standard method (training one point-based and sequencebased model per entity) outperforms the combined method for MSL and SMD datasets. This is not surprising, given that entity-to-entity variations might be large. However, for the SMAP dataset, we observe that the combined method performs better. We attribute such results to the fact that the SMAP spacecraft are routine, hence the resulting telemetry between entities can have similar underlying distributions. This contributes to additive learning from the increased training data [13].

## 4.4 Ablation Study

The ablation study conducted in this section sheds light on several aspects of the proposed method. In Table 3, we compare the performance of five methods that yield different anomaly scores (either A ( · ) or ˆ A ( · ) ). For the first two methods, the reconstruction errors of M pt and M seq are used, respectively. Since M pt mostly performs better than M seq , we use the point-based reconstruction error as A ( · ) to calculate ˆ A ( · ) for the last three methods. The third method corresponds to an unnormalized simple smoothed value of A ( · ) (cf. section 3.4). The fourth and fifth methods use gate functions (11) and (8), respectively, but the same θ N that corresponds to the 98.5 percentile of the nominality score from the training data ( N trn ). This method for setting θ N works well enough and can be applied across different datasets. Illustrations of the distribution of nominality scores for the SWaT and WADI datasets, along with the 98.5% threshold value are shown in Fig. 4. We can observe that the distributions are close to appropriate (cf. section 3.3). The results for the last three methods are averaged over induction lengths d = 1 , 2 , 4 , 8 , 16 , 32 , 64 , 128 , and 256 . Each entity is trained separately for multi-entity datasets and the results are pooled together.