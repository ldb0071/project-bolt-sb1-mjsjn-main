Page 9

Explanation Analysis We further manually check the explanation performance by the fine-tuned LLaMA3 on the five anomaly types over the three datasets, where there are 60 samples for each anomaly type. The results are shown in Table 6. Similar to GPT-4, we notice that the model explains well for point-aware anomalies than the context-aware anomalies. Different from GPT-4, the explanations provided by LLaMA3 are more general, such as " The anomalies are mostly due to the sudden changes in the value of the time series ". Examples for the explanations provided by LLaMA3 can be found in Figure 4.

## 6 Conclusion

In this paper, we comprehensively investigate the capability of Large Language Models (LLMs) in time series anomaly detection by addressing three key questions: Can LLMs be directly applied for explainable time series anomaly detection? How can LLMs detect and explain time series anomalies via prompt engineering? Can we improve LLMs' detection performance through instruction finetuning? The answers to these questions are: No, Yes, and Yes, respectively, with evidence showing that GPT-4 demonstrates competent performance compared to baseline methods with minimal effort in prompt engineering, and LLaMA3 achieves better performance after instruction fine-tuning. In summary, LLMs show promising potential for time series anomaly detection, while customized prompts and instructions are essential.

## References

| Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.                                                                                                                                                       |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Hao Wang, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, and Yuyang Wang. Chronos: Learning the language of time series, 2024. |
| Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. TEMPO:                                                                                                                                                                                                                                                                                             |
| Prompt-based generative pre-trained transformer for time series forecasting. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/                                                                                                                                                                                                       |
| Ching Chang, Wei-Yao Wang, Wen-Chih Peng, and Tien-Fu Chen. Llm4ts: Aligning pre-trained                                                                                                                                                                                                                                                                                                   |
| Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning, 2023.                                                                                                                                                                                                                                  |
| Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=0RDcd5Axok .                                                                                                                        |
| Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza                                                                                                                                                                                                                                                                                                   |
| Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,                                                                                                              |
| et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations , 2021. Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yux- uan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming                                                                     |
| Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT , pages                                                                                                                                                                                                       |
| Kwei-Herng Lai, Daochen Zha, Junjie Xu, Yue Zhao, Guanchu Wang, and Xia Hu. Revisiting time series outlier detection: Definitions and benchmarks. In Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 1) , 2021.                                                                                                                      |
| Chen Liu, Shibo He, Qihang Zhou, Shizhong Li, and Wenchao Meng. Large language model guided knowledge distillation for time series anomaly detection. arXiv preprint arXiv:2401.15123 , 2024a. Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In                                                                                                                         |
| 2008 eighth ieee international conference on data mining , pages 413-422. IEEE, 2008.                                                                                                                                                                                                                                                                                                      |
| Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in                                                                                                                                                                                                                                                                                            |
| Haoxin Liu, Zhiyuan Zhao, Jindong Wang, Harshavardhan Kamarthi, and B Aditya Prakash. Lst-                                                                                                                                                                                                                                                                                                 |

| Pankaj Malhotra, Lovekesh Vig, Gautam Shroff, Puneet Agarwal, et al. Long short term memory networks for anomaly detection in time series. In Esann , volume 2015, page 89, 2015.                                                                                                                                                                                               | Pankaj Malhotra, Lovekesh Vig, Gautam Shroff, Puneet Agarwal, et al. Long short term memory networks for anomaly detection in time series. In Esann , volume 2015, page 89, 2015.                                                                |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations , 2022.                                                                                                                                                       |                                                                                                                                                                                                                                                  |
| instructions with human feedback. 27730-27744, 2022.                                                                                                                                                                                                                                                                                                                            | Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow Advances in Neural Information Processing Systems , 35: |
| Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly detection: A review. ACM Computing Surveys , 54(2):1-38, 2021.                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                  |
| John Paparrizos, Yuhao Kang, Paul Boniol, Ruey S Tsay, Themis Palpanas, and Michael J Franklin. Tsb-uad: an end-to-end benchmark suite for univariate time-series anomaly detection. Proceedings of the VLDB Endowment , 15(8):1697-1711, 2022.                                                                                                                                 |                                                                                                                                                                                                                                                  |
| Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.                                                                                                                                                                                                        |                                                                                                                                                                                                                                                  |
| Mayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear dimen- sionality reduction. In Proceedings of the MLSDA 2014 2nd workshop on machine learning for sensory data analysis , pages 4-11, 2014.                                                                                                                                               |                                                                                                                                                                                                                                                  |
| Sean J Taylor and Benjamin Letham. Forecasting at scale. The American Statistician , 72(1):37-45, 2018.                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
| Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023a.                                                                                                           |                                                                                                                                                                                                                                                  |
| Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023b.                                                                                                                |                                                                                                                                                                                                                                                  |
| Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large lan- guage models. Transactions on Machine Learning Research , 2022a. ISSN 2835-8856. URL                    |                                                                                                                                                                                                                                                  |
| Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in                                                                                                                                                                                       |                                                                                                                                                                                                                                                  |
| neural information processing systems , 35:24824-24837, 2022b.                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                  |
| Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In The eleventh international conference on learning representations , 2022.                                                                                                                                             |                                                                                                                                                                                                                                                  |
| Hao Xue and Flora D Salim. Promptcast: A new prompt-based learning paradigm for time series forecasting. IEEE Transactions on Knowledge and Data Engineering , 2023.                                                                                                                                                                                                            |                                                                                                                                                                                                                                                  |
| Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding, Hoang Anh Dau, Diego Furtado Silva, Abdullah Mueen, and Eamonn Keogh. Matrix profile i: all pairs similarity joins for time series: a unifying view that includes motifs, discords and shapelets. In 2016 IEEE 16th international conference on data mining (ICDM) , pages 1317-1322. Ieee, 2016. |                                                                                                                                                                                                                                                  |

| Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence , volume 37, pages 11121-11128, 2023.                                                                         |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, and Jingbo Shang. Large language models for time series: A survey, 2024.                                                                                                                                                            |
| Zheng Zhang, Hossein Amiri, Zhenke Liu, Andreas Züfle, and Liang Zhao. Large language models for spatial trajectory patterns mining. arXiv preprint arXiv:2310.04942 , 2023.                                                                                                            |
| Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence , volume 35, pages 11106-11115, 2021. |
| Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series analysis by pretrained lm. In Thirty-seventh Conference on Neural Information Processing Systems , 2023.                                                                             |

## A Experimental Settings

## A.1 Benchmark Dataset Settings

We selected four widely used time series anomaly detection datasets: YAHOO, ECG, SVDB, and IOPS, as referenced in the paper by Paparrizos et al. (2022)[Paparrizos et al., 2022]. The original datasets can be downloaded from the repository 4 . We constructed the evaluation dataset by manually selecting segments from the time series data. First, we determined the window size for each time series using the Fast Fourier Transform (FFT) and then computed the median window size across the dataset. The segment length was set to four times the median window size, resulting in a segment length of 1080 based on the window sizes of the four datasets. Each time series was partitioned into multiple segments of this length. We manually inspected the segments with a length of 1080 for each dataset, selecting time series with diverse distributions. From these, we randomly extracted 100 segments for evaluation. Table 7 presents the specifics of segment lengths and the number of time series used, where '# Time Series' denotes the distinct time series in the original dataset, and '% Anomalies' denotes the average proportion of anomalies in the selected segments. Examples for each dataset can be seen in Figure 3 and Figure 18.

Table 7: Dataset Details

| Dataset   |   Segment Length |   # Time Series |   %Anomalies |
|-----------|------------------|-----------------|--------------|
| YAHOO     |             1080 |             100 |         1.42 |
| ECG       |             1080 |              21 |        19.99 |
| SVDB      |             1080 |              21 |        27.94 |
| IOPS      |             1080 |              21 |         3.78 |