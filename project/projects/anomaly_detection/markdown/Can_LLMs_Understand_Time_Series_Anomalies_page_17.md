Page 17

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems , volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper\_files/paper/2020/ hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html .

Wenchao Chen, Long Tian, Bo Chen, Liang Dai, Zhibin Duan, and Mingyuan Zhou. Deep variational graph convolutional recurrent network for multivariate time series anomaly detection. In Proceedings of the 39th International Conference on Machine Learning , pp. 3621-3633. PMLR, jun 2022. URL https://proceedings.mlr.press/v162/chen22x.html .

Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. (arXiv:2404.16821), apr 2024. doi: 10.48550/arXiv.2404.16821. URL http://arxiv.org/abs/2404.16821 . arXiv:2404.16821 [cs].

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. (arXiv:2110.14168), nov 2021. doi:

| 10.48550/arXiv.2110.14168. URL http://arxiv.org/abs/2110.14168 [cs].                                                                                                                                                                             | . arXiv:2110.14168                                                                                                                                                                                                                                                                                                                          |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| URL                                                                                                                                                                                                                                              | LMDeploy Contributors. Lmdeploy: A toolkit for compressing, deploying, and serving llm, 2023. https://github.com/InternLM/lmdeploy .                                                                                                                                                                                                        |
| Google. Gemini flash, sep 2024. URL flash/ .                                                                                                                                                                                                     | https://deepmind.google/technologies/gemini/                                                                                                                                                                                                                                                                                                |
| Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero- shot time series forecasters. (arXiv:2310.07820), oct 2023. 2310.07820 . arXiv:2310.07820 [cs].                                                   | URL http://arxiv.org/abs/ Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text                                                                                                                                                                                                                    |
| degeneration. (arXiv:1904.09751), feb 2020. doi: 10.48550/arXiv.1904.09751. URL http: //arxiv.org/abs/1904.09751 . arXiv:1904.09751 [cs].                                                                                                        | Lun Huang, Jiajun Zhou, Chengqing Zhang, and Hainan Huang. Multimodal neural machine trans- Proceedings of the 57th Annual Meeting of the Association , pp. 1065-1075, 2019. Alexis Huet, Jose Manuel Navarro, and Dario Rossi. Local evaluation of time series anomaly de- Proceedings of the 28th ACM SIGKDD Conference on Knowledge Dis- |
| lation with language scene graph. In for Computational Linguistics                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                             |
| tection algorithms. In covery and Data Mining , 4503-9385-0. doi: 10.1145/3534678.3539339. 3534678.3539339 .                                                                                                                                     | pp. 635-645, Washington DC USA, aug 2022. ACM. ISBN 978-1- URL https://dl.acm.org/doi/10.1145/ Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series forecasting                                                         |
| by reprogramming large language models. (arXiv:2310.01728), jan 2024a. URL                                                                                                                                                                       | http://arxiv. Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong Wang, Shirui                                                                                                                                                                                                                                    |
| org/abs/2310.01728 . arXiv:2310.01728 [cs].                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                             |
| Pan, and Qingsong Wen. Position: What can large language models tell us about time series analysis. (arXiv:2402.02713), jun 2024b. arxiv.org/abs/2402.02713                                                                                      | doi: 10.48550/arXiv.2402.02713. URL http:// . arXiv:2402.02713 [cs]. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. (arXiv:2205.11916), jan 2023. doi: 10.48550/arXiv.                                                                                   |
| 2205.11916. URL http://arxiv.org/abs/2205.11916                                                                                                                                                                                                  | . arXiv:2205.11916 [cs]. Proceedings of the ACM SIGOPS 29th Symposium on Operating                                                                                                                                                                                                                                                          |
| Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In                              |                                                                                                                                                                                                                                                                                                                                             |
| Systems Principles , 2023.                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                             |
|                                                                                                                                                                                                                                                  | Chih-Yu Andrew Lai, Fan-Keng Sun, Zhengqi Gao, Jeffrey H Lang, and Duane Boning. Nominality score conditioned time series anomaly detection by point/sequential reconstruction. Advances in                                                                                                                                                 |
| Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. Unicoder: A universal encoder for text, image and video. In Vision (ECCV) , pp. 154-170, 2019.                                                                            | Proceedings of the European Conference on Computer                                                                                                                                                                                                                                                                                          |
| Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms struggle with long in-context learning. (arXiv:2404.02060), jun 2024. doi: 10.48550/arXiv.2404.02060. URL http://arxiv.org/abs/2404.02060 . arXiv:2404.02060 [cs]. |                                                                                                                                                                                                                                                                                                                                             |
| Matthew D. Lieberman. Reflexive and reflective judgment processes: A social cognitive neuro- science approach , pp. 44-67. Cambridge University Press, New York, NY, US, 2003. 978-0-521-82248-0.                                                | ISBN                                                                                                                                                                                                                                                                                                                                        |

| Chen Liu, Shibo He, Qihang Zhou, Shizhong Li, and Wenchao Meng. Large language model guided knowledge distillation for time series anomaly detection. (arXiv:2401.15123), jan 2024a. URL http://arxiv.org/abs/2401.15123 . arXiv:2401.15123 [cs].                                                                                                                                                                                                  |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In 2008 eighth ieee international conference on data mining , pp. 413-422. IEEE, 2008.                                                                                                                                                                                                                                                                                            |
| Haoxin Liu, Shangqing Xu, Zhiyuan Zhao, Lingkai Kong, Harshavardhan Kamarthi, Aditya B. Sasanur, Megha Sharma, Jiaming Cui, Qingsong Wen, Chao Zhang, and B. Aditya Prakash. Time- mmd: A new multi-domain multimodal dataset for time series analysis. (arXiv:2406.08627), jun 2024b. URL http://arxiv.org/abs/2406.08627 . arXiv:2406.08627 [cs].                                                                                                |
| Haoyi Liu, Shanghang Bai, Yuyang Shen, Xu Han, Wei Zhang, and Jiliang Gao. Multimodal trans- former for unaligned multimodal time series analysis. In Proceedings of the Web Conference                                                                                                                                                                                                                                                            |
| Peiyuan Liu, Hang Guo, Tao Dai, Naiqi Li, Jigang Bao, Xudong Ren, Yong Jiang, and Shu-Tao Xia. Taming pre-trained llms for generalised time series forecasting via cross-modal knowledge distillation. (arXiv:2403.07300), mar 2024c. doi: 10.48550/arXiv.2403.07300. URL http: //arxiv.org/abs/2403.07300 . arXiv:2403.07300 [cs].                                                                                                                |
| Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolin- guistic representations for vision-and-language tasks. Advances in Neural Information Processing                                                                                                                                                                                                                                                 |
| motion patterns. PLoS ONE , 11(2):e0149413, feb 2016. ISSN 1932-6203. doi: 10.1371/journal. pone.0149413. Andreas Opedal, Alessandro Stolfo, Haruki Shirakami, Ying Jiao, Ryan Cotterell, Bernhard                                                                                                                                                                                                                                                 |
| Scholkopf, Abulhair Saparov, and Mrinmaya Sachan. Do language models exhibit the same cognitive biases in problem solving as human learners? (arXiv:2401.18070), jun 2024. doi: 10.48550/arXiv.2401.18070. URL http://arxiv.org/abs/2401.18070 . arXiv:2401.18070 [cs]. OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence, sep 2024. URL https://openai.                                                                                  |
| com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/ .                                                                                                                                                                                                                                                                                                                                                                                     |
| Maria Riveiro and Goran Falkman. Interactive visualization of normal behavioral models and expert rules for maritime anomaly detection. In Imaging and Visualization 2009 Sixth International Conference on Computer Graphics , pp. 459-466, August 2009. doi: 10.1109/CGIV.2009.54. URL https://ieeexplore.ieee.org/document/5298765/?arnumber=5298765 . M. Saquib Sarfraz, Mei-Yen Chen, Lukas Layer, Kunyu Peng, and Marios Koulakis. Position: |
| Quo vadis, unsupervised time series anomaly detection? (arXiv:2405.02678), jun 2024. URL http://arxiv.org/abs/2405.02678 . arXiv:2405.02678 [cs]. Chufan Shi, Cheng Yang, Yaxin Liu, Bo Shui, Junjie Wang, Mohan Jing, Linran Xu, Xinyu Zhu,                                                                                                                                                                                                       |
| Siheng Li, Yuxiang Zhang, Gongye Liu, Xiaomei Nie, Deng Cai, and Yujiu Yang. Chart- mimic: Evaluating lmm's cross-modal reasoning capability via chart-to-code generation. (arXiv:2406.09961), jun 2024. doi: 10.48550/arXiv.2406.09961. URL http://arxiv.org/                                                                                                                                                                                     |
| Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint                                                                                                                                                                                                                                                                                                                                                       |
| model for video and language representation learning. arXiv preprint arXiv:1904.01176 , 2019. Mingtian Tan, Mike A. Merrill, Vinayak Gupta, Tim Althoff, and Thomas Hartvigsen. Are language                                                                                                                                                                                                                                                       |
| models actually useful for time series forecasting? (arXiv:2406.16964), jun 2024. URL http:                                                                                                                                                                                                                                                                                                                                                        |
| //arxiv.org/abs/2406.16964 . arXiv:2406.16964 [cs]. Hua Tang, Chong Zhang, Mingyu Jin, Qinkai Yu, Zhenting Wang, Xiaobo Jin, Yongfeng Zhang, and Mengnan Du. Time series forecasting with llms: Understanding and enhancing model capabilities. (arXiv:2402.10835), aug 2024. doi: 10.48550/arXiv.2402.10835. URL http:                                                                                                                            |

| Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar Mitra. Large language models can accu- rately predict searcher preferences. (arXiv:2309.10621), may 2024. URL http://arxiv.org/                                                                                                                                                                                                                                             |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Shreshth Tuli, Giuliano Casale, and Nicholas R. Jennings. Tranad: Deep transformer networks for anomaly detection in multivariate time series data. (arXiv:2201.07284), may 2022. URL http://arxiv.org/abs/2201.07284 . arXiv:2201.07284 [cs].                                                                                                                                                                                     |
| Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. (arXiv:2406.01574), jun 2024. doi: 10.48550/arXiv.2406.01574. URL http://arxiv.org/abs/2406.01574 . arXiv:2406.01574 [cs]. |
| Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. arXiv.org , jan 2022. URL https://arxiv.org/abs/2201.11903v6 .                                                                                                                                                                              |
| Christopher Wimmer and Navid Rekabsaz. Leveraging vision-language models for granular market change prediction. (arXiv:2301.10166), jan 2023. URL http://arxiv.org/abs/2301.10166 . arXiv:2301.10166 [cs, q-fin].                                                                                                                                                                                                                  |
| Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? arXiv.org , mar 2023. URL https://arxiv.org/                                                                                                                                                                                                                                                    |
| Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? (arXiv:2205.13504), aug 2022. URL http://arxiv.org/abs/2205.13504 . arXiv:2205.13504 [cs].                                                                                                                                                                                                                                |

## A MODEL DETAILS

## A.1 (M)LLM ARCHITECTURES AND VALIDATION