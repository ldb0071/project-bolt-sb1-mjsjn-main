Page 2

To the best of our knowledge, this work represents a very first to comprehensively investigate and enhance the performance of LLMs in time series anomaly detection, with specific strategies designed to expand LLMs to the broad time series and anomaly detection domains.

## 2 Related Work

Transformers have demonstrated remarkable success in natural language processing (NLP) and, given their proficiency in handling sequential data, a significant number of transformer-based models have been proposed for time series forecasting. Early works focused on modifications to transformer modules, ranging from position embeddings [Nie et al., 2022] to attention mechanisms [Zhou et al., 2021], to better fit time series analysis [Zhou et al., 2023]. Most approaches can be regarded as aligning approaches, as they start training from transformer backbones (such as BERT [Kenton and Toutanova, 2019], GPT-2 [Radford et al., 2019], and T5 [Raffel et al., 2020]), and train the encoder, decoder, or middle layers of the transformers via fine-tuning [Zhou et al., 2023, Chang et al., 2024, Cao et al., 2024] or full parameter pretraining [Ansari et al., 2024]. However, these models often overlook the rich textual information within the pretrained models, where the fine-tuned model is still primarily used to process only time series data.

Starting with ChatGPT [Ouyang et al., 2022], we have witnessed the power of large language models (LLMs) such as GPTs [Achiam et al., 2023] and LLaMAs [Touvron et al., 2023a,b]. These models, with larger parameters and trained on more extensive datasets [Hoffmann et al., 2022], exhibit powerful reasoning capabilities for handling complex tasks. This has triggered initial research in time series analysis, where some studies directly treat the time series as tokens and feed them into LLMs for forecasting [Gruver et al., 2023], or incorporate time series data with instruction prompts [Xue and Salim, 2023] or chain-of-thought prompts [Liu et al., 2024c]. Some even further fine-tune the

Figure 1: Example of responses from LLaMA-3 and GPT-4 to time series with shape anomalies using direct and multi-modal instructions. The bottom panel shows the overall performance across all trial examples for different anomaly types: global point anomalies, local point anomalies, seasonal anomalies, trend anomalies, and shape anomalies; and prompting strategies: directly use LLMs, multimodal instruction, in-context and chain-of-thought strategies anomaly detection. For each anomaly type and prompting strategy, we conduct five trials and evaluate the correctness of both identified indices and explanations. A correctness rate of 100% means the model provided correct results in all five trials.

<!-- image -->

LLMs [Jin et al., 2024]. However, these outputs remain within the time series domain, limiting their applicability to tasks such as generating descriptions for time series data.

Few works have investigated utilizing LLMs for time series anomaly detection, a critical task in various real-world applications. Zhou et al. [2023] is likely the first to fine-tune language models for time series anomaly detection. They fine-tuned a general model for diverse time series tasks, such as classification, anomaly detection, forecasting, and few-shot or zero-shot learning, treating anomaly detection as a binary classification problem and adding layers on top of transformer modules for classification. Liu et al. [2024a] used LLMs as the teacher model and trained a student network to mimic the LLM outputs, identifying anomalies as points with distinct values between the teacher and student networks. However, these works did not utilize the reasoning ability of LLMs to provide textual explanations for detections. Zhang et al. [2023] evaluated GPT-4 and Claude-2 2 with prompt engineering for human mobility trajectory behavior anomaly detection, asking the models to provide explanations for the detections. However, they treated anomaly detection as a binary classification task (i.e., whether a given human mobility trajectory behavior sequence contains anomalies) and did not delve into the explanations provided by the LLMs. In this work, we focus on detecting specific anomaly points or segments within a time series and investigate whether LLMs can accurately capture the indices of anomalies and explain their detection results. To the best of our knowledge, this is one of the first comprehensive studies on time series anomaly detection using LLMs.

## 3 Can LLMs Be Directly Applied for Time Series Anomaly Detection?

We begin with an empirical study on evaluating two representative large language models (LLMs), GPT-4 and LLaMA-3, in identifying and explaining anomalies in time series data. Our approach involves interpreting time series data as text tokens and tasking the LLMs with: i) determining the

## Multi-modal Instruction

Figure 2: Templates for different prompt strategies, where the ' requirements ' include the tasks for the LLMs to do, e.g., providing the indices for the anomalies, and explaining the reason if anomalies are detected, with examples in Figure 1. More details can be found in Appendix A.3