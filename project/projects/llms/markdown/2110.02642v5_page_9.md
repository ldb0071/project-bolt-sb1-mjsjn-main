# Page 9

## Page Information

- **Type**: figure_page
- **Word Count**: 500
- **Has Tables**: True
- **Has Figures**: False

## Content

# Page 9

## 3.2 MINIMAX ASSOCIATION LEARNING

As an unsupervised task, we employ the reconstruction loss for optimizing our model. The reconstruction loss will guide the series-association to find the most informative associations. To further amplify the difference between normal and abnormal time points, we also use an additional loss to enlarge the association discrepancy. Due to the unimodal property of the prior-association, the discrepancy loss will guide the series-association to pay more attention to the non-adjacent area, which makes the reconstruction of anomalies harder and makes anomalies more identifiable. The loss function for input series X ∈ R N × d is formalized as:

where ̂ X ∈ R N × d denotes the reconstruction of X . ‖·‖ F , ‖·‖ k indicate the Frobenius and k -norm. λ is to trade off the loss terms. When λ > 0 , the optimization is to enlarge the association discrepancy. A minimax strategy is proposed to make the association discrepancy more distinguishable.

L Total ( ̂ X , P , S , λ ; X ) = ‖X -̂ X‖ 2 F -λ ×‖ AssDis( P , S ; X ) ‖ 1 (4)

Minimax Strategy Note that directly maximizing the association discrepancy will extremely reduce the scale parameter of the Gaussian kernel (Neal, 2007), making the prior-association meaningless. Towards a better control of association learning, we propose a minimax strategy (Figure 2). Concretely, for the minimize phase , we drive the prior-association P l to approximate the seriesassociation S l that is learned from raw series. This process will make the prior-association adapt to various temporal patterns. For the maximize phase , we optimize the series-association to enlarge the association discrepancy. This process forces the series-association to pay more attention to the non-adjacent horizon. Thus, integrating the reconstruction loss, the loss functions of two phases are:

Minimize Phase: L Total ( ̂ X , P , S detach , -λ ; X ) Maximize Phase: L Total ( ̂ X , P detach , S , λ ; X ) , (5)

where λ > 0 and ∗ detach means to stop the gradient backpropagation of the association (Figure 1). As P approximates S detach in the minimize phase, the maximize phase will conduct a stronger constraint to the series-association, forcing the time points to pay more attention to the non-adjacent area. Under the reconstruction loss, this is much harder for anomalies to achieve than normal time points, thereby amplifying the normal-abnormal distinguishability of the association discrepancy.

Association-based Anomaly Criterion We incorporate the normalized association discrepancy to the reconstruction criterion, which will take the benefits of both temporal representation and the distinguishable association discrepancy. The final anomaly score of X ∈ R N × d is shown as follows:

AnomalyScore( X ) = Softmax ( -AssDis( P , S ; X ) ) glyph[circledot] [ ‖X i, : -̂ X i, : ‖ 2 2 ] i =1 , ··· ,N (6)

where glyph[circledot] is the element-wise multiplication. AnomalyScore( X ) ∈ R N × 1 denotes the point-wise anomaly criterion of X . Towards a better reconstruction, anomalies usually decrease the association discrepancy, which will still derive a higher anomaly score. Thus, this design can make the reconstruction error and the association discrepancy collaborate to improve detection performance.

## Visual Content

### Page Preview

![Page 9](/projects/llms/images/2110.02642v5_page_9.png)

## Tables

### Table 1

| 4 4
3 3
Input
2 eulav 2 Time
1 Series1
0 0
-1 -1
0 20 40 60 80 100 0 20 40 60 80 10
length Time
2.2 2.2
2.1 2.1
2 eulav !! 2
1.9 1.9
1.8 1.8
1.7 1.7
0 20 40 60 80 100 0 20 40 60 80 10
length Time
(cid:1153)a(cid:1154)Point-Global
Figure6: Learn
Prior-associationvis
to get close to the se
degreeoftimeseries. | p
m
ri | 4 4
3 3
ut
2 eulav 2 e
1 es1
0 0
-1 -1 |  |  |  |  |  |  | eulav
- -
- -
0 | 11..55
1
00..55
0
00..55
-1
11..55 |  |  |  |  |  |  | 111...555
11
000...555
00
00..5.55
--11
11..5.55 |  |  |  |  |  | 1.5 1.5
1
0.5 0.5
0 eulav
-0.5 -0.5
-1
-1.5 -1.5 |  |  |  |  |  |  | -0 - -
-0 - -
-0 - -
-0 eulav - eulaV -
-0 - -
-0 - -
-0 - -
0 | 00...333
00...444
00...555
00...666
00...777
00...888
00...999
--11 |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| None | None | None | None | None | None | None | None | None | None | None |  |  |  |  |  | None | None |  |  |  |  | None | None |  |  |  |  |  |  | None | None |  |  |  |  |  |  | None |
| None | None | None |  |  |  |  |  |  | None | None |  |  |  |  |  | eeuullaaVv | None |  |  |  |  |  | None |  |  |  |  |  |  | None | None |  |  |  |  |  |  | None |
| None | None | None | 00 2200 4400 6600 8800 11000 | None | None | None | None | None | None | None | 00 2200 4400 6600 8800 110000 | None | None | None | None | --0-
--1- | None | 000 222000 444000 666000 888000 111000000 | None | None | None | None | None | 00 2200 4400 6600 8800 11000 | None | None | None | None | None | None | None | 000 222000 444000 666000 888000 111000000 | None | None | None | None | None | None |
| None | !! | 2.2 2.2
2.1 2.1
2 eulav 2
1.9 1.9
1.8 1.8
1.7 1.7 | lTeinmgteh | None | None | None | None | None | eulav | 22..33
22..22
22..11
22
11..99
11..88
11..77 | leTnimgthe | None | None | None | None | 2
2
eulav 1
1
1
1
1 | 2..22
2..11
22
1..99
1..88
1..77
1..66
.5 | leTTniimgmtehe | None | None | None | None | 2.1 2.1
2 2
1.9 eulav 1.9
1.8 1.8
1.7 1.7 | lTeinmgteh | None | None | None | None | None | eulav | 22..33
22..22
22..11
22
11..99 | leTTniimgmtehe | None | None | None | None | None |  |
| None | None | None |  |  |  |  |  |  | None | None |  |  |  |  |  | None | None |  |  |  |  |  | None |  |  |  |  |  |  | None | None |  |  |  |  |  |  | None |
| None | None | None | None | None | None | None | None | None | None | None |  |  |  |  |  | None | None |  |  |  |  |  | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None |
| None | None | None |  |  |  |  |  |  | None | None |  |  |  |  |  | None | None |  |  |  |  |  | None |  |  |  |  |  |  | None | None |  |  |  |  |  |  | None |
| None | None | None |  |  |  |  |  |  | None | None |  |  |  |  |  | None | None |  |  |  |  |  | None |  |  |  |  |  |  | None | None |  |  |  |  |  |  | None |
| None | None | None |  |  |  |  |  |  | None | None |  |  |  |  |  | None | None |  |  |  |  |  | None | None | None | None | None | None | None | None | None |  |  |  |  |  |  | None |
| None | None | None |  |  |  |  |  |  | None | None |  |  |  |  |  | None | None |  |  |  |  |  | None |  |  |  |  |  |  | None | None | None | None | None | None | None | None | None |
| None |  |  | 00 2200 4400 6600 8800 11000
lTeinmgteh | None | None | None | None | None | 0 |  | 00 2200 4400 6600 8800 110000
leTnimgthe | None | None | None | None |  | 1.5 | 00 2200 4400 6600 8800 110000
lTeinmgteh | None | None | None | None | 00 2200 4400 6600 8800 1100
leTnimgteh | None | None | None | None | None | None | 00 | 11..8800 2200 4400 6600 8800 110000
lTeinmgeth | None | None | None | None | None | None | None |
| None | None | None | (cid:1153)a(cid:1154)Point-Global | None | None | None | None | None |  | None | (cid:1153)b(cid:1154)Point-Contextual | None | None | None | None |  | None | (cid:1153)c(cid:1154)Pattern-Shapelet (cid:1153)d(cid:1154)Pattern-Seasonal (cid:1153)e(cid:1154)Pattern-Trend | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None |
| None |  | None | None | None | None | None | None | None | ed
ual
rie
As | None | scaleparameterσfordifferenttypesofanomalies(highlightinred).
ization Duringtheminimaxoptimization,theprior-associationislearned
s-association. Thus, the learned σ can reflect the adjacent-concentrating
showninFigure6,wefindthatσchangestoadapttovariousdatapatterns | None | None | None | None | σf
gth
hus
6, | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None |
