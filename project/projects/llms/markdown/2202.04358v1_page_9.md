# Page 9

## Page Information

- **Type**: table_page
- **Word Count**: 621
- **Has Tables**: False
- **Has Figures**: False

## Content

# Page 9

## 2.5 Neural Network Design and Implementation

The model uses a traditional neural network and the specific process is shown in Figure 4. Combined with

10-fold cross-validation can ensure the robustness and reliability of the algorithm. All layers of the spatially weighted neural network are full-connected with each other, and the Dropout technique proposed by Srivastava et al. is applied to improve the generalization ability of the model. [30] Each hidden layer is combined with the Batch Normalization technique. The parameter initialization is adopted by the method proposed by He [15] et al. and the activation function is adopted by PReLU.

Figure 4: Experiment Process

<!-- image -->

In the training process of GNNWR, we use the RMSE as the loss function. We used the more popular Adam optimizer and achieved better results than the stochastic gradience descent used in GNNWR's previous practice. When the loss function of the validation set grows or remains constant beyond a certain number of iterations, the model is considered to be overfitted and the neural network computation is automatically stopped. For a given set of hyperparameters, 10 models can be generated on a randomly selected 10-fold data set (a total of 2440 items, accounting for 85%) with 9 folds selected as the training set and the remaining 1 fold as the validation set. Secondly, by summing the loses of these ten models, the total model loses corresponding to the hyperparameters can be obtained. Finally, the hyperparameter of the model with the lowest mean value of loss was selected as the best, and the GNNWR it generated will be compared with the other two schemes.

After several trials, in the hyperparameters, the value of learning rate is 10 -2 . 95 ≈ 0 . 00112 , β 1 = 0 . 8 , β 2 = 0 . 999 , batch size = 128 . Percentage of loss at Dropout layer is 0.9; Epoch has a maximum number of iterations of 90,000. After comparing the results of several neural network hidden layers, the results are shown in Table 1. Note that the data used to calculate the mean squared error here are derived from normalized house prices, so the order of magnitude is different from the analysis below.

Table 1: Loss for Different Structures

| Structure of Hidden Layers    |   Validation Loss |   Train Loss |   Test Loss |
|-------------------------------|-------------------|--------------|-------------|
| [1024, 512, 256, 128, 64, 32] |          0.00647  |    0.00279   |    0.008867 |
| [512, 128, 64, 16]            |          0.006427 |    0.003804  |    0.008661 |
| [512, 128, 32]                |          0.006529 |    0.0040193 |    0.008683 |
| [256, 64, 16]                 |          0.006537 |    0.0043795 |    0.008555 |
| [256, 32, 8]                  |          0.006527 |    0.0049904 |    0.008379 |
| [256, 32]                     |          0.006567 |    0.0046721 |    0.008992 |

After pre-experimental comparison, it was found that increasing the number of hidden layers not only greatly improved the fitting accuracy, but also did not significantly weaken the generalization effectiveness. Considering the number of neurons in the input and output layers, the article adopts a 6-layer neural network structure, containing 1 input layer, 4 hidden layers with 512, 128, 64, 16 neurons, and 1 output layer. The number of neurons in the input layer is the number of training samples, and the number of neurons in the output layer is the number of parameters of the linear regression model (the number of independent variables plus one).

To further reflect the optimization in the iterations, Figure 5 shows the change in test indicators of one fold during model training. After running more than 30,000 epochs, if the loss on the validation set is not improved after 9000 epochs, the neural network training will be terminated.

To fully demonstrate the superiority of GNNWR, three models, OLR, GWR, and GNNWR, are developed and compared on the Shenzhen house price dataset. Among them, GWR uses the golden search method to find the most suitable number of neighboring elements according to the AICc index.

Since the NSS, QASP variables take values in small integers, the design matrix used in GWR modeling will show multicollinearity when the number of neighboring elements is small. Therefore, the search lower bound is set to 100, i.e., at least 100 neighboring elements are involved in the solution of the local regression coefficients.

Through a simple pre-experiment, 2440 data are extracted for modeling and the remaining 431 data are used for testing, and it can be compared to find that bi-square significantly outperforms Gaussian among the two weighting kernel functions of GWR.The specific parameters are shown in the following Table 2. Therefore, the next comparisons in this paper all take bi-square as the weight kernel function of the GWR model.

In the experiments, we used OLR as a comparison. Both OLR and GWR solutions are built on ArcGIS Pro. GNNWR built with TensorFlow 1.15.0 library under Python 3.6.13 kernel. The commonly

(a) The Decrease of AICc on Validation Set (b) The Decrease of AICc on Train Set(c) The Decrease of Average Absolute Error

<!-- image -->

Figure 5: Performance Variations for the Train (Orange) and Validation (Blue) Sets of GNNWR

<!-- image -->

Table 2: Performance for Different Kernel Types

|                 | Train   | Train    | Train    | Train    | Train   | Train                   | Test   | Test                    |
|-----------------|---------|----------|----------|----------|---------|-------------------------|--------|-------------------------|
| GWR Kernel Type | R2      | RMSE     | MAE      | MAPE/%   | AICc    | Correlation Coefficient | R2     | Correlation Coefficient |
| Bi-square(105)  | 0.8861  | 7655.203 | 5623.454 | 0.094994 | 51842.0 | 0.941789                | 0.7935 | 0.892818                |
| Gaussian(101)   | 0.7471  | 11408.08 | 8372.857 | 0.141284 | 52790.3 | 0.865382                | 0.6120 | 0.783185                |

used AICc criterion is chosen for GWR bandwidth optimization /uni3002

We use these three modeling approaches with the help of 10-fold cross-validation to be able to build the model on the training set, use the results on the validation set to calculate each metric of the model, evaluate the generalization ability of the model, and exclude the influence of chance factors. Finally, the modeling result with the strongest generalization ability is selected for all three methods, and the predictive ability of the model is tested on the test set. For 2871 data, we extracted 431 of them (about 15%) as the test set, and the remaining 2440 data were equally divided into 10 folds to participate in cross-validation, each containing 244 data (about 8.5%).

## Visual Content

### Page Preview

![Page 9](/projects/llms/images/2202.04358v1_page_9.png)
