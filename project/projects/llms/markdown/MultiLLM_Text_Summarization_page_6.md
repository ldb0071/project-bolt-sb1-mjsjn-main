# Page 6

## Page Information

- **Type**: figure_page
- **Word Count**: 341
- **Has Tables**: False
- **Has Figures**: False

## Content

# Page 6

## 2.2 Multi-LLM

The concept of leveraging multiple LLMs collaboratively has gained traction in recent research, particularly for tasks requiring complex reasoning and factual accuracy. For instance, Liang et al. (2024) introduced the Multi-Agent-Debate (MAD) framework, where LLMs engage in iterative debates to refine their reasoning. This framework demonstrated that a multi-agent GPT-3.5-Turbo setup outperformed GPT-4 on reasoning datasets. Similarly, Chen et al. (2024) proposed RECONCILE, a framework where LLMs collaboratively refine answers and explanations, achieving significant improvements over single-agent systems. Li et al. (2024) extended this line of research by optimizing agent connections, showing that sparse networks can maintain performance while reducing computational overhead.

Although these studies reveal the potential of multi-LLM approaches, their focus remains on structured reasoning tasks, such as question answering and fact-checking. They have not been

adequately explored in the context of long-text summarization, where the challenges include synthesizing distributed information, addressing content imbalances, and preserving the coherence of summaries across extended texts.

We hope to bridge this gap by adapting multiLLM frameworks to the domain of long-document summarization, addressing limitations of both single LLM and traditional hierarchical techniques, and positioning multi-LLM summarization as a promising solution for long-form content.

## Visual Content

### Page Preview

![Page 6](/projects/llms/images/MultiLLM_Text_Summarization_page_6.png)
