# Page 2

## Page Information

- **Type**: figure_page
- **Word Count**: 726
- **Has Tables**: False
- **Has Figures**: False

## Content

# Page 2

## I. INTRODUCTION

Large language models (LLMs) possess billions of parameters and have been trained on extensive corpora using training strategies like instruction tuning [1] [2] and Direct Preference Optimization(DPO) [3], enabling them to exhibit powerful reasoning and semantic representation capabilities, thereby advancing AI intelligence closer to human levels. Undoubtedly, LLMs currently serve as the foundation model for NLP tasks [4] [5] [6], showcasing strong generalization abilities to handle various NLP tasks such as question answering [7] [8], machine translation [9], code generation [10] [11], etc. LLMs have demonstrated extensive common knowledge and robust semantic comprehension abilities, fundamentally transforming existing text-processing workflows. While initially designed for text data, LLMs are increasingly being utilized for tasks

Fig. 1: Illustration of the LLM-GGA domain. LLM-GGA domain includes three principal components: LLM-based graph query processing (LLM-GQP), which necessitates the melding of graph analytics techniques and LLM prompts for query processing; LLM-based graph inference and learning (LLM-GIL), focusing on learning and reasoning over graphs; Graph-LLMbased applications that employ the graph-LLM framework to address non-graph tasks, such as recommendation systems.

<!-- image -->

beyond language processing, aiming to leverage the robust capabilities of LLMs across different tasks, showcasing superior performance.

Graphs, as structured data, play a crucial role in various realworld application scenarios, including the citation networks [12], social networks [13], molecular graphs [14], web links [15], and to name a few. Various graph analytics tasks have been studied to show their usefulness, e.g., node classification, link prediction, subgraph mining, influence maximization, and so on. Their versatility and ability to capture complex relationships have made graphs indispensable tools in academic research and industry platforms. Recently, one kind of graphbased learning model, graph neural network (GNN) [16] [17], has been widely studied and applied to solve challenging graph tasks. The GNN models utilize recursive message passing [18] and aggregation mechanisms [19] among nodes to derive representations of nodes, edges, or entire graphs, which have been used for various downstream tasks. This is thanks to the strong ability of GNN models to capture both graph structure and node features. However, GNNs exhibit weak generalization capabilities [20] [21] [22], requiring retraining for different graph tasks and showing limited transfer ability.

In other words, no universal graph foundation model could be easily generalized to handle various types of graph tasks.

Therefore, whether LLMs' powerful reasoning, semantic representation, and generalization capabilities can be applied to address graph tasks, leading to the inspiration of a graph foundation model, is the core of current efforts in leveraging existing large language models for graph-related tasks. In one word, can LLMs solve graph data tasks? More specifically, we study three detailed questions: (a) what specific graph tasks can LLMs answer? (b) How do LLMs tackle these tasks? (c) What is the effectiveness of LLM-based methods in solving these tasks compared with the existing graph-based approaches?

To address the above question, this survey conducts a comprehensive study of existing relevant work on graph analytics and LLMs, focusing on exploring the key issue of the LLM-based generative graph analytics (LLM-GGA) field. Drawing from a thorough investigation of the LLM-GGA domain, we offer a structured and methodical analysis that delineates the field into three principal components: LLMbased graph query processing (LLM-GQP), which necessitates the melding of graph analytics techniques and LLM prompts for query processing; LLM-based graph inference and learning (LLM-GIL), focusing on learning and reasoning over graphs; and lastly, graph-LLM-based applications that employ the graph-LLM framework to address non-graph tasks, such as recommendation systems. The framework is shown in Figure

1.

We categorize these three main components into a total of six directions to provide a guideline for researchers to conduct more in-depth studies. LLM-GQP includes graph understanding and KG-based augmented retrieval directions. LLM-GIL covers graph learning, graph-formed reasoning, and graph representation directions. The sixth direction is graphLLM-based applications. The following section details these six directions:

- · Graph understanding tasks. This research direction is studying whether LLMs can solve graph algorithm problems, exploring whether LLMs can comprehend graph structures to conduct graph mining and graph search. Current methods have primarily explored LLMs' understanding of graph structures, such as shortest path, clustering coefficient computation [23] [24], and more complex problems like maximum flow and Hamilton path [25] [26] [27]. Two main methods are introduced: prompting and supervised fine-tuning (SFT). The prompting methods explore the LLM's current structural understanding ability through query processing. Meanwhile, SFT methods enhance LLMs' structure understanding capability by tuning it on specific graph datasets. However, many more tasks are yet to be explored, such as the community search, keyword search, subgraph pattern mining, and other NP-hard complex graph problems [28] [29].
- · Graph learning tasks. This direction explores whether LLMs can combine graph structure and attributes for learning, extracting features of nodes, edges, and graphs,
- and understanding the semantic information of graphs, for example, tasks like node classification, graph classification, and GQL generation [30] [31] [32] [33]. There are two main pipelines: LLM-GNN pipelines and LLM pipelines. LLMs can leverage their powerful reasoning ability and vast knowledge repository to enhance GNNs and also can predict results directly.
- · Graph-formed reasoning. This direction explores how LLMs use graph structures to simulate human thinking during reasoning [34] [35] [36], enabling them to solve more complex reasoning problems such as algorithmic, logical, and mathematical tasks. Graph-formed reasoning involves two types of reasoning: think on the graph and verify on the graph. Think on the graph refers to LLMs deriving the final conclusion through the graph structure. Verify on the graph refers to verifying the correctness of the LLMs' intermediate or final outputs through the graph structure.
- · Graph representation. This direction explores enhancing graph representation with LLMs, particularly for Text Attribute Graphs (TAGs). LLMs' strong text representation capabilities allow text embeddings to capture deeper semantic nuances. However, the key challenge in this area remains how to capture and integrate graph structure into graph representation effectively [37] [38] [39]. There are three forms of graph representation: graph embedding, graph-enhanced text embedding, and graph-encoded prompts. Graph embedding methods transform a graph into a sequential format for LLM processing. Graphenhanced text embedding methods integrate structure into text embedding, where the integration method can be concatenation. Graph-encoded prompts focus on the way a graph is described within prompts.
- · Knowledge Graph (KG) based augmented retrieval. This direction investigates the relationship between LLMs and Knowledge Graphs (KGs). With the emergence of LLMs, discussions have arisen regarding the potential replacement of KGs [40] [41] [42] [43]. Consequently, this paper discusses the limitations of LLMs in processing factual knowledge, evaluates strategies for improving LLM efficacy via KG-based augmented retrieval, and investigates potential avenues for future advancements in this field.
- · Graph-LLM-based applications. This part explores the tasks where graph-LLM-based methods can be applied for useful downstream application [44] [45] [46], such as recommendation systems, conversational understanding, and so on.

We comprehensively analyze these six research directions of LLM-GGA to provide valuable definitions and highlighted methodologies. We also highlight the pros and cons of these methods and showcase future directions. To further explore the capabilities of LLMs reliably, this paper uses the prompting method to test the effectiveness of LLMs in tasks such as graph structure understanding, graph learning, and graph-

formed reasoning. Details of the prompts and results obtained during testing are also provided. Additionally, we refine and compile commonly used and effective prompts for graphrelated tasks, assisting researchers in conducting experiments. Furthermore, this paper also organizes and introduces the code for existing popular methods, benchmarks for LLM-GGA tasks, and evaluations measuring LLM performance in graph tasks to facilitate future research.

Our contributions and the identified challenges for future research. In this paper, we provide a comprehensive survey of the state-of-the-art work on LLMs applied to graph data. We begin by delineating six critical directions in the field of LLMGGA: graph structure understanding, graph learning, graphformed reasoning, graph representation, KG-based augmented retrieval, and graph-LLM-based applications. This categorization clarifies the current work and offers a guideline for future research endeavors. In each direction, we propose a structured introduction and summarization using vivid examples and offer suitable specific pipelines. We analyze the advantages and limitations of current methodologies and suggest avenues for future research. Furthermore, we organize resources related to benchmarks, evaluations, and code links within the LLMGGA domain to facilitate further investigation by researchers. Lastly, we identify the fundamental challenges in the LLMGGA field, which are the primary obstacles to advancing LLM in solving graph tasks, including the fundamental issue of how sequential LLM handles structural graph data, the efficiency issue of large-scale graph data, and the NP-hard problems of complex graph analytics. This clarification guides the research direction for future work on LLM-GGA.

Roadmaps . The organization of this paper is as follows. We first present the fundamental preliminaries and summarize the graph description language, which converts graphs into sequences before inputting them into LLMs in Section II. Then, we introduce six tasks of LLM-based graph analytics one by one. We present the graph structure understanding direction in Section III, graph learning direction in Section IV, graph-formed reasoning in Section V, graph representation in Section VI, KG-based augmented retrieval in Section VII and graph-LLM-based applications in Section VIII. In the above six directions, we clarify the tasks that LLMs can perform, discuss the methodologies, conduct a comparative analysis, and propose guidelines and principles in this direction. Following this, Section IX introduces the popular datasets and new datasets for solving the above tasks and also provides metrics for evaluating LLMs or tasks in different directions. In Section X, we identify and discuss the current and upcoming challenges that LLM-GGA faces and future directions. Finally, our conclusions are presented in Section XI.

## Visual Content

### Page Preview

![Page 2](/projects/llms/images/A_Survey_of_Large_Language_Models_on_Generative_Graph_Analytics_Query_Learning_and_Applications_page_2.png)
