# Page 4

## Page Information

- **Type**: figure_page
- **Word Count**: 514
- **Has Tables**: False
- **Has Figures**: True

## Content

# Page 4

## 1 Introduction

Deep neural networks (DNNs) have achieved unprecedented success in many artificial intelligence fields, such as computer vision, natural language processing and speech recognition (He et al. 2016; Conneau et al. 2016; Yu and Deng 2014). Despite their current popularity and prosperity, DNNs are still facing several severe challenges, especially

their high vulnerability to adversarial attack (Goodfellow, Shlens, and Szegedy 2014; Carlini and Wagner 2017), which adds well-designed tiny perturbations to the legitimate inputs to cause the intended misclassification of DNN models. Such attacks could cause severe safety, economic and social problems if launched to the DNNs deployed in practical applications ranging from face recognition, autonomous driving to speech authentication.

Figure 1: Adversarial images generated with CAG using ImageNet dataset. From top row to bottom row: legitimate images, adversarial images, perturbations (enhanced).

<!-- image -->

In order to address this critical challenge, the machine learning community has conducted extensive researches on the vulnerability of DNNs, from both the attack and defense aspects. Adversarial attack technique was pioneered by Szegedy et al. (2013). Since then, researchers have developed various adversarial attacking algorithms, targeting different types of DNN models including convolutional neural networks, recurrent neural networks and graph neural networks, and also different application scenarios, ranging from image classification, machine translation, to graph classification etc. Among those algorithms, one popular class of attack techniques is fast gradient sign method (FGSM), which performs one-step gradient computation to craft untargeted adversarial examples (Goodfellow, Shlens, and Szegedy 2014). Considering the relatively weak attack performance of FGSM, the machine learning community has proposed several iterative optimization-based techniques including C&W, I-FGSM and PGD that deliver the stateof-the-art attack performance (Carlini and Wagner 2017; Goodfellow, Shlens, and Szegedy 2014; Madry et al. 2017). Furthermore, some recent work has also proposed to use generative models, e.g., GAN and U-Net, to generate ad-

versarial examples (Poursaeed et al. 2018; Xiao et al. 2018; Goodfellow et al. 2014; Ronneberger, Fischer, and Brox 2015).

Although the existing adversarial attack methods can already exhibit high attack success rate (ASR), especially in white-box attack scenario, from the perspective of practical deployment, they are still suffering one or more drawbacks, namely long adversarial example generating time, high memory cost for launching adversarial attack, insufficient robustness against defense methods and low transferability in black-box attack scenario.

Aiming to overcome these drawbacks, in this paper we propose a Content-aware Adversarial Attack Generator (CAG), to achieve real-time, low-cost, enhanced-robustness and high-transferability adversarial attack. We show some adversarial images generated by CAG in Figure 1. The features and benefits of CAG are summarized as follows:

- 路 CAG is a generative model-based attack, so it can avoid time-consuming iterative optimization procedure to generate adversarial examples. Compared with the state-ofthe-art iterative attacks such as PGD and C&W, CAG achieves significant speedup (at least 500 times), and hence makes real-time attack possible.
- 路 CAG utilizes a trainable embedding layer to encode all label information to one single model, unlike prior generative model-based methods which require different generative models for different targeted classes. In n -class targeted attack scenario, the number of the required generative models is reduced from n to 1 , thereby drastically reducing the memory cost for launching attacks.
- 路 CAG integrates the class activation maps (CAMs) information into the training process, in contrast to many other attack methods that generate adversarial perturbations over the entire input. Consequently, CAG is able to generate adversarial perturbations that focus on the critical areas of input, and thus improves the attack's robustness against the state-of-art defense approaches.
- 路 CAG exhibits high transferability across different DNN classifier models in black-box attack scenario. CAG can generate adversarial perturbations with better generality by introducing random dropout in the perturbationsgeneration process. As a result, CAG's adversarial examples have higher transferability when attacking unseen classifiers.

The rest of this paper is organized as follows. Section 2 introduces the related work on adversarial attack and defense methods. Section 3 discusses our motivation. Section 4 describes the technical details of CAG. The experimental results are presented and analyzed in Section 5. Section 6 draws the conclusions of all findings in our paper.

## Visual Content

### Page Preview

![Page 4](/projects/llms/images/CAG_A_Realtime_Lowcost_Enhancedrobustness_Hightransferability_Contentaware_Adversarial_Attack_Genera_page_4.png)

### Figures

![](/projects/llms/figures/CAG_A_Realtime_Lowcost_Enhancedrobustness_Hightransferability_Contentaware_Adversarial_Attack_Genera_page_4_figure_1.png)

