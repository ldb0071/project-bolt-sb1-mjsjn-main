# Page 19

## Page Information

- **Type**: figure_page
- **Word Count**: 514
- **Has Tables**: False
- **Has Figures**: True

## Content

# Page 19

## GQL Generation (e)

Given <graph>, the director who directs Inception also direct what? Use Cypher to answer.

<!-- image -->

Fig. 8: Graph Learning tasks.

issues, such as GoT [59], which can help address more intricate graph tasks like generating GNN frameworks, k-truss tasks, kd-core tasks, etc. The second direction is API call prompts. Inspired by ToolFormer [58], LLMs can be trained as agents to utilize tools for graph tasks that are hard to solve. However, current API call prompt methods [59] utilize LLMs not as agents but solely to convert user queries into API command strings for processing by subsequent programs, exemplified in Prompt III-5 .

However, compared to prompting methods, fine-tuning LLMs with graph data seems a better way to enhance their understanding of graph structures. There are two mainstream methods for fine-tuning LLMs: Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF) [6]. SFT helps LLMs understand prompts and generate meaningful responses. However, SFT only offers a single humanwritten response for each prompt, whereas RLHF provides detailed human feedback through pairwise comparison labeling. Furthermore, to address the instability issue in PPO [68] training, the Reward Ranked Fine-Tuning (RAFT) [69] can also be attempted which requires online interaction. For offline algorithms, methods like DPO [3] and Preference Ranking Optimization (PRO) [70] can also be utilized for training

LLMs.

## Visual Content

### Page Preview

![Page 19](/projects/llms/images/A_Survey_of_Large_Language_Models_on_Generative_Graph_Analytics_Query_Learning_and_Applications_page_19.png)

### Figures

![](/projects/llms/figures/A_Survey_of_Large_Language_Models_on_Generative_Graph_Analytics_Query_Learning_and_Applications_page_19_figure_1.png)

