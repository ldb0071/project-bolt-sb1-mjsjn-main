# Page 18

## Page Information

- **Type**: table_page
- **Word Count**: 304
- **Has Tables**: False
- **Has Figures**: False

## Content

# Page 18

## Algorithm 1 Anomaly-Attention Mechanism (multi-head version).

Input: R N × d model : input; = ( ( j i ) 2 ) R N × N : relative distance matrix

```
X ∈ D -i,j ∈{ 1 , ··· ,N } ∈ Layer params: MLP input : linear projector for input; MLP output : linear projector for output 1: Q , K , V , σ = Split ( MLP input ( X ) , dim=1 ) glyph[triangleright] Q , K , V ∈ R N × d model , σ ∈ R N × h 2: for ( Q m , K m , V m , σ m ) in ( Q , K , V , σ ) : glyph[triangleright] Q m , K m , V m ∈ R N × d model h , σ m ∈ R N × 1 3: for σ m = Broadcast ( σ m , dim=1 ) glyph[triangleright] σ m ∈ R N × N 4: for P m = 1 √ 2 πσ m exp ( -D 2 σ 2 m ) glyph[triangleright] P m ∈ R N × N 5: for P m = P m / Broadcast ( Sum ( P m , dim=1 ) ) glyph[triangleright] Rescaled P m ∈ R N × N 6: for S m = Softmax ( √ h d model Q m K T m ) glyph[triangleright] S m ∈ R N × N 7: for ̂ Z m = S m V m glyph[triangleright] ̂ Z m ∈ R N × d model h 8: ̂ Z = MLP output ( Concat ([ ̂ Z 1 , · · · , ̂ Z h ] , dim=1 ) ) glyph[triangleright] ̂ Z ∈ R N × d model 9: Return ̂ Z glyph[triangleright] Keep the P m and S m , m = 1 , · · · , h
```

## Visual Content

### Page Preview

![Page 18](/projects/llms/images/2110.02642v5_page_18.png)
