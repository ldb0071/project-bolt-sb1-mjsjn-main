# Page 27

## Page Information

- **Type**: citation_rich
- **Word Count**: 901
- **Has Tables**: False
- **Has Figures**: False

## Content

# Page 27

## Keyword Counting (e)

Count the frequency of how many times each country is explicitly named in the input text. You can generate any intermediate lists and states, but the final output should only contain the frequency of each country that appears at least once in the following json format, prefixed with 'Output: ' (make sure to keep the same spelling for each country in the output as in the input text): {{ 'country1': frequency1, 'country2': frequency2, ... }}

<!-- image -->

Graph Reasoning

<!-- image -->

<!-- image -->

Merge the following 4 NDA documents - into a single NDA, maximizing retained information and minimizing redundancy. Output only the created NDA between the tags and , without any additional text. Here are NDAs: [four documents]

<!-- image -->

<!-- image -->

Question triplets: ('Hypocrite', directed by, $1), ($1, death date, $2) Question: When did the director of film Hypocrite (Film) die? To answer this question, we answer the following subquestions: (1) Who directed Hypocrite (Film)? (2) When did Miguel Morayta die?

Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?

<!-- image -->

- · Premises:
- 1. It is not true that some giant language models do not have good performance.
- 2. All language models with good performance are used by some researchers.
- 3. If a language model is used by some researchers, it is popular.
- 4. If BERT is a giant language model, then GPT-3 is also a giant language model.
- 5. BERT is a giant language model.

· Hypothesis: GPT -3 is popular. Give hypothesis label, true or false.

Fig. 16: Graph-formed Reasoning Tasks.Fig. 17: Illustration of human logical derivation. [35]

<!-- image -->

parameters compared to LLM-GNN and LLM pipelines; however, they have a shallower semantic understanding of graph attributes. In LLM pipelines, LLMs need to undergo alignment tuning before they can be used for various downstream tasks. In LLM-GNN pipelines, there is a general trend of training GNNs. Combining LLM-GNN and graph prompts is possible because graph prompts are designed for GNNs through prompt engineering and can be applied to LLM-GNN pipelines. By leveraging LLM's robust semantic representation capabilities and the lightweight fine-tuning of graph prompts, similar results can be achieved.

Classical graph tasks, such as node classification on attributed static networks, have recently obtained the most attention. However, there is potential for more complex tasks in the future, such as predicting graph evolution on dynamic graphs. Leveraging LLM models that are suitable for handling sequential data and can process time series data, along with GNNs that are adept at capturing changes in graph structures, can help address a broader range of problems effectively. By combining the strengths of LLM and GNN, we can tackle more challenging tasks in the field of graph analysis.

## Visual Content

### Page Preview

![Page 27](/projects/llms/images/A_Survey_of_Large_Language_Models_on_Generative_Graph_Analytics_Query_Learning_and_Applications_page_27.png)
