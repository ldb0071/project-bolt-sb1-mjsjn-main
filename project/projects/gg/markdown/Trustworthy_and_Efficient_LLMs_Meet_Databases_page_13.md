Page 13

Xupeng et al. [194] presented a tutorial at SIGMOD 2024 about the role of data management in the development (training, /finetuning) and deployment (inference) of LLMs. It focused on how the

knowledge is encoded into model parameters and extracted during the inference, and explained the concept of KV caching but not LLM inference systems. Trummer [272] presented at VLDB 2023 about Transformer architecture, pre-training//fine-tuning/prompting in LLMs, and LLM applications in data management. As pointed out by [194], most of other tutorials presented at SIGMOD and VLDB about AI and databases focused on traditional machine learning and deep learning tasks not tailored to LLMs [31, 70, 156, 165, 201, 207, 290], or speci/fic LLM-related applications such as tabular data understanding [12] and queries with natural languages [132, 133]. Dong et al. [68] presented at SIGKDD 2023 about the role of LLMs in building intelligent AR/VR assistants.

In this tutorial, we will focus on more recent, general approaches to enhance the trustworthiness and e/fficiency of LLMs, which have not been addressed in previous tutorials. For trustworthiness, we will start with enhancing LLMs alone, LLMs with tools, and agentic LLMs and collaboration. For e/fficiency, we will explain how LLM inference systems resemble DBMSs. Then we will discuss how we can integrate LLMs and databases in depth. We expect that these are what researchers, who aim to use LLMs in their applications or optimize LLMs using database techniques, need to know about. Instead of a common analogy that LLMs are knowledge bases as they generate plausible facts, we will use analogies that LLMs behave as DBMSs and improve as how humans solve challenging problems.

## References

- [1] Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kau/ffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone. CoRR abs/2404.14219 (2024). https://doi.org/10.48550/ARXIV.2404.14219 arXiv:2404.14219
- [2] Amey Agrawal, Junda Chen, Íñigo Goiri, Ramachandran Ramjee, Chaojie Zhang, Alexey Tumanov, and Esha Choukse. 2024. Mnemosyne: Parallelization Strategies for E/fficiently Serving Multi-Million Context Length LLM Inference Requests Without Approximations. CoRR abs/2409.17264 (2024). https://doi.org/10.48550/ARXIV.2409.17264 arXiv:2409.17264
- [3] Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav S. Gulavani, Ramachandran Ramjee, and Alexey Tumanov. 2024. VIDUR: A Large-Scale Simulation Framework for LLM Inference. In Proceedings of the Seventh Annual Conference on Machine Learning and Systems, MLSys 2024, Santa Clara, CA, USA, May 13-16, 2024 , Phillip B. Gibbons, Gennady Pekhimenko, and Christopher De Sa (Eds.). mlsys.org. https://proceedings.mlsys.org/paper\_/files/paper/2024/hash/ b74a8de47d2b3c928360e0a011f48351-Abstract-Conference.html
- [4] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024. Taming Throughput-Latency Tradeo/ff in LLM Inference with Sarathi-Serve. In 18th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2024, Santa Clara, CA, USA, July 10-12, 2024 , Ada Gavrilovska and Douglas B. Terry (Eds.). USENIX Association, 117-134. https://www.usenix.org/ conference/osdi24/presentation/agrawal
- [5] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, and Ramachandran Ramjee. 2023. SARATHI: E/fficient LLM Inference

by Piggybacking Decodes with Chunked Pre/fills. CoRR abs/2308.16369 (2023). https://doi.org/10.48550/ARXIV.2308.16369 arXiv:2308.16369

- [6] Minseon Ahn, Thomas Willhalm, Norman May, Donghun Lee, Suprasad Mutalik Desai, Daniel Booss, Jungmin Kim, Navneet Singh, Daniel Ritter, and Oliver Rebholz. 2024. An Examination of CXL Memory Use Cases for In-Memory Database Management Systems using SAP HANA. Proc. VLDB Endow. 17, 12 (2024), 3827-3840. https://www.vldb.org/pvldb/vol17/p3827-ahn.pdf
- [7] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 4895-4901. https://doi.org/ 10.18653/V1/2023.EMNLP-MAIN.298
- [8] Peter Akioyamen, Zixuan Yi, and Ryan Marcus. 2024. The Unreasonable Effectiveness of LLMs for Query Optimization. arXiv preprint arXiv:2411.02862 (2024).
- [9] Uday Allu, Biddwan Ahmed, and Vishesh Tripathi. 2024. Beyond Extraction: Contextualising Tabular Data for E/fficient Summarisation by Language Models. CoRR abs/2401.02333 (2024). https://doi.org/10.48550/ARXIV.2401.02333 arXiv:2401.02333
- [10] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. 2024. BlackMamba: Mixture of Experts for State-Space Models. CoRR abs/2402.01771 (2024). https://doi.org/10.48550/ARXIV.2402.01771 arXiv:2402.01771
- [11] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through SelfRe/flection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net. https://openreview. net/forum?id=hSyW5go0v8
- [12] Gilbert Badaro and Paolo Papotti. 2022. Transformers for Tabular Data Representation: A Tutorial on Models and Applications. Proc. VLDB Endow. 15, 12 (2022), 3746-3749. https://doi.org/10.14778/3554821.3554890
- [13] Sourav Banerjee, Ayushi Agarwal, and Saloni Singla. 2024. LLMs Will Always Hallucinate, and We Need to Live With This. CoRR abs/2409.05746 (2024). https://doi.org/10.48550/ARXIV.2409.05746 arXiv:2409.05746
- [14] Matthew Barnett. 2024. An Empirical Study of Scaling Laws for Transfer. CoRR abs/2408.16947 (2024). https://doi.org/10.48550/ARXIV.2408.16947 arXiv:2408.16947
- [15] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-Document Transformer. CoRR abs/2004.05150 (2020). arXiv:2004.05150 https://arxiv.org/abs/2004.05150
- [16] Loubna Ben Allal, Anton Lozhkov, and Elie Bakouch. 2024. SmolLM - blazingly fast and remarkably powerful. https://huggingface.co/blog/smollm. Accessed: December 15, 2024.
- [17] Nathan Benaich and Ian Hogarth. 2024. State of AI Report 2024. https://www. stateof.ai.
- [18] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoe/fler. 2024. Graph of Thoughts: Solving Elaborate Problems with Large Language Models. In Thirty-Eighth AAAI Conference on Arti/ficial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Arti/ficial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Arti/ficial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada , Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (Eds.). AAAI Press, 17682-17690. https://doi.org/10.1609/AAAI.V38I16.29720
- [19] Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwasniewski, Jürgen Müller, Lukas Gianinazzi, Ales Kubicek, Hubert Niewiadomski, Onur Mutlu, and Torsten Hoe/fler. 2024. Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. CoRR abs/2401.14295 (2024). https://doi.org/10.48550/ARXIV.2401. 14295 arXiv:2401.14295
- [20] Luca Beurer-Kellner, Marc Fischer, and Martin T. Vechev. 2024. Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation. In Forty-/first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024 . OpenReview.net. https://openreview.net/forum?id=pXaEYzrFae
- [21] Altan Birler, Alfons Kemper, and Thomas Neumann. 2024. Robust Join Processing with Diamond Hardened Joins. Proc. VLDB Endow. 17, 11 (2024), 3215-3228. https://www.vldb.org/pvldb/vol17/p3215-birler.pdf
- [22] Asim Biswal, Liana Patel, Siddarth Jha, Amog Kamsetty, Shu Liu, Joseph E. Gonzalez, Carlos Guestrin, and Matei Zaharia. 2024. Text2SQL is Not Enough: Unifying AI and Databases with TAG. CoRR abs/2408.14717 (2024). https: //doi.org/10.48550/ARXIV.2408.14717 arXiv:2408.14717
- [23] Quinten Bolding, Baohao Liao, Brandon James Denis, Jun Luo, and Christof Monz. 2023. Ask Language Model to Clean Your Noisy Translation Data. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 3215-3236. https://doi.org/10.18653/V1/ 2023.FINDINGS-EMNLP.212

| [24] Angela Bonifati, Stefania Dumbrava, George Fletcher, Jan Hidders, Matthias Hofer, Wim Martens, Filip Murlak, Joshua Shinavier, Slawek Staworko, and                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Dominik Tomaszuk. 2023. Threshold Queries. SIGMOD Rec. 52, 1 (2023), 64-73. https://doi.org/10.1145/3604437.3604452                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| [25] Sebastian Borgeaud, Arthur Mensch, Jordan Ho/ffmann, Trevor Cai, Eliza Ruther-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| International USA (Proceedings of Machine Learning Research, Vol. 162) , Kamalika Chaudhuri,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Sa/ffron Huang, Loren Maggiore, Chris Jones, Albin Cas- sirer, Andy Brock, Michela Paganini, Geo/ffrey Irving, Oriol Vinyals, Simon Osin- dero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improv- ing Language Models by Retrieving from Trillions of Tokens. In Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,                                                                                              |
| Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Tong Wang, Samuel Marks, Charbel-Raphaël Ségerie, Micah Carroll, Andi Peng, Phillip J. K. Christo/ffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca D. Dragan, David Krueger, Dorsa Sadigh, and Dylan Had/field-Menell. 2023. Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. Trans. Mach. Learn. Res. 2023 (2023). https://openreview.net/forum? id=bx24KpJ4Eb |
| Computational Linguistics, 693-704. https://aclanthology.org/2024.acl-short.64 [33] Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen Zhao, Samuel R. Bowman, and Kyunghyun Cho. 2024. Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs. Trans. Mach. Learn. Res. 2024 (2024). https://openreview.net/forum?id=5nBqY1y96B                                                                                                                                                                                                                                                                                   |
| Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. CoRR abs/2310.14735 (2023). https://doi.org/10.48550/ ARXIV.2310.14735 arXiv:2310.14735 [35] Huaben Chen, Wenkang Ji, Lufeng Xu, and Shiyu Zhao. 2023. Multi-Agent Consensus Seeking via Large Language Models. CoRR abs/2310.20151 (2023).                                                                                                                                                                                                                                                                                                       |
| https://doi.org/10.48550/ARXIV.2310.20151 arXiv:2310.20151 [36] Jiaao Chen, Derek Tam, Colin Ra/ffel, Mohit Bansal, and Diyi Yang. 2023. An Empirical Survey of Data Augmentation for Limited Data Learning in NLP. Trans. Assoc. Comput. Linguistics 11 (2023), 191-211. https://doi.org/10.1162/                                                                                                                                                                                                                                                                                                                                                 |
| Model on Knowledge Graphs. CoRR                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| CoRR                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| abs/2410.23875 (2024). https://doi.org/10. FrugalGPT: How to                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| arXiv preprint arXiv:2403.02419 (2024).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| [37] Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou. 2024. Are more llm calls all you need? towards scaling laws of compound inference systems.                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| [38] Liyi Chen, Panrong Tong, Zhongming Jin, Ying Sun, Jieping Ye, and Hui Xiong. 2024. Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| 48550/ARXIV.2410.23875 arXiv:2410.23875 [39] Lingjiao Chen, Matei Zaharia, and James Zou. 2023.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| mance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| abs/2305.05176 (2023). https://doi.org/10.48550/ARXIV.2305.05176                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| TACL\_A\_00542                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| Use Large Language Models While Reducing Cost and Improving Perfor-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |