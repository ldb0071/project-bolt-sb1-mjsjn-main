Page 12

To balance e/fficiency and accuracy, during the physical query optimization we should select proper models (ones used for execution) to avoid calling heavy LLMs unnecessarily. Depending on the complexity of the task, simple ML models with a small set of supervised data [123], or larger deep generative models such as in tabular foundation models tailored to domain-speci/fic data [141, 160, 308], or LLMs with world knowledge and reasoning capacity [296] can /fit the task with di/fferent accuracy-e/fficiency trade-o/ffs.

Small language models (SLMs) [1, 16, 67, 189, 196, 202, 226] are also a good choice. Automatically /finding the best prompt con/figuration [136, 280] tailored to the mixed workloads and more (e.g., previously mentioned /fine-tuning or multi-hop/multi-path reasoning with adaptivity during inference) might be desired.

Furthermore, unlike the TPC benchmarks for databases, another problem is that there is no comprehensive benchmark for relationalLLM workloads yet. [22, 182] provide exploratory benchmarks without focusing on semantic joins.

- 2.3.5 DBs with LLMs: Integrated System. Other than the cost models, we also need DBMSs with native LLM support to increase the optimization opportunities, alike systems for relational-vector workloads [242, 327]. Current prototypes for relational-LLM workloads [177, 182, 216] separate table processing (e.g., pandas [192]) and LLM inference engine (e.g., /v.scLLM [146]).

To maximize e/fficiency and scalability, we should focus on hardware utilization, data movement [122], caching hot data, locating computations close to data (e.g., computation pushdown in storageaggregation setting) [84, 191], asynchronous API calls [95], balancing loads, and multi-tenancy just like in DBMSs [248, 310]. One also has to decide whether to maintain a separate vector database for faster online vector retrievals, or use just-in-time vectorization for reducing storage overhead. This also applies to precomputing KVs of data tokens [184] for faster LLM inferences or not, but with a higher caution as KVs are typically larger than vectors.

- 2.3.6 Convergence and Future. We envision LLMs and databases to converge (e.g., neuro-symbolic systems [48, 79, 268, 321]), more than just applying the techniques from one domain to another. A new LLM inference system optimized for DBMSs might be developed from an open-source cloud DBMS, utilizing recent implementations and optimizations for processing relational operators, such as storage-disaggregation and computation pushdown for scalable data and model management [310], GPU-based OLAP processing [86, 219, 220] for the full use of GPUs for both relational operators and LLMs, hybrid operators with heterogeneous data transfer paths [53, 310], adaptive query execution [307] and more. A uni/fied query optimizer and data model for both relational data, KVs, and model weights, could o/ffer opportunities for better data management and hardware utilization. Finally, if we look into the near future, we could also harness the emerging CXL technology for memory disaggregation [6, 148, 180] to manage model weights and KVs, and increased interest in pruning unnecessary data in OLAP [21, 24, 174, 206] could lead to higher trustworthiness (due to reduced noise) and e/fficiency (due to less data to process) in the relational-LLM workloads, with connections to online aggregation [254] and incremental view maintenance [28].

Target. The audience will understand the di/fferent depths of LLMdatabase integrations and be able to /find interesting research topics from each of the integration, which are closely related to the current and near-future trends of databases.

## 3 Related Tutorial