Page 6

However, LLMs experience hallucinations [13, 124, 270, 305], generating plausible-sounding but incorrect or fabricated information. This is an unavoidable aspect of LLMs [13, 305] which arises from limitations in capturing real-world knowledge, inherent approximations in training and inference, input noise, etc. Even slight input perturbations can signi/ficantly in/fluence hallucinations [65, 101], and the detection of hallucinations has been a major problem [47, 54, 77, 195, 204, 233, 264].

Additionally, the lost-in-the-middle problem [117, 181] indicates that LLMs may struggle to utilize information located in the middle of long contexts, often performing better when relevant information is at the beginning or end of the input, exhibiting a U-shaped performance curve. This phenomenon has been attributed to inherent attention biases within LLMs, where tokens at the start and end of the input receive higher attention, regardless of their relevance [117]. This tendency can lead to increased hallucinations as context lengthens [230].

Scaling laws [113, 130, 221] explain that error rates decrease as model size and training data increase, with optimal scaling requiring proportional growth in both [113]. However, this may not hold for smaller models [221]. Laws can also relate to temporal loss in the training curve [297], downstream tasks [121], model quantization [306], transfer learning [14, 111], number of generated samples [27], and inference time [205] with the advance of using long, complex reasoning paths. Due to automatic prompting techniques [43, 136, 255, 302, 330] and that larger models tend to be less sensitive to prompt variations [75], we focus less on prompting techniques.

Target. The audience will distinguish pre-training, /fine-tuning, and in-context learning phases of LLMs, and understand the inherent challenges in making LLMs trustworthy.

2.1.2 Improving Bare LLMs. We brie/fly explain the approaches to improve the LLM itself to make it more trustworthy. Since LLM

is a speci/fic class of machine learning (ML) models, general ML approaches to enhance accuracy may work for LLMs. However, as such approaches have been extensively studied from the classic ML era, we target more LLM-speci/fic approaches.

As it is infeasible to increase the model size inde/finitely, and the models typically follow the Transformer architecture [274], e/fforts have been put to increase or augment training data (where LLMs themselves can be used to generate data) [36, 64, 82, 88, 142, 157, 163, 235, 241, 279, 341], improve data quality (again, LLMs can be used to clean data) [23, 66, 72, 103, 143, 328], make inferences more robust [91, 276], and apply better training and /fine-tuning methods.

Speci/fically, /fine-tuning covers a broad spectrum of work for example, parameter-e/fficient /fine-tuning (PEFT) [116, 119, 152, 162, 222], instruction tuning [51, 198, 199, 243, 285, 288], reinforcement learning from human feedback (RLHF) [30, 52, 57, 81, 102, 134, 167, 208, 250, 262], and direct preference optimization (DPO) [83, 138, 234, 338]. RLHF leverages human feedback to train a reward model in reinforcement learning (RL), guiding the LLM through RL to produce desired outputs. DPO simpli/fies the alignment process by directly optimizing the policy model without a separate reward model. While these approaches rely on RL that continuously interacts with human or the world external to LLMs, such interactions are often limited to training and do not occur in inferences, which we explain in the following sections.