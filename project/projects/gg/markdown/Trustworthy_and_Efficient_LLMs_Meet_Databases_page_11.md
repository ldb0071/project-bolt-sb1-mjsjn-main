Page 11

This speculation and healing patterns also appear in speculative decoding [153] and model cascades [39, 177, 316, 325, 336], accelerating the generation of tokens by leveraging smaller, faster models then validate the tokens using larger models, since the validation costs less than the generation.

## 2.3 LLMs Meet Databases

The last part of the tutorial discusses the intersection between LLMs and databases, opportunities and challenges in how we can exploit LLMs for databases, how the development of databases can help LLMs, and how we can exploit new types of workloads and integrations of LLMs and databases. We explain from more wellknown to more untapped, deeper integrations in Sections 2.3.1-2.3.5 and provide more proactive visions in Section 2.3.6.

2.3.1 LLMs for DBs: DBAs and DBMS Internal. We brie/fly explain how LLMs are utilized for well-known tasks of DBAs and DBMS internals such as database tuning [271, 340], text2sql [151, 224] and query optimization [8, 168]. As we mentioned in Section 1, we will

not cover every detail, as many of these e/fforts are covered in a previous tutorial [194] and its additional list of papers [55].

- 2.3.2 DBs for LLMs: Adaptive Cost-based Scheduling. Unlike the sophisticated query optimizers in DBMSs, LLMs lack cost models and cost-based scheduling of LLM requests. [3] measures the batch times across various inputs (number of tokens to process and KV size to read). [322] computes batch times based on the roo/fline models. These can be used to model batch times and formulate the problem of /finding optimal schedules as a constrained satisfaction problem (CSP) [139]. While schedulers try to avoid preemptions to maximize performance, [139] shows that harnessing preemptions can rather reduce overall latency compared to zero-preemptions. As the exact hardware utilization of each request is not known in advance, the scheduling should be adaptive based on the observations, and it has not been explored much to schedule dependent requests connected via semantic variables or shared pre/fixes [139].

2.3.3 DBs with LLMs: Mixed Relational-LLM Workload. Not only solving existing tasks with LLMs, LLMs o/ffer new functionalities when integrated into DBMSs. Semantic operators [216] extend relational operators to batch-process the tabular data with LLMs (e.g., /filters and joins using LLMs), which can be regarded as an AQP. Workloads with LLMs provide a justi/fication to use LLMs inside DBMSs(heavy LLM calls in plan optimization can be negligible compared to query execution with LLMs). However, di/fferent pipelines (with semantic operators) lead to di/fferent accuracy and e/fficiency, thus de/fining the equivalence between two pipelines is non-trivial. Furthermore, more complex pipelines or LLM calls do not always guarantee higher accuracy [37, 74], and searching similar entities with LLMs can be replaced with e/fficient vector-similarity searches [177, 242] as a type of model cascade.

2.3.4 DBswith LLMs: Multi-objective /Q\_uery Optimization and Benchmarks. The challenge is therefore how we can automatically /find good pipelines for mixed relational-LLM workloads under the multiobjective of accuracy and e/fficiency [22, 273, 321] as in compound AI systems [95, 244, 267]. This calls for development of accurate cost models and accuracy-prediction models for LLMs and mixed relational-LLM workloads, in order to enable the holistic optimization of query plans consisting of both relational and non-relational operators. The cost model itself can be learned via LLMs (or any ML models), possibly using RLHF or feedback from query execution without human intervention, where such an automatic training data generation is one of the advantages of solving database tasks compared to conventional ML tasks (e.g., natural language processing with human-labeled translation data) [140]. Another model for predicting the output accuracy or detecting hallucination may be chosen from the scaling laws (using the general fact that larger models are more trustworthy) or separately trained.