Page 2

Inference e/fficiency is particularly important because, while training LLMs demands substantial resources and expertise, inference occurs daily across numerous users, leading to signi/ficant operational costs. For instance, OpenAI handles millions of requests, incurring substantial monthly expenses to run ChatGPT [73, 215]. Integrating LLMs with external data sources, such as vector databases and document retrieval systems in retrieval-augmented generation

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro/fit or commercial advantage and that copies bear this notice and the full citation on the /first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci/fic permission and/or a fee. Request permissions from permissions@acm.org.

Conference'17, July 2017, Washington, DC, USA

https://doi.org/10.1145/nnnnnnn.nnnnnnn

## Anastasia Ailamaki

anastasia.ailamaki@ep/fl.ch

EPFL

Switzerland