Page 3

(RAG) [154], increases the number and complexity of LLM calls, especially with longer inputs. Recent trends in chain-of-thought and multi-path reasoning, exempli/fied by models like OpenAI's o1 [205], further amplify inference demands, as generating /final answers may require multiple LLM calls to enhance trustworthiness.

From a systems perspective, improving LLM inference e/fficiency parallels database management system (DBMS) development, presenting opportunities for database researchers to contribute to creating more e/fficient LLMs, promoting economic and environmental sustainability by reducing the CO2 footprint associated with extensive GPU usage.

After introducing the essential ideas in making LLMs more trustworthy and e/fficient, we will explain the intersection of LLMs and databases with new challenges and opportunities.

## 1.1 Target Audience and Prerequisites

Our tutorial is designed for conference attendees, focusing on three key areas to maximize engagement:

Trustworthy LLMs (Section 2.1): Aimed at individuals seeking to e/ffectively utilize large language models (LLMs) in database tasks with minimal errors. Prerequisites include experience with LLMs like ChatGPT and the distinction between training and inference in machine learning. No in-depth knowledge of LLM internals is required.

E/fficient LLMs (Section 2.2): Targeted at those interested in enhancing LLM inference e/fficiency or contributing to the development of fast LLM inference systems by applying database techniques. Prerequisites include basic database knowledge and an understanding of GPUs. Familiarity with Transformer architecture, attention mechanisms, and key-value (KV) caching is advantageous.

LLMs Meet Databases (Section 2.3): Intended for participants exploring new research opportunities at the intersection of databases and LLMs. A background in databases, including OLAP, relational algebra, cost-based query optimization, and approximate/adaptive query processing, will be helpful.