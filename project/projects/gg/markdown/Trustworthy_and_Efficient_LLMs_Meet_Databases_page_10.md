Page 10

Signi/ficant e/fforts have been made to increase the e/fficiency of LLM inference [342], largely based on operating and database systems. O/r.sc/c.sc/a.sc [318] forms a new batch of requests after each iteration (of pre/fills and decodes) whenever resources are available. Thus, a new request does not have to wait for all current running requests to /finish, just like the pipelining in OS. /v.scLLM [146] adopts paging and virtual memory to manage KVs, reducing memory fragmentation and enlarging the batch size. Since pre/fills are typically more costly than decodes, making stalls for decodes when batched together, S/a.sc/r.sc/a.sc/t.sc/h.sc/i.sc [4, 5] chunks pre/fills to reduce pipeline bubbles. Some other work [217, 263, 339] rather disaggregates the pre/fills and decodes into di/fferent GPUs, so the workload for each GPU is homogeneous. /v.scT/e.sc/n.sc/s.sc/o.sc/r.sc [298] decouples the KV cache management and attention computation of /v.scLLM for better /flexibility and performance. N/a.sc/n.sc/o.scF/l.sc/o.sc/w.sc [343] splits each batch into nano-batches for /finer-grained pipelining, increasing the overlap of computation, memory operation, and data transfer between GPUs. It also hides CPU scheduling latency by asynchronous scheduling. I/n.sc/f.sc/i.sc/n.sc/i.scG/e.sc/n.sc [149] o/ffloads the KVs to CPU memory to extend the KV cache and reloads the KVs from CPU layer-wise, but fetches a subset of KVs for e/fficiency, similarly to sparse attentions (Section 2.2.3). I/n.sc/s.sc/t.scI/n.sc/hyphen.sc /f.sc/e.sc/r.sc [213] o/ffloads KVs and attention computations to /flash drives, just like the storage-disaggregation and computation pushdown in databases [310]. NEO [127] selectively o/ffloads attention computations and KVs from GPU to CPU, in order to maximize both GPU and CPU utilization.

Target. The audience will understand why LLMs behave similarly to DBMSs and how database techniques can improve LLM inference e/fficiency. In subsequent sections, the audience will learn about e/fforts and challenges in further improving e/fficiency in four dimensions: operation data, hardware, and workload.

2.2.3 Operation: A/t\_tention. While matrix multiplications take the major portion in LLM latency in general [5], attentions can dominate the runtime for large inputs due to their quadratic complexity. FlashAttention [59, 60, 249] has become a de facto standard as an e/fficient attention implementation, utilizing recent GPU technologies to boost the inference speed. The ideas include kernel operator fusion and GPU cache-aware KV transfer. As in approximate query processing (AQP) in databases, sparse attentions [15, 178, 186] do not compute the full attention scores for all preceding tokens but a subset as an approximation. Some attentions rather optimize for long contexts [2, 62]. FlexAttention [107] o/ffers /flexible and performant implementation of such attentions.

2.2.4 Data: KV and Model Weights. Reading KVs from GPU memory in decode-attentions is similar to sequential table scan. As KVs are maintained per each attention layer, reading KVs for a layer can overlap with other layers' operators [149, 263]. While o/ffloading KVs and attention computation have been popular recently [127, 149, 213], we need to be careful as it is challenging to predict the output lengths of LLM requests and thus their utilization patterns, and a KV for a single token may consume a few MBs. KVs of long documents can be precomputed, compressed, and fetched

for later retrievals [184]. To reduce memory latency, one can opt for KV sharing across di/fferent attention heads [7, 26, 50], KV compression [63, 129, 176, 184, 236], model quantization [94, 97, 106, 147, 164, 200, 251, 295, 306], or di/fferent model architectures than Transformer, such as State Space Models (SSMs) [61, 100] that do not rely on attentions, thereby not generating KVs. While hybrid architectures [10, 67, 99, 108, 171, 223, 237] can balance between the e/fficiency of SSMs and memorization capacity of Transformers, SSMs remain niche in the market [17]. A recent work even shows that tokenizers can be removed from the models [209].

2.2.5 Hardware: Theory and Practice. We brie/fly explain 1) the roo/fline model [322] and 2) some e/fforts to overcome the hardware limits [161, 238, 253, 261, 317, 333] or leverage advanced hardware for LLM inference [170, 304]. The roo/fline model is based on the computation speed (e.g., GPU FLOPS) and memory bandwidth, which acts as a theoretical hardware bound and determines whether an operator is compute-intensive or memory-intensive across di/fferent inputs.

2.2.6 Workload: Scheduling, Prefix Sharing, Speculation. To handle multiple LLM requests, LLM inference systems implement request scheduler to send LLM requests to appropriate machines or GPUs to maximize throughput or minimize latency. Assuming independent requests, early schedulers either prioritize pre/fills [146] or decodes [4], which tend to optimize latency or throughput, respectively. More complex schedulers consider fairness [252, 291] while compromising performance, or predict the output lengths of requests (not known in advance) and schedule shorter requests /first [85, 231, 337].

If di/fferent LLM requests can share a pre/fix in their inputs, the KVs of the pre/fix can be stored just once and reused for multiple requests [334, 335]. This forms a trie structure with shared pre/fixes. However, a single-token di/fference in inputs may invalidate the sharing of KVs of all subsequent tokens. To increase the sharing opportunity, [311] uses the KVs of multiple token sequences to approximate the KVs of the concatenated sequence. The mechanism is similar to the speculation in OLAP [260] and healing protocol in transactions [293] in databases.