Page 2

## [2,8,11,13,33,39,39,42,44,46,47,49,58-60,69,74] are un-

supervised, which assume the availability of normal training samples only, i.e ., anomaly-free training data, because it is difficult, if not impossible, to collect large-scale anomaly data. However, a small number of ( e.g ., one to multiple) labeled anomaly examples are often available in many relevant real-world applications, such as some defect samples identified during random quality inspection, lesion images confirmed by radiologists in daily medical screening, etc. These anomaly examples provide valuable knowledge about application-specific abnormality [30, 35, 37, 45], but the unsupervised detectors are unable to utilize them. Due to the lack of knowledge about anomalies, the learned features in unsupervised models are not discriminative enough to distinguish anomalies (especially some challenging ones) from normal data, as illustrated by the results of KDAD [47], a recent state-of-the-art (SotA) unsupervised method, on two MVTec AD defect detection datasets [3] in Fig. 1.

In recent years, there have been some studies [30, 35, 37,45] exploring a supervised detection paradigm that aims at exploiting those small, readily accessible anomaly datarare but previously occurred exceptional cases/events, a.k.a.

gray swans [23] - to train anomaly-informed detection models. The current methods in this line focus on fitting these anomaly examples using one-class metric learning with the anomalies as negative samples [30,45] or one-sided anomaly-focused deviation loss [35,37]. Despite the limited amount of the anomaly data, they achieve largely improved performance in detecting anomalies that are similar to the anomaly examples seen during training. However, these seen anomalies often do not illustrate every possible class of anomaly because i) anomalies per se are unknown and ii) the seen and unseen anomaly classes can differ largely from each other [36], e.g ., the defective features of color stains are very different from that of folds and cuts in leather defect inspection. Consequently, these models can overfit the seen anomalies, failing to generalize to unseen/unknown anomaly classes-rare and previously unknown exceptional cases/events, a.k.a. black swans [55], as shown by the result of DevNet [35, 37] in Fig. 1 where DevNet improves over KDAD in detecting the seen anomalies but fails to discriminate unseen anomalies from normal samples. In fact, these supervised models can be biased by the given anomaly examples and become less effective in detecting unseen anomalies than unsupervised detectors (see DevNet vs. KDAD on the Tile dataset in Fig. 1).

To address this issue, this paper tackles open-set supervised anomaly detection, in which detection models are trained using the small anomaly examples in an openset environment, i.e ., the objective is to detect both seen anomalies ('gray swans') and unseen anomalies ('black swans'). To this end, we propose a novel anomaly detection approach, termed DRA, that learns disentangled representations of abnormalities to enable the generalized detection. Particularly, we disentangle the unbounded abnormalities into three general categories: anomalies similar to the limited seen anomalies, anomalies that are similar to pseudo anomalies created from data augmentation or external data sources, and unseen anomalies that are detectable in some latent residual-based composite feature spaces. We further devise a multi-head network, with separate heads enforced to learn each type of these three disentangled abnormalities. In doing so, our model learns diversified abnormality representations rather than only the known abnormality, which can discriminate both seen and unseen anomalies from the normal data, as shown in Fig. 1.

In summary, we make the following main contributions:

- 路 To tackle open-set supervised AD, we propose to learn disentangled representations of abnormalities illustrated by seen anomalies, pseudo anomalies, and latent residual-based anomalies. This learns diversified abnormality representations, extending the set of anomalies sought to both seen and unseen anomalies.
- 路 We propose a novel multi-head neural network-based model DRA to learn the disentangled abnormality rep-

- sentations, with each head dedicated to capturing one specific type of abnormality.
- 路 We further introduce a latent residual-based abnormality learning module that learns abnormality upon the residuals between the intermediate feature maps of normal and abnormal samples. This helps learn discriminative composite features for the detection of hard anomalies ( e.g ., unseen anomalies) that cannot be detected in the original non-composite feature space.
- 路 We perform comprehensive experiments on nine realapplication datasets from industrial inspection, roverbased planetary exploration and medical image analysis. The results show that our model substantially outperforms five SotA competing models in diverse settings. The results also establish new baselines for future work in this important emerging direction.

## 2. Related Work

Unsupervised Approaches . Most existing anomaly detection methods, such as autoencoder-base methods [13, 19, 39, 72, 74], GAN-base methods [40, 46, 49, 69], selfsupervised methods [2, 11, 12, 26, 51, 57, 61], and one-class classification methods [7,8,41,44], assume that only normal data can be accessed during training. Although they do not have the risk of biasing towards the seen anomalies, they are difficult to distinguish anomalies from normal samples due to the lack of knowledge about true anomalies.

Supervised Approaches . A recently emerging direction focuses on supervised (or semi-supervised) anomaly detection that alleviates the lack of anomaly information by leveraging small anomaly examples to learn anomaly-informed models. This is achieved by one-class metric learning with the anomalies as negative samples [14, 30, 34, 45] or onesided anomaly-focused deviation loss [35,37,71]. However, these models rely heavily on the seen anomalies and can overfit the known abnormality. A reinforcement learning approach is introduced in [38] to mitigate this overfitting issue, but it assumes the availability of large-scale unlabeled data and the presence of unseen anomalies in those data. Supervised anomaly detection is similar to imbalanced classification [6,16,31] in that they both detect rare classes with a few labeled examples. However, due to the unbound nature and unknowingness of anomalies, anomaly detection is inherently an open-set task, while the imbalanced classification task is typically formulated as a closed-set problem.