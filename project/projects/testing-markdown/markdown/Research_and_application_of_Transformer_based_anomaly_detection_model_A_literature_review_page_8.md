Page 8

A2Log [46] employs an unsupervised approach to establish optimal decision boundaries for the model. It initiates the anomaly scoring process on datasets such as BGL, thunderbird, and spirit1, followed by anomaly decision-making based on the obtained scores. Xu et al. [47] believed that the key to unsupervised time series anomaly detection is to learn information representation and find distinguishable standards. They used Transformer to find more anomaly associations and solved the problem with association discrepancy. CT-D2GAN [48] applied an unsupervised approach to the video anomaly detection task. The authors used a hybrid model based on convolutional Transformer and dual discriminator to perform anomaly detection on videos by extracting spatial information from the convolutional encoder and contextual temporal information from Transformer. Similar to CT-D2GAN, DCT-GAN [49] is also a hybrid model based on Transformer, CNN, and GAN for unsupervised time series anomaly detection. Experimental results show that the training strategy of DCT-GAN with sliding window, GAN loss, and Gradient Penalty (GP) could enhance the robustness of the unsupervised model within a certain range of anomaly rates (5% -20%). Tajiri et al. [50] used the structure of Set Transformer [51] to detect anomalies in unsupervised information and communication technology systems (ICT).

LAnoBERT [52], and LSADNET [53] are both unsupervised anomaly detection models for logs. The distinction lies in the approaches employed by LAnoBERT and LSADNET. LAnoBERT utilizes the BERT model without relying on a log parser. It detects anomalies by leveraging a preprocessing technique that involves minimization and a regularization approach to handle unstructured text, such as log sequences. These preprocessed sequences are then inputted into the BERT model for anomaly detection through log sequence masking and prediction. While LSADNET draws inspiration from both Transformer and CNN techniques used in image anomaly detection. It divides the log anomaly detection task into two key aspects: local information extraction and global sparse Transformer. Local information extraction entails capturing local correlations through multi-layer convolution. In contrast, global sparse Transformer utilizes the Transformer model to learn global correlations among remote logs. GTA [54], MT-RVAE [55] are unsupervised methods for anomaly detection of MTS data. GTA uses Transformer and Graph Convolutional Networks (GCN) to automatically learn graph structure and model temporal correlations. MT-RAVE, on the other hand, uses an up-sampling algorithm to capture multi-scale temporal information, employs a self-attention mechanism to capture potential correlations between sequences, and integrates the extracted features through a residual VAE.

Metaformer [56] is an unsupervised universal anomaly detection model combining Model-agnostic meta learning (MAML) and Transformer, which can solve the anomaly classification and anomaly location tasks in image anomaly detection. Schneider et al. [57] evaluated with an unsupervised paradigm including Reconstruction Convolutional Autoencoder (R-CAE), Prediction Convolutional Autoencoder (P-CAE),

Prediction Convolutional LSTM (P-ConvLSTM), Reconstruction Vision Transformer (R-ViT-AE) and Prediction Vision Transformer (P-VIT-AE), and investigated their performance for TOF depth image anomaly detection.

Pinaya et al. [5] integrated a vector-quantized VAE (VQ-VAE) and an autoregressive Transformer model to learn the brain's 2D image probability density function for unsupervised brain anomaly detection and segmentation tasks. Xu et al. [58] employed a pre-trained Transformer model and fine-tuning techniques for unsupervised OOD detection. They utilized the Mahalanobis distance to extract potential representations from all layers of the pre-trained BERT model. By identifying the layers with more significant feature importance, they were able to reduce the problem to low-dimensional constrained convex optimization. This approach effectively streamlined the detection process. UTRAD [59] is a U-Transformer model using the skip-connection method for unsupervised image anomaly detection. UTRAD firstly extracts multi-scale features through a pre-trained CNN trunk, then uses U-Transformer as the multi-scale reconstruction model for feature reconstruction, and adopts reconstruction error to measure the anomaly score. UCAD [60] innovatively applies Transformer to the unsupervised contextual anomaly detection task for database systems.