Page 42

consists of stacked encoders, is a two-stage framework that starts with pre-training and then fine-tuning according to each specific task. In the pre-training stage, BERT requires a huge amount of data and consumes a lot of system resources. When the length of the training sentence is 512 Tokens, more than 40G of physical memory is often required to effectively support the parallel operation of BERT. Therefore, NLP open source communities such as HuggingFace [107] have opened a series of pre-trained BERT models for the general research community to reduce the cost of using BERT models. BERT has two outputs: pooler output, corresponding output of [ CLS ], and sequence output, corresponding to the last layer of the hidden output of all words in the sequence. Pooler output can be used for classification/regression tasks, and sequence output can be used for processing sequence tasks. BERT is only slightly different from the Vanilla Transformer Encoder in the input layer, which is modified to the following format:

[ CLS ] + SENTENCEA (+ SEP + SENTENCEB +[ SEP ]) (32)

Where [ CLS ] represents the special token for the classification task, which is the pooler output, and [ SEP ] is the separator. SENTENCEA and SENTENCEB are both the input text of the model, where SENTENCEB can be empty. BERT does not use the sin and cos function encoding method of Vanilla Transformer but obtains the position information through a method similar to word embedding. Since BERT may contain multiple sentence segment inputs, it is also necessary to add segment embedding.

Another innovation of BERT is the Mask Language Model (MLM) mechanism, which is the reason why BERT can be unconstrained by one-way language models. BERT predicts the original word at [ MASK ] position by randomly replacing the token in each training sequence with a mask token at 15% probability. BERT also performs the Next Sentence Prediction (NSP) task in the pre-training phase. Since MLM tasks tend to extract token-level representations, sentence-level representations cannot be obtained directly. To make the model capable of understanding the relationship between sentences, BERT uses NSP to predict whether two sentences are connected.

A large number of researchers have already applied BERT to anomaly detection tasks, most of which are for anomaly detection in log sequences [108], due to the structural proximity of unstructured text-like log sequences to the NLP tasks handled by BERT. Based on HitAnomaly [2], Huang et al. further proposed HilBERT [109], a pretrained log representation model with hierarchical bidirectional encoder Transformers. In addition, BERT itself can be used as a superior feature extractor for extracting anomalous features from logs. However, related researches also lead to some conflicting views, such as whether a pre-training task should be performed when using BERT for anomaly detection. These questions are explored in further detail in Section 7.1.