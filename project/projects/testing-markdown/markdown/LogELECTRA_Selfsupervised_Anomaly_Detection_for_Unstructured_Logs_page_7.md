Page 7

Existing unsupervised and semi-supervised methods depend on the grouping method for detection performance. This is especially true for the Spirit dataset, where the F1 Scores for DeepLog and LogAnomaly are significantly degraded more in the 100-messages grouping setting than in the 60min grouping setting. On the other hand, LogELECTRA is able to achieve relatively stable and high performance with any grouping method. This shows that LogELECTRA is less dependent on the grouping method, since it specializes in the analysis of a single log message. The results indicate that LogELECTRA can quickly find anomalous logs even under diverse circumstances.

When training data is selected for chronological, the detection performance of the existing unsupervised and semisupervised methods is degraded compared with that of randomly selected training data. This indicates that chronological

COMPARISON OF DETECTION PERFORMANCE OF EACH MODEL ON BGL DATASET.

TABLE III

| Model      | 60 minutes grouping (random/chronological)   | 60 minutes grouping (random/chronological)   | 60 minutes grouping (random/chronological)   | 60 minutes grouping (random/chronological)   | 100 messages grouping (random/chronological)   | 100 messages grouping (random/chronological)   | 100 messages grouping (random/chronological)   | 100 messages grouping (random/chronological)   |
|------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|------------------------------------------------|------------------------------------------------|------------------------------------------------|------------------------------------------------|
|            | Prec                                         | Rec                                          | Spec                                         | F1                                           | Prec                                           | Rec                                            | Spec                                           | F1                                             |
| LogELECTRA | 0.845 / 0.882                                | 0.955 / 0.882                                | 0.943 / 0.968                                | 0.897 / 0.882                                | 0.945 / 0.523                                  | 0.983 / 0.946                                  | 0.993 / 0.706                                  | 0.963 / 0.674                                  |
| DeepLog    | 0.926 / 0.311                                | 0.773 / 0.941                                | 0.982 / 0.440                                | 0.842 / 0.468                                | 0.972 / 0.298                                  | 0.737 / 0.338                                  | 0.997 / 0.921                                  | 0.838 / 0.317                                  |
| LogAnomaly | 0.915 / 0.325                                | 0.730 / 0.811                                | 0.980 / 0.547                                | 0.812 / 0.465                                | 0.921 / 0.187                                  | 0.694 / 0.915                                  | 0.992 / 0.610                                  | 0.792 / 0.312                                  |
| PLELog     | 0.965 / 0.802                                | 0.987 / 0.625                                | 0.996 / 0.943                                | 0.976 / 0.701                                | 0.646 / 0.925                                  | 0.856 / 0.402                                  | 0.959 / 0.985                                  | 0.736 / 0.560                                  |
| RobustLog  | 0.993 / 0.973                                | 0.957 / 0.961                                | 0.998 / 0.993                                | 0.975 / 0.967                                | 0.974 / 0.906                                  | 0.985 / 0.951                                  | 0.996 / 0.991                                  | 0.979 / 0.928                                  |
| CNN        | 0.985 / 0.921                                | 0.852 / 0.908                                | 0.996 / 0.978                                | 0.914 / 0.914                                | 0.971 / 0.953                                  | 0.982 / 0.949                                  | 0.996 / 0.995                                  | 0.976 / 0.951                                  |

COMPARISON OF DETECTION PERFORMANCE OF EACH MODEL ON SPIRIT DATASET.

TABLE IV COMPARISON OF DETECTION PERFORMANCE OF EACH MODEL ON THUNDERBIRD DATASET.

| Model      | 60 minutes grouping (random/chronological)   | 60 minutes grouping (random/chronological)   | 60 minutes grouping (random/chronological)   | 60 minutes grouping (random/chronological)   | 100 messages grouping (random/chronological)   | 100 messages grouping (random/chronological)   | 100 messages grouping (random/chronological)   | 100 messages grouping (random/chronological)   |
|------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|------------------------------------------------|------------------------------------------------|------------------------------------------------|------------------------------------------------|
|            | Prec                                         | Rec                                          | Spec                                         | F1                                           | Prec                                           | Rec                                            | Spec                                           | F1                                             |
| LogELECTRA | 0.879 / 0.961                                | 1.000 / 0.943                                | 0.447 / 0.968                                | 0.935 / 0.952                                | 0.928 / 0.687                                  | 0.994 / 0.936                                  | 0.949 / 0.977                                  | 0.960 / 0.792                                  |
| DeepLog    | 0.811 / 0.457                                | 1.000 / 1.000                                | 0.063 / 0.008                                | 0.896 / 0.627                                | 0.769 / 0.271                                  | 0.891 / 0.739                                  | 0.823 / 0.653                                  | 0.826 / 0.397                                  |
| LogAnomaly | 0.803 / 0.462                                | 1.000 / 1.000                                | 0.021 / 0.011                                | 0.891 / 0.632                                | 0.797 / 0.296                                  | 0.899 / 0.751                                  | 0.849 / 0.629                                  | 0.845 / 0.425                                  |
| PLELog     | 0.909 / 0.952                                | 0.918 / 0.507                                | 0.660 / 0.861                                | 0.914 / 0.662                                | 0.865 / 0.199                                  | 0.807 / 0.654                                  | 0.910 / 0.970                                  | 0.867 / 0.305                                  |
| RobustLog  | 0.999 / 0.991                                | 1.000 / 1.000                                | 0.999 / 0.992                                | 0.999 / 0.995                                | 0.999 / 0.928                                  | 0.998 / 0.823                                  | 0.999 / 0.997                                  | 0.998 / 0.872                                  |
| CNN        | 0.999 / 0.981                                | 1.000 / 0.926                                | 0.999 / 0.983                                | 0.999 / 0.971                                | 0.999 / 0.757                                  | 0.998 / 0.838                                  | 0.999 / 0.991                                  | 0.998 / 0.796                                  |

TABLE II

| Model      | 60 minutes grouping (random/chronological)   | 60 minutes grouping (random/chronological)   | 60 minutes grouping (random/chronological)   | 60 minutes grouping (random/chronological)   | 100 messages grouping (random/chronological)   | 100 messages grouping (random/chronological)   | 100 messages grouping (random/chronological)   | 100 messages grouping (random/chronological)   |
|------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|------------------------------------------------|------------------------------------------------|------------------------------------------------|------------------------------------------------|
|            | Prec                                         | Rec                                          | Spec                                         | F1                                           | Prec                                           | Rec                                            | Spec                                           | F1                                             |
| LogELECTRA | 0.778 / 0.723                                | 1.000 / 0.838                                | 0.664 / 0.901                                | 0.875 / 0.776                                | 0.967 / 0.643                                  | 0.995 / 0.997                                  | 0.978 / 0.922                                  | 0.981 / 0.781                                  |
| DeepLog    | 0.694 / 0.535                                | 1.000 / 0.967                                | 0.601 / 0.648                                | 0.819 / 0.689                                | 0.926 / 0.645                                  | 0.872 / 0.942                                  | 0.956 / 0.945                                  | 0.898 / 0.766                                  |
| LogAnomaly | 0.625 / 0.547                                | 1.000 / 0.935                                | 0.454 / 0.676                                | 0.771 / 0.691                                | 0.924 / 0.586                                  | 0.891 / 0.936                                  | 0.954 / 0.927                                  | 0.907 / 0.729                                  |
| PLELog     | 0.960 / 0.903                                | 0.676 / 0.667                                | 0.941 / 0.952                                | 0.793 / 0.767                                | 0.965 / 0.958                                  | 0.961 / 0.624                                  | 0.983 / 0.993                                  | 0.967 / 0.756                                  |
| RobustLog  | 1.000 / 0.960                                | 0.920 / 0.774                                | 1.000 / 0.986                                | 0.958 / 0.857                                | 0.999 / 0.982                                  | 0.993 / 0.705                                  | 0.999 / 0.998                                  | 0.996 / 0.705                                  |
| CNN        | 0.998 / 0.958                                | 0.932 / 0.741                                | 1.000 / 0.986                                | 0.963 / 0.836                                | 0.996 / 0.919                                  | 0.992 / 0.864                                  | 0.999 / 0.991                                  | 0.995 / 0.891                                  |

train data selection is a more difficult setting than random train data selection. The reason is that with random selection, the model can make more accurate predictions since it can see future log events during the training phase. On the other hand, LogELECTRA's accuracy degradation is less than that of other unsupervised and semi-supervised methods, even in the chronological setting. This indicates that LogELECTRA generalizes well to unseen normal logs and is able to capture the context of normal logs differently from anomaly logs.

Focusing on supervised methods, even in the case of chronological setting, the detection performance is not significantly different from that of random data selection. This suggests that if the features that separate normal and abnormal contexts are captured, high accuracy can be achieved even in the case of chronological data selection, even with unlabeled learning. Future challenges include capturing this feature from normal log messages alone, without label information, and closing the gap between LogELECTRA and these supervised methods.

## V. CONCLUSION

In this paper, we proposed a novel log anomaly detection method, LogELECTRA. Our method utilizes the pre-training task of ELECTRA, Replaced Token Detection, to capture the context of normal log messages and detect anomalous log messages with a different context. LogELECTRA is specialized to detect point anomalies, so there is no unnecessary delay in detection unlike in existing methods. Furthermore, since our method is based on the ELECTRA, a superior natural language processing model, it accurately captures the context of normal log messages and has the advantage of generalization performance for unseen log messages. This was confirmed in experiments on three different public log datasets. LogELECTRA outperforms existing state-of-the-art methods that do not use anomaly data for training and achieves detection performance comparable to supervised methods under some settings. Future work will include studies for realworld adaptations, such as scalability, additional experiments on other system logs, model transferability, and measurement of actual anomaly-to-detection delays.

## REFERENCES

- [1] V.-H. Le and H. Zhang, 'Log-based anomaly detection with deep learning: How far are we?' in Proceedings of the 44th International Conference on Software Engineering , 2022, pp. 1356-1367.
- [2] Z. Chen, J. Liu, W. Gu, Y. Su, and M. R. Lyu, 'Experience report: Deep learning-based system log analysis for anomaly detection,' arXiv preprint arXiv:2107.05908 , 2021.
- [3] S. He, J. Zhu, P. He, and M. R. Lyu, 'Experience report: System log analysis for anomaly detection,' in 2016 IEEE 27th international symposium on software reliability engineering (ISSRE) . IEEE, 2016, pp. 207-218.
- [4] Q. Fu, J.-G. Lou, Y. Wang, and J. Li, 'Execution anomaly detection in distributed systems through unstructured log analysis,' in 2009 ninth IEEE international conference on data mining . IEEE, 2009, pp. 149158.
- [5] H. Guo, S. Yuan, and X. Wu, 'Logbert: Log anomaly detection via bert,' in 2021 international joint conference on neural networks (IJCNN) . IEEE, 2021, pp. 1-8.
- [6] S. He, P. He, Z. Chen, T. Yang, Y. Su, and M. R. Lyu, 'A survey on automated log analysis for reliability engineering,' ACM Comput. Surv. , vol. 54, no. 6, jul 2021. [Online]. Available: https://doi.org/10.1145/3460345
- [7] P. He, J. Zhu, Z. Zheng, and M. R. Lyu, 'Drain: An online log parsing approach with fixed depth tree,' in 2017 IEEE international conference on web services (ICWS) . IEEE, 2017, pp. 33-40.
- [8] M. Du and F. Li, 'Spell: Streaming parsing of system event logs,' in 2016 IEEE 16th International Conference on Data Mining (ICDM) . IEEE, 2016, pp. 859-864.
- [9] A. Makanju, A. N. Zincir-Heywood, and E. E. Milios, 'A lightweight algorithm for message type extraction in system application logs,' IEEE Transactions on Knowledge and Data Engineering , vol. 24, no. 11, pp. 1921-1936, 2011.
- [10] Z. M. Jiang, A. E. Hassan, P. Flora, and G. Hamann, 'Abstracting execution logs to execution events for enterprise applications (short paper),' in 2008 The Eighth International Conference on Quality Software , 2008, pp. 181-186.
- [11] J. Zhu, S. He, J. Liu, P. He, Q. Xie, Z. Zheng, and M. R. Lyu, 'Tools and benchmarks for automated log parsing,' in Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice , ser. ICSE-SEIP '19. IEEE Press, 2019, p. 121-130. [Online]. Available: https: //doi.org/10.1109/ICSE-SEIP.2019.00021
- [12] Y. Yu, X. Si, C. Hu, and J. Zhang, 'A review of recurrent neural networks: Lstm cells and network architectures,' Neural computation , vol. 31, no. 7, pp. 1235-1270, 2019.
- [13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin, 'Attention is all you need,' Advances in neural information processing systems , vol. 30, 2017.
- [14] S. Kabinna, W. Shang, C.-P. Bezemer, and A. E. Hassan, 'Examining the stability of logging statements,' in 2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER) , vol. 1, 2016, pp. 326-337.
- [15] N. Japkowicz and S. Stephen, 'The class imbalance problem: A systematic study,' Intelligent data analysis , vol. 6, no. 5, pp. 429-449, 2002.
- [16] T. Wittkopp, P. Wiesner, D. Scheinert, and O. Kao, 'A taxonomy of anomalies in log data,' in Service-Oriented Computing - ICSOC 2021 Workshops , H. Hacid, M. Aldwairi, M. R. Bouadjenek, M. Petrocchi, N. Faci, F. Outay, A. Beheshti, L. Thamsen, and H. Dong, Eds. Cham: Springer International Publishing, 2022, pp. 153-164.
- [17] K. Clark, M. Luong, Q. V. Le, and C. D. Manning, 'ELECTRA: pre-training text encoders as discriminators rather than generators,' in 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. [Online]. Available: https://openreview.net/forum?id=r1xMH1BtvB
- [18] X. Li, P. Chen, L. Jing, Z. He, and G. Yu, 'Swisslog: Robust and unified deep learning based log anomaly detection for diverse faults,' in 2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE) . IEEE, 2020, pp. 92-103.
- [19] W. Meng, Y. Liu, Y. Zhu, S. Zhang, D. Pei, Y. Liu, Y. Chen, R. Zhang, S. Tao, P. Sun et al. , 'Loganomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs.' in IJCAI , vol. 19, no. 7, 2019, pp. 4739-4745.
- [20] M. Du, F. Li, G. Zheng, and V. Srikumar, 'Deeplog: Anomaly detection and diagnosis from system logs through deep learning,' in Proceedings of the 2017 ACM SIGSAC conference on computer and communications security , 2017, pp. 1285-1298.
- [21] S. Nedelkoski, J. Bogatinovski, A. Acker, J. Cardoso, and O. Kao, 'Selfattentive classification-based anomaly detection in unstructured logs,' in 2020 IEEE International Conference on Data Mining (ICDM) . IEEE, 2020, pp. 1196-1201.
- [22] L. Yang, J. Chen, Z. Wang, W. Wang, J. Jiang, X. Dong, and W. Zhang, 'Semi-supervised log-based anomaly detection via probabilistic label estimation,' in 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE) . IEEE, 2021, pp. 1448-1460.
- [23] S. Lu, X. Wei, Y. Li, and L. Wang, 'Detecting anomaly in big data system logs using convolutional neural network,' in 2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress (DASC/PiCom/DataCom/CyberSciTech) . IEEE, 2018, pp. 151-158.
- [24] X. Zhang, Y. Xu, Q. Lin, B. Qiao, H. Zhang, Y. Dang, C. Xie, X. Yang, Q. Cheng, Z. Li et al. , 'Robust log-based anomaly detection on unstable log data,' in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering , 2019, pp. 807-817.
- [25] Y. Liang, Y. Zhang, H. Xiong, and R. Sahoo, 'Failure prediction in ibm bluegene/l event logs,' in Seventh IEEE International Conference on Data Mining (ICDM 2007) . IEEE, 2007, pp. 583-588.
- [26] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, 'Detecting large-scale system problems by mining console logs,' in Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles , 2009, pp. 117-132.
- [27] S. Hochreiter and J. Schmidhuber, 'Long short-term memory,' Neural computation , vol. 9, no. 8, pp. 1735-1780, 1997.
- [28] Y. Goldberg and O. Levy, 'word2vec explained: deriving mikolov et al.'s negative-sampling word-embedding method,' arXiv preprint arXiv:1402.3722 , 2014.
- [29] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, 'Enriching word vectors with subword information,' arXiv preprint arXiv:1607.04606 , 2016.
- [30] A. Krizhevsky, I. Sutskever, and G. E. Hinton, 'Imagenet classification with deep convolutional neural networks,' Communications of the ACM , vol. 60, no. 6, pp. 84-90, 2017.
- [31] R. Kiryo, G. Niu, M. C. Du Plessis, and M. Sugiyama, 'Positiveunlabeled learning with non-negative risk estimator,' Advances in neural information processing systems , vol. 30, 2017.
- [32] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, 'Bert: Pre-training of deep bidirectional transformers for language understanding,' arXiv preprint arXiv:1810.04805 , 2018.
- [33] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, 'Glue: A multi-task benchmark and analysis platform for natural language understanding,' arXiv preprint arXiv:1804.07461 , 2018.
- [34] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, 'Generative adversarial networks,' Communications of the ACM , vol. 63, no. 11, pp. 139-144, 2020.
- [35] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey et al. , 'Google's neural machine translation system: Bridging the gap between human and machine translation,' arXiv preprint arXiv:1609.08144 , 2016.
- [36] S. He, J. Zhu, P. He, and M. R. Lyu, 'Loghub: a large collection of system log datasets towards automated log analytics,' arXiv preprint arXiv:2008.06448 , 2020.
- [37] A. Oliner and J. Stearley, 'What supercomputers say: A study of five system logs,' in 37th annual IEEE/IFIP international conference on dependable systems and networks (DSN'07) . IEEE, 2007, pp. 575584.
- [38] J. Pennington, R. Socher, and C. D. Manning, 'Glove: Global vectors for word representation,' in Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) , 2014, pp. 1532-1543.
- [39] I. Loshchilov and F. Hutter, 'Decoupled weight decay regularization,' arXiv preprint arXiv:1711.05101 , 2017.