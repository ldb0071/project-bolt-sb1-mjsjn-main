Page 5

## 7 Privacy-preserving Anomaly Detection

Many anomaly detection tasks involve the use of personal data, such as online troll detection, bank fraud detection, or insider threat detection. Therefore, privacy-preservation as a process of protecting sensitive information against being revealed or misused by unauthorized users has been studied extensively [Xu et al. , 2021c]. The goal of privacy-preserving anomaly detection is to achieve anomaly detection with the protection of sensitive information and without compromising its effectiveness.

## 7.1 Approaches

Privacy-preserving anomaly detection can be categorized into three classes, anonymization-based, cryptographic-based, and perturbation-based approaches.

Anonymization-based approaches are to achieve privacy protection by removing personally identifiable information while keeping the utility of the data for anomaly detection. [Keshk et al. , 2019] develops a privacy-preserving anomaly detection framework for cyber-physical systems (PPAD-CPS). Its key idea is to filter and transform original data into a new format in the data-processing step, and adapt the Gaussian Mixture Model and Kalman Filter to detect anomalies on the anonymized data.

Privacy-preserving LOF (PPLOF) is a cryptographic-based approach for anomaly detection over vertically partitioned datasets among two parties [Li et al. , 2015]. A privacypreserving windowed Gaussian anomaly detection algorithm on encrypted data is developed for an edge computing environment in [Mehnaz and Bertino, 2020].

Differential privacy (DP) is the de facto standard for conducting perturbation-based privacy-preserving data analysis [Dwork and Roth, 2014]. However, fraud detection and differential privacy protection are intrinsically conflicting because the goal of DP is to conceal presence or absence of any particular instance and the problem of fraud detection is to identify anomaly instances. [Okada et al. , 2015] balances these two tasks by using two differentially private queries, counting the number of outliers in a subspace and discovering top subspaces with a large number of outliers, to understand behavior of outliers. DP can generally be achieved by injecting some random noises to model's input data, output, or objective function. To ensure differential privacy in fraud detection, we can collect input data with local differential privacy protection, use differentially private aggregates or statistics for downstream anomaly detection tasks [Fan and Xiong, 2013], or train deep learning based anomaly detection models by differentially private stochastic gradient descent (DPSGD) [Abadi et al. , 2016], which adds Gaussian noise to the aggregated gradients during the gradient descent process.

## 7.2 Discussions

There is always a trade-off between privacy protection and other quality of models, such as model effectiveness or efficiency. It is well-known that cryptographic-based approaches require high computational overhead, and perturbation-based approaches could damage the model's accuracy. Recently [Du et al. , 2020] shows that applying differential privacy can potentially improve the robustness of anomaly detection when the training set contains noises. The intuition is that deep learning models with millions of parameters are capable of remembering all the training samples including the noises. In the unsupervised anomaly detection scenario, the model would consider rare samples, such as backdoor samples, in the training set as normal ones due to potential overfitting to rare samples. However, DP makes the model underfit rare samples by injecting random noise during the training so that the overall performance can be improved. It is worth studying in what circumstances privacy-preserving approaches can also improve performance and robustness.

## 8 Future Directions

In this brief survey, we review the current research in trustworthy anomaly detection from four perspectives, interpretability, fairness, robustness, and privacy-preservation. Overall, compared with widely-studied classification models, ensuring trustworthiness of anomaly detection models still has a large room for improvement. Below we point out several potential future directions.

- · Benchmark Datasets. There are limited datasets designed for trustworthy anomaly detection. For interpretable anomaly detection, only a few image and time series datasets with fine-grained labels regarding abnormal regions can be used for evaluation [Bergmann et al. , 2019; Jacob et al. , 2021]. Meanwhile, for fair anomaly detection, current studies usually adopt widely-used datasets in the fair machine learning domain, such as COMPAS [Dressel and Farid, 2018], but these datasets are not designed for anomaly detection. Therefore, developing datasets related to trustworthy anomaly detection is critical for conducting rigorous evaluation and boosting this line of research.
- · Model Agnostic Approaches. Current approaches to achieve specific trustworthy desideratum for anomaly detection are often developed for each specific model, such as fair autoencoder or interpretable CNN. Different anomaly detection models are developed to tackle different types of data such as images, videos, audios, text, graphs, and sequences, developing model-agnostic approaches for trustworthy anomaly detection is important yet exploited.
- · Achieving Multiple Desiderata. Interpretability, fairness, robustness, and privacy are not orthogonal to each other. Several recent works have studied their connections in learning. Note that one desideratum would affect another in an either positive or negative manner. For example, [Bagdasaryan and Shmatikov, 2019] shows that differential privacy can lead to disparate impact against minority groups and [Xu et al. , 2021a] studies how to mitigate the disparate impact. On the other hand, [Du et al. , 2020] shows differential privacy improves model robustness against backdoor attacks. Moreover, privacy-preserving techniques can also act as defenders to protect models from some other attacks, such as membership inference attacks. [Xu et al. , 2021b] shows adversarial learning based defense techniques do not provide sufficient protection to minority groups, which incurs unfairness. On the contrary, [Liu et al. , 2021b] shows adversarial attack and defense can help interpretation because both adversarial attack and interpretation need to identify important features of input samples or important parameters of models. In addition to understanding their inherent relationships, it is imperative to develop a unified anomaly detection framework and new mechanisms that can achieve multiple trustworthy desiderata simultaneously. For example, [Xu et al. , 2019] shows both differential privacy and fairness can be achieved by adding less noise to the objective function of logistic regression than adding noise separately. Moreover, in addition to four desiderata studied in this survey, it is also important to study other dimensions such as auditability, and environmental well-being [Liu et al. , 2021a] in anomaly detection. However, there are very limited research in this direction.

## References

[Abadi et al. , 2016] Mart'ın Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep Learning with Differential Privacy. In CCS , 2016.

[Antwarg et al. , 2020] Liat Antwarg, Ronnie Mindlin Miller, Bracha Shapira, and Lior Rokach. Explaining Anomalies Detected by Autoencoders Using SHAP. arXiv:1903.02407 , June 2020.

[Bagdasaryan and Shmatikov, 2019] Eugene Bagdasaryan and Vitaly Shmatikov. Differential Privacy Has Disparate Impact on Model Accuracy. In NeurIPS , October 2019.

[Bergmann et al. , 2019] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. MVTec AD A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection. In CVPR , 2019.