Page 17

H = LN ( FFN ( H ' ) + H ' ) (7)

Where SelfAttn represents the self-attention layer mentioned in section 4.1, and LN represents the operation of the normalization layer.

The core innovation of Vanilla Transformer lies in the excellent feature extraction capability and parallel computing feature of the MHA mechanism, so researchers often apply it to anomaly detection tasks of serialized data, including but not limited to time series data, log data, stream data, etc.

Kozik et al. [68] used Vanilla Transformer to detect anomalies in network traffic generated by IoT devices. They collected data in the form of stream data collection on the Aposemat IoT-23 dataset and converted it to a sliding window format as the input of Transformer. Zhang et al. [3] applied Vanilla Transformer to an ICS traffic anomaly detection task and obtained the final anomaly detection result through a classifier. A2Log [46] only uses the Transformer encoder structure, and performs the task of anomaly scoring and anomaly decision on this basis. A2Log's experiments demonstrate the universality of Vanilla Transformer, which can achieve excellent performance on different log datasets. Similarly, Li et al. [69] used only Transformer encoder to extract

the hidden features of traffic and perform anomaly detection. Falt et al. [70] argued that training a single Transformer model would suffer from large performance differences, so they used a bagging strategy to ensemble many Transformers, where each underlying Transformer votes on the prediction results. The system selects the highest prediction score for anomaly detection of log sequences by calculating the number of predicted votes and multiplying them by the corresponding weights. However, the performance overhead of this method is very large, as well as low training efficiency.