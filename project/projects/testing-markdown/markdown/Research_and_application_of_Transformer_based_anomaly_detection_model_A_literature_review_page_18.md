Page 18

Meng et al. [6, 71] used the structure of Vanilla Transformer for anomaly detection of spacecraft datasets (MSL, SMAP). UT-ATD [72] uses Vanilla Transformer to learn the embedding of trajectory mapping and detect anomalous trajectory from trajectory embedding through a Multilayer Perceptron (MLP) layer. Huang et al. [2] proposed HitAnomaly, a model for log anomaly detection using a hierarchical Transformer structure and pooling layer. They used a log encoder and a parameter value encoder to encode log templates and parameter values respectively, and used an attention-based approach to classify anomaly detection results. Similarly, Xiao et al. [73] also applied Vanilla Transformer to log anomaly detection tasks and designed a more flexible and robust 'top-p' algorithm. Zhang et al. [74] designed a Transformerenabled feature encoder to convert the input task-agnostic features into discriminative task-specific features by mining the semantic correlation and position relation between video snippets. Wibsono et al. [75], and Guo et al. [76] both applied Vanilla Transformer only to the task of anomaly detection on log data. They did not propose too many modifications and thus had limited results.

As the research progresses, the drawbacks of Vanilla Transformer become apparent. The performance overhead of Vanilla Transformer is too large to be deployed in edge computing devices. For example, the adaptive Spatio-Temporal Attention Transformer proposed by Kumar et al [77]. requires the simultaneous computation of temporal attention and MHA, resulting in significant performance overhead. Further, the inherent encoder-decoder structure greatly limits the application scenarios of Vanilla Transformer, so the Transformer decoder has to be discarded for better transfer learning tasks [46]. Therefore, Ding et al [78]. made improvements to the Vanilla Transformer. They proposed the concept drift adaptation method (CDAM), a kind of distribution adaptation method, to dynamically tune the learning rate of the Transformer. They also utilized root square sparse self-attention, which only requires O ( L √ L ) time complexity, for anomaly detection in time series data.

## 4.2 Anomaly detection based on ViT

The origin of ViT [17] is to accomplish tasks in the field of Computer Vision (CV) using only the attention mechanism in Vanilla Transformer through transfer learning, without relying on CNN structure. ViT essentially divides the original images into blocks, flattens them into sequences, feeds them into the Encoder of Vanilla Transformer, and finally classifies the images through a fully connected layer. Specifically, suppose the original input image data is H × W × C , Where H , W , and C represent the length, width, and number of channels of the image, respectively. Then ViT first divides the picture into N blocks, where N = ( H × W ) / ( P × P ), then each block is flattened into a 1-dimensional vector with vector size of P × P × C . Therefore, the

total input is then transformed into n × ( P 2 × C ). Then, a linear transformation is performed on each of the input vectors, and the dimension is compressed to D . ViT adds an extra dimension based on the Position Encoding of the Vanilla Transformer. The reason is that ViT only uses the encoder structure, so the added dimension is an extra dimension for classification. It is also a learnable parameter, which is concatenated with input. Many ViT-based anomaly detection models also use this dimension to classify anomalies.