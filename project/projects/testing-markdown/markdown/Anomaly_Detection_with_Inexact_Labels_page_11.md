Page 11

## 5.3 Settings of the proposed method

Weused three-layer feed-forward neural networks for the encoder and decoder, where the hidden unit size was 128, and the output layer of the encoder and the input layer of the decoder was 16. Hyperparameter Î» was selected from { 0 , 10 -3 , 10 -2 , 10 -1 , 1 , 10 , 10 2 , 10 3 } using the inexact AUC on the validation data. The validation data were also used for early stopping, where the maximum number of training epochs was 1000. We optimized the neural network parameters using ADAM (Kingma and Ba, 2015) with learning rate 10 -3 , where we randomly sampled eight inexact anomaly sets and 128 non-anomalous instances for each batch. We implemented all the methods based on PyTorch (Paszke et al., 2017).

## 5.4 Results

Figure 2 shows the estimated anomaly scores by AE (c), SAE (d), MIL (e) and the proposed method (f) on the synthetic dataset. Figure 3 shows the ROC curve and test AUC by the AE (a), SAE (b), MIL (c) and the proposed method (d) on the synthetic dataset. The test AUCs were 0.919 with AE, 0.790 with SAE, 0.905 with MIL, and 0.982 with the proposed method. The AE successfully gave relatively high anomaly scores to the test anomalous instances at the top in Figure 2(c). However, since the AE is an unsupervised method and cannot use label information, some anomalous instances at the bottom center were misclassified as non-anomaly. The SAE is a supervised method, therefore the anomalous instances at the bottom center in Figure 2(d) were identified as anomaly more appropriately than the AE. However, since the SAE cannot handle inexact labels, some test non-anomalous instances, that were located around non-anomalous instances in the training inexact anomaly sets, were falsely classified as anomaly. The MIL correctly gave lower anomaly scores to test non-anomalous instances by handling inexact labels than the SAE in Figure 2(e). However, the MIL failed to correctly give high anomaly scores to unseen anomalous instances at the top. On the other hand, because the proposed method is trained by minimizing the anomaly scores for non-anomalous instances while maximizing the inexact AUC, the proposed method succeeded to detect unseen anomalous instances at the top as well as anomalous instances at the bottom center, and correctly classified test non-anomalous instances as non-anomaly in Figure 2(f).

Table 3 shows AUC on the nine anomaly detection datasets with ten inexact anomaly sets and five instances per set. Our proposed method achieved the highest AUC in most cases. Since the number of supervised labels was small, the performance of the supervised methods, KNN, SVM, RF and MIL, was not high. The proposed method outperformed them by incorporating an unsupervised method (AE) in a supervised framework. SIF and SAE also used both unsupervised and supervised anomaly detection frameworks. However, the performance was worse than the proposed method because SIF and SAE cannot handle inexact labels. Although MIL can handle inexact labels, AUC with MIL was low since it does not have an unsupervised training mechanism, i.e., it does not minimize anomaly scores for non-anomalous instances. The average computational time for training the proposed method was

Figure 2: Synthetic dataset and the estimated anomaly scores in the two-dimensional instance space. (a) Test data: anomalous instances are represented by red circles and non-anomalous instances are represented by blue triangles. (b) Training data: instances in the same inexact anomalous set are represented by circles with identical color, and non-anomalous instances are represented by gray triangles. Note that anomalous instances at the top in the test data are not contained in the training data. (c-f) Estimated anomaly scores by the AE (c), SAE (d), MIL (e) and the proposed method (f). The shape indicates the true label (circle: anomalous, triangle: non-anomalous), the color indicates the estimated anomaly score; the darker red indicates the higher anomaly score, and the darker blue indicates lower anomaly score.

<!-- image -->

1.0, 0.4, 1.4, 8.1, 0.7, 0.2, 0.5, 0.6 and 0.7 minutes with Annthyroid, Cardiotocography, InternetAds, KDDCup99, PageBlocks, Pima, SpamBase, Waveform and Wilt datasets, respectively, on computers with 2.60GHz CPUs.