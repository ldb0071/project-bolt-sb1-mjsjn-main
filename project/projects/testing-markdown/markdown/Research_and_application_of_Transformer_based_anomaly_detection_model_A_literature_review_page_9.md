Page 9

Although the unsupervised paradigm may not exhibit superior performance compared to other approaches, it is better aligned with the requirements of most anomaly detection tasks in unlabeled contexts. Additionally, this approach finds application in diverse fields, including images, logs, time series, etc. The detection performance can be further improved by different auxiliary methods such as data enhancement. Therefore, the unsupervised paradigm is one of the current research hotspots in anomaly detection.

## 3.4 Self-supervised learning

Self-supervised learning is essentially a variant of unsupervised learning methods. Different from unsupervised learning, self-supervised learning mainly uses a pretext to mine its supervised information from large-scale unsupervised data and trains the network with the constructed supervised information, which in turn can learn representations that are valuable for downstream tasks. In other words, the labels of self-supervised learning are not manually labeled and can be trained using a similar approach to supervised learning after the labels are obtained through pretexts.

Guo et al. [16] used BERT to detect anomalies in log sequences by training two self-supervised tasks, namely masked log message prediction and hypersphere volume minimization. Marino et al. [61] proposed Network Transformer, a network traffic anomaly detection framework using Transformer. Network Transformer draws lessons from the idea of Graph Neural Network (GNN), extracts the corresponding hierarchical graph features from the network nodes, and trains the neural network in a self-supervised way by collecting data only during the normal operation of the system. Park et al. [62] adopted the UNETR model for self-supervised learning, intending to solve the inaccuracy of OOD detection caused by insufficient labeled data. They pointed out that some anomalous images of rare diseases in the medical field are very difficult to obtain, so self-supervised learning is of great significance in the research field of medical OOD detection. Guo et al. proposed TransLog [63], a self-supervised

anomaly detection method composed of pre-training and adapter-based tuning phases. TransLog uses the method of transfer learning to improve the generalization ability of anomaly detection. Mai et al. [64] conducted experiments to evaluate the detection performance of a fine-tuned Transformer model on near OOD tasks, specifically focusing on semantic and syntactic anomalies. Through a self-supervised approach, they confirmed the exceptional feature extraction capabilities of Transformer.