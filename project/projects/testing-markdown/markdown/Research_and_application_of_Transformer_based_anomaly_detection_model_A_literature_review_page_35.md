Page 35

Fig. 8 The structure of convolutional Transformer (NOTE: The figure is from the paper [65])

<!-- image -->

Multi-Sequence Learning essentially changes the single MIL video sequence segment to the average anomaly scores of consecutive segments. The whole model first extracts feature F from videos containing T segments through a backbone network, and then MSLNet takes F as input, including a video classifier and a segment regressor. The video classifier is used to predict whether the video contains anomalies and contains two layers of Convolutional Transformer Encoder (CTE) and a linear layer. The segment regressor is used to predict the anomaly score of each video segment, which has the same structure as the video classifier. The final total loss is based on the hinge-based Multi-Sequence Learning ranking loss with classification loss. A score correction method is used to reduce the fluctuation of anomaly scores predicted by the segment regressor, i.e., making adjustments based on the prediction results of the video classifier, maintaining the anomaly scores if they are predicted to contain anomalies with high probability, and weakening the anomaly scores otherwise. In the training stage, a two-stage self-training mechanism is used. In the first stage, Multi-Sequence Learning is used to select the tag (as it is a weak-supervised learning). Maxsi is used to choose the maximum average pseudo-label sequences so that the model has the

preliminary ability to predict anomaly scores. In the second stage, MSLNet is used to select the sequence by prediction. Finally, the prediction scores are gradually refined by continuously halving the sequence length K and repeating the above two stages. Tian et al. [67] pointed out that the limitation of MIL is the inability to select rare anomalous segments in anomalous videos, so they used the contrastive snippet mining (CSM) algorithm to identify hard-to-recognize normal and anomalous clips and planned to further increase the online reasoning capability in the future.

Since 2022, convolutional Transformers have become even more of a research hotspot in multiple anomaly detection tasks. Researchers tend to incorporate Transformer as a crucial component in the entire algorithm pipeline (i.e., feature extraction, feature reconstruction, etc.). ADTR [95], for instance, utilizes a frozen pre-trained CNN backbone to extract features and Transformer for feature reconstruction with an auxiliary learnable query embedding. They also proposed novel loss functions for image-level and pixel-level anomaly tasks, respectively. Jin et al. [96] applied the convolutional Transformer to video anomaly detection tasks. They utilized a Transformer encoder to encode and extract the spatio-temporal features of the video and employed a convolutional decoder for anomaly prediction. Extracting spatio-temporal features is also a research focus of Sun et al [97]. Similarly, Deshpande et al. [98] proposed utilizing a pre-trained videoswin Transformer model to extract better quality features and encode long and short-range dependencies in the temporal domain through attention layers. They performed anomaly detection using the Robust Temporal Feature Magnitude Learning (RTFM) model.