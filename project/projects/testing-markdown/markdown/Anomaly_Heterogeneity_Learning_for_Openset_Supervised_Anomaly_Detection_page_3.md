Page 3

Unsupervised Anomaly Detection. Most existing AD approaches rely on unsupervised learning with anomaly-free training samples due to the difficulty of collecting largescale anomaly observations. One-class classification methods aim to learn a compact normal data description using support vectors [4, 10, 39, 46, 59]. Another widely used AD approach learns to reconstruct normal data based on generative models such as Autoencoder (AE) [21] and Generative Adversarial Networks (GAN) [17]. These reconstruction methods rely on the assumption that anomalies are more difficult to be reconstructed than normal samples [2, 18, 25, 36, 41, 54, 55, 61, 63, 64]. Other popular approaches include knowledge distillation [6, 9, 14, 40, 48, 49, 66] and selfsupervised learning methods [16, 19, 22, 37, 57]. Another related line of research is domain-adapted AD [28, 51, 56]. Methods in this line typically focus on a cross-domain setup, requiring the data from multiple relevant domains, whereas we focus on training detection models in singledomain data. One major problem with all these unsupervised AD approaches is that they do not have any prior knowledge about real anomalies, which can lead to many false positive errors [1, 8, 12, 15, 24, 32, 34, 35, 39, 68].

Towards Supervised Anomaly Detection. Supervised AD aims to reduce the detection errors using less costly supervision information, such as weakly-supervised information like video-level supervision to detect frame-level anomalies [11, 29, 43, 47, 52, 53] and a small set of anomaly examples from partially observed anomaly classes [8, 27, 32, 34, 35, 35, 39, 62]. OSAD addresses the problem in the latter case. One OSAD approach is one-class metric learning, where the limited training anomalies are treated as negative samples during the normality learning [24, 31, 39]. However, AD is inherently an open-set task due to the unknowingness nature of anomaly, so the limited negative samples are not sufficient to support an accurate one-class learn-

ing. Recently DevNet [32] introduces a one-sided anomalyfocused deviation loss to tackle this problem by imposing a prior on the anomaly scores. It also establishes an OSAD evaluation benchmark. DRA [15] enhances DevNet via a framework that learns disentangled representations of seen, pseudo, and latent residual anomalies in order to better detect both seen and unseen anomalies. More recently, BGAD [58] uses a decision boundary generated by a normalizing flow model to learn an anomaly-informed model. PRN [65] learns residual representations across multi-scale feature maps using both image-level and pixel-level anomaly data. However, their implementation uses training anomaly examples from all anomaly types, different from our openset AD settings that have unseen anomaly types in test data. UBnormal [1] and OpenVAD [68] extend OSAD to video data and establish corresponding benchmarks. However, these methods often treat the training anomalies as from homogeneous distributions in a closed-set setting, which can restrict their performance in detecting unseen anomalies. Using the anomaly examples for anomaly generation or pseudo anomaly labeling [3, 62] is explored as another way to reduce the false positives, but it is performed in an unsupervised setting.

## 3. Anomaly Heterogeneity Learning

Problem Statement : We assume to have a set of training images and annotations { ω i , y i ) } i =1 , where ω i ∈ Ω ⊂ R H × W × C denotes an image with RGB channels and y i ∈ Y ⊂ { 0 , 1 } denotes an image-level class label, with y i = 1 if ω i is abnormal and y i = 0 otherwise. Due to the rareness of anomaly, the labeled data is often predominantly presented by normal data. Given an existing AD model f ( · ) that can be used to extract low-dimensional image features for constructing the training feature set D = { x i , y i } , where x i = f ( ω i ) ∈ X indicates corresponding i -th image features, with X n = { x 1 , x 2 , ..., x N } and X a = { x 1 , x 2 , ..., x M } ( N ≫ M ) respectively denoting the feature set of normal and abnormal images, then the goal of our proposed AHL framework is to learn an anomaly detection function g : X -→ R that is capable of assigning higher anomaly scores to anomaly images drawn from different distributions than to the normal ones. Note that in OSAD the training anomalies X a are from seen anomaly classes S , which is only a subset of C that can contain a larger set of anomaly classes during inference, e.g ., S ⊂ C .

## 3.1. Overview of Our Approach

The key idea of our AHL framework is to learn a unified anomaly heterogeneity model by a collaborative differentiable learning of abnormalities embedded in diverse simulated anomaly distributions. As demonstrated in Fig. 2, AHL consists of two main components: Heterogeneous Anomaly Distribution Generation ( HADG ) and Collaborative Dif-

ferentiable Learning of the anomaly heterogeneity ( CDL ). Specifically, the HADG component simulates and generates T heterogeneous distribution datasets from the training set, T = {D i } T i =1 , with each D i containing a mixture of normal data subset and randomly sampled anomaly examples. Each D i is generated in a way that represents a different anomaly distribution from the others. CDL is then designed to learn a unified heterogeneous abnormality detection model g ( T ; θ g ) that synthesizes a set of T base models, denoted as { ϕ i ( D i ; θ i ) } T i =1 , where θ g and θ i denotes learnable weight parameters of the unified model g and the base model ϕ i respectively, and each ϕ i : D i → R learns from one anomaly distribution for anomaly scoring. The weight parameters θ g are collaboratively updated based on the base model weights { θ i } T i =1 . Further, the effectiveness of individual base models can vary largely, so a module ψ is added in CDL to increase the importance of θ i in the collaborative weight updating if its corresponding base model ϕ i is estimated to have small generalization error. During inference, only the unified heterogeneous abnormality model g ( T ; θ g ) is used for anomaly detection.

AHL is a generic framework, in which off-the-shelf OSAD models can be easily plugged to instantiate ϕ i and gain significantly improved performance.

## 3.2. Heterogeneous Anomaly Distribution Data Generation