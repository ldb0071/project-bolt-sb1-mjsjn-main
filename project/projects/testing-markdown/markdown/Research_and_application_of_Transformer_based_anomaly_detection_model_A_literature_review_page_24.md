Page 24

The main innovation of TabTransformer is its ability to train for semi-supervised scenarios effectively and has relative robustness to handle missing and noisy data. However, it is also easy to see from Figure 4 that TabTransformer also has significant structural defects. Since TabTransformer only sends categorical features into the Transformer Block for feature extraction after vector encoding, continuous features are only connected to the output of the Transformer block after a simple normalization operation. Therefore, if the input contains few category features and many continuous features, the structure of TabTransformer can bring very limited gain. On the contrary, the training efficiency will be reduced due to too many Transformer training parameters. Therefore, most of the TabTransformer-based anomaly detection models use the ensemble learning method to compensate for the deficiencies of TabTransformer. For example, GTF [31] simplifies TabTransformer by removing the continuous features in the input, as well as the normalization layer and connection layer, to take full advantage of the TabTransformer's ability in processing tabular input data. GTF also converts GBDT into a neural network structure GBDT2NN using the expressive ability of neural networks, to better integrate with TabTransformer. To enhance the training efficiency of the ensemble method, GTF incorporates the adaptive training technique. This approach defines a range of parameter values to be trained by setting upper and lower limits. Additionally, a target index, such as the F 1 score, is specified to automatically search for the optimal parameter configuration that yields the best performance during the training process. By employing this adaptive training method,

GTF aims to optimize its performance effectively. All the above optimization measures ensure that GTF retains the performance of TabTransformer while reducing the computational complexity required for model training, allowing GTF to be deployed on performance-constrained edge computing devices.

## 4.5 Anomaly detection based on Swin-Transformer

Swin-Transformer [88] proposed by Microsoft is a universal Transformer architecture for CV tasks. Similar to DeiT, it is improved based on ViT and is mainly used to solve 2 major problems of Transformer in CV tasks:

- 1. In NLP tasks, a Token is used as the input, so the scale is relatively fixed, but the scale varies greatly in CV tasks;
- 2. CV requires higher processing resolution (higher fine-grained) than NLP tasks, and the computational complexity is often the square of NLP tasks. Transformer has difficulty adapting to pixel-level tasks on high-resolution images.