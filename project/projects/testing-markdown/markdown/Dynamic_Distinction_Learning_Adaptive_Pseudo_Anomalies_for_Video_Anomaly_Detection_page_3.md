Page 3

To improve the performance of AEs and UNets, some studies have incorporated supervised learning techniques, Generative Adversarial Networks (GANs), and Object Detection to refine the distinction between normal and anomalous samples. GANs, in particular, create a generativediscriminative adversarial relationship that enhances the model's ability to reconstruct outputs indistinguishable from the original input [12, 19, 31, 33, 37, 40]. Object Detection focuses the anomaly detection process on significant frame objects, albeit limited by the detection model's scope and accuracy [7, 11, 12, 19, 31, 31, 33, 34, 37, 40]. Memory modules have also been proposed to prevent anomaly reconstruction by referencing normal samples, suggesting enhanced model complexity as a pathway to more effective anomaly detection [7, 14, 23, 28, 30, 34, 35, 39].

The imbalance between normal and anomalous samples in datasets has necessitated the development of innovative approaches that introduce pseudo-anomalies. These methods are designed to enhance the capability of reconstruction-based models to distinguish between normal and anomalous samples with greater precision. Techniques for generating pseudo-anomalies vary widely, some strategies involve the use of external datasets to inject anomalies into a dataset of normal samples. This can involve leveraging attention mechanisms to identify and transfer key features from third-party datasets to normal samples, thus creating pseudo-anomalies [1], or introducing noise into the latent space of models using external data [24]. Other methods consider a more creative approach, which utilize the previous state of the model to generate lower quality reconstructions which would be represented as anomalous samples [41]. More traditional approaches attempt to invoke abnormality during training by directly providing the model with human defined anomalous behavior, such as reversing the sequence of input frames [2]. More recent pseudo anomalous methods attempt to attain superior results by injecting a suite of human defined anomalies, including the manipulation of video frames by reversing their sequence, skipping frames, adding noise, fusing frames, or incorporating random patches [3, 4]. Anomalies, regardless of their specific nature (skipping frames, repeating frames, introducing extraneous shapes, etc), are perceived by convolutional layer kernels as unusual collections of vector representations-noise. Despite their efficacy, these methods rely on manual intervention to simulate anomalies, requiring a subjective determination of the degree of anomaly introduced-raising the question, 'What constitutes the appropriate level of noise to be considered anomalous?'

Against this backdrop, our research introduces a sophisticated approach that not only incorporates the concept of dynamic anomaly weighting but also presents a novel distinction loss function. This methodology aims to advance the anomaly detection domain by providing a more refined mechanism for distinguishing between normal and anomalous events, thereby segueing into the detailed explanation of our proposed methodology outlined in Section 3.

## 3. Methodology

The Dynamic Distinction Learning (DDL) architecture is outlined in Figure 1. Consider a sequence of normal video frames represented as a tensor X ∈ R c × T × H × W , where c is the number of channels, T is the number of frames (which must be an odd number, as we will be reconstructing the middle frame), and H and W are the height and width of the frames respectively. To simulate anomalies over the sequence, we pass the model through an Object Detection and Tracking model, followed by Random Object Masking which selects a random tracked object across all frames and returns a sequence of binary masks M ∈ { 0 , 1 } c × T × H × W delineating the regions of the frames where the pseudoanomaly will be present. Alongside, we also introduce a noise tensor A ∈ R c × T × H × W , which is uniformly random generated.

We also define a trainable parameter ℓ ∈ R , which is passed through a sigmoid function, σ ( ℓ ) ∈ (0 , 1) , to represent the anomaly weight. We chose a sigmoid function to bound the trainable parameter between the values of 0 and 1 , so to portray a percentage of anomaly inflicted. The sequence of normal frames X , the masks M , the noise tensor A , and the anomaly weight σ ( ℓ ) are passed into the Pseudo Anomaly Creator to fabricate pseudo anomalies X A .

Both the sequence of normal frames X , and the pseudo anomalies X A , are passed through a reconstruction model and calibrated using a linear combination of the Reconstruction Loss and Distinction Loss. The anomaly weight is heavily calibrated by the Distinction Loss, a loss function designed to converge the anomaly weight to represent the minimum anomaly capable of being detected. The adaptability of the anomaly weight σ ( ℓ ) allows the model to dynamically calibrate the degree of anomaly present in the training data, ensuring an effective balance between the recognition of normal patterns and the detection of deviations. This is critical for preventing the model from either becoming desensitized to subtle anomalies or overreacting to minor irregularities, thus maintaining a nuanced representation of what constitutes an anomaly throughout the training process.

## 3.1. Pseudo Anomaly Creator

Our approach to fabricating pseudo-anomalies within video sequences begins with the application of object detection

and tracking at each frame, then randomly selecting an object from the set of tracked objects for masking. We employ object tracking to consistently mask the same object across all frames within a temporal window, T . These masks, denoted as M , are crucial in defining the regions for anomaly simulation, ensuring the anomalies are contextually integrated around objects.

Following the identification and masking of objects, we proceed to the creation of pseudo-anomalies, via the Pseudo Anomaly Creator, a two-step noise integration process shown in Figure 2. Initially, we generate noise-infused frames, X ¯ A , by blending the original input frames, X , with a noise tensor, A , using the dynamically learned anomaly weight, σ ( ℓ ) . This blend is achieved through a linear combination, ensuring the proportionate integration of noise and original content as per the following equation:

X ¯ A = (1 -σ ( ℓ )) · X + σ ( ℓ ) · A (1)