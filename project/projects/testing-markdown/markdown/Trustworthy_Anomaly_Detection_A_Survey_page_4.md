Page 4

Afew notions of fair anomaly detection are proposed, such as treatment parity or statistical parity [Shekhar et al. , 2021; Song et al. , 2021]. The basic idea is to ensure the anomaly detection result is invariant or independent of the protected attributes. Therefore, fairness can be evaluated based on the detection difference on protected and unprotected groups in terms of an anomaly detection metric such as AUC.

FairLOF [P and Abraham, 2020] is one pioneering work to tackle the fair anomaly detection problem with the focus on the local outlier factor (LOF) algorithm. FairLOF proposes several heuristic principles to mitigate unfairness via incorporating a correction term on LOF's distance function so that samples in under-represented groups have a similar distance distribution with the whole population.

In the category of one-class anomaly detection, Deep Fair SVDD enhances the Deep SVDD model with a fairness guarantee [Zhang and Davidson, 2021]. The approach adopts an adversarial fair representation learning model such that a discriminator cannot predict the protected attribute given representations derived by Deep SVDD. Therefore, the anomaly detection results are independent of the protected attribute.

In the category of reconstruction-based anomaly detection, two approaches based on autoencoder are developed. DCFOD adopts adversarial training to ensure the representations derived from autoencoder free from the protected attributes' information with the discriminator's guidance [Song et al. , 2021]. FairOD incorporates fairness regularization terms in addition to the regular reconstruction loss into the objective function to minimize the absolute correlation between the reconstruction error and protected attributes [Shekhar et al. , 2021].

## 5.2 Discussions

Currently, research on fair anomaly detection is still in its infancy. In the literature of fair machine learning, researchers have proposed a large number of fairness notions in categories of group fairness, sub-group fairness, and individual fairness. Current fair anomaly detection approaches only achieve group fairness in terms of demographic parity that requires equal detection rates across groups. Other fairness notions, especially individual fairness, are still under-exploited. Meanwhile, as minority groups could have a strong correlation with anomalies, correlation-based anomaly detection approaches could mislabel minority groups as anomalies. It is imperative to develop causation-based anomaly detection approaches that can provide a way to achieve causation-based fair anomaly detection.

## 6 Robust Anomaly Detection

Recent studies have demonstrated that machine learning models are vulnerable to intentionally-designed input samples from attackers, which can lead to catastrophic results.

There are two common attacking strategies: 1) data poisoning attacks that are conducted on the training time by injecting poisoned samples into training data to compromise the learning model; 2) evasion attacks that try to fool the detection models to make incorrect predictions on the testing phase by carefully designed samples [Chakraborty et al. , 2018; Zhang et al. , 2020]. Meanwhile, attacks can be further categorized into white-box attacks or black-box attacks based on the amount of information available to attackers. In whitebox attacks, the attacker has complete access to the model, including its structure and weights, and in black-box attacks, the attacker can only access the probabilities or the labels of some probed inputs.

One connection between adversarial attacks and anomaly detection is that adversarial samples can be considered as extremely hard-to-detect out-of-distribution samples [Bulusu et al. , 2021; Ruff et al. , 2021] because these samples are crafted to mislead learning models. Several studies have demonstrated that out-of-distribution detection can improve the model robustness against adversarial attacks [Hendrycks et al. , 2019]. While it is interesting to study model robustness through out-of-distribution detection, in this paper, we focus on reviewing adversarial attacks and defenses for anomaly detection itself, where the attacker aims to craft adversarial samples to evade detection or corrupt detection models.

## 6.1 Approaches

Some traditional one-class anomaly detection models, such as PCA, are sensitive to noisy samples in the training set. Some outliers in the normal training set may totally destroy the performance of detection models. Therefore, robust models, such as robust PCA or robust SVM, are developed to make detection models robust to noisy data that are arbitrarily corrupted. [Kloft and Laskov, 2010] studies the poisoning attacks in an online one-class anomaly detection scenario, where an anomaly score is derived based on a data point's distance to the center of normal samples. The goal of the attack is to make the detection model accept anomalies with large distances to the center. The theoretical analysis shows that if the attacker can control the training data more than a traffic ratio, the poisoning attack can succeed to push the center towards the attacking points. On the other hand, if the attacker can control only a small ratio of training data, the attack would fail with an infinite effort.

Recent studies have demonstrated that many popular white-box adversarial attack approaches can be adapted for attacking anomaly detection models, especially the reconstruction-based anomaly detection models, while common defense approaches for classifiers are not very effective for anomaly detection models [Lo et al. , 2021; Goodge et al. , 2020]. For example, the autoencoder-based anomaly detection models are vulnerable to many white-box attacking approaches, such as fast gradient sign method (FGSM) [Kurakin et al. , 2017] or projected gradient descent (PGD) [Madry et al. , 2018]. As a result, perturbations on anomalous samples

can cause the model to misclassify the anomalies as normal samples by making the reconstruction errors decrease rather than increase. To defend against such attacks, [Goodge et al. , 2020] develops an approximate projection autoencoder (APAE) model with the combination of two defense strategies, approximate projection and feature weighting. The approximate projection is to update the latent embeddings derived from the encoder to ensure the reconstructions have minimum changes, while the feature weighting is to normalize reconstruction errors to prevent the anomaly score from being dominated by the poorly reconstructed features. [Lo et al. , 2021] also applies the idea of updating latent embeddings and develops Principal Latent Space (PLS), which purifies the latent embeddings of autoencoder based on principal component analysis.

## 6.2 Discussions

From the attack perspective, there are very limited studies in literature focusing on the adversarial attack against anomaly detection models. Recently, [Xu et al. , 2020] develops a poisoning attack approach against RNN-based anomaly detection models and shows the attack can generate discrete adversarial samples to damage the RNN model with a few instances. Various adversarial attack strategies, such as exploratory attacks that try to gain information about the model states by probing the models [Chakraborty et al. , 2018] or backdoor attacks that create a backdoor into a learning model [Chen et al. , 2017], are not explored in the anomaly detection context. It is imperative to know how effective these potential adversarial attacks would be against anomaly detection models before they are launched by attackers in real-world applications.

From the defense perspective, existing studies focus on model-specific defending strategies for whit-box attacks. Other defending strategies are also worth exploring. For example, adversarial training is a widely-used strategy to improve model robustness against adversarial attacks by incorporating adversarial samples in the training set [Goodfellow et al. , 2015]. However, under the settings of unsupervised or one-class learning, we do not have anomalies. In such cases, how to craft adversarial samples, especially the anomalies in the purpose of evading detection, is challenging. It is also interesting to develop new anomaly detection approaches to detect adversarial samples that are crafted for anomaly detection models, e.g., based on the idea of using out-of-distribution detection models to detect adversarial samples from a polluted training set [Bulusu et al. , 2021].