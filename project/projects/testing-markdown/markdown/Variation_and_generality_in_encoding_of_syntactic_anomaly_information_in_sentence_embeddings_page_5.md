Page 5

Comparing between encoders, we see clear stratification in performance. InferSent shows the least anomaly awareness, performing for half of the tasks at chance level with the BoW baseline. GenSen and Skip-thoughts, by contrast, consistently occupy a higher performance tier, often falling not far behind (but never quite on par with) the highest level of performance. The latter distinction is reserved for BERT and RoBERTa, which show the strongest anomaly sensitivity on all tasks. All models show stronger performance on Verb-Ob than Mod-Noun , but the hierarchical difference between these tasks seems to have particularly significant impact for InferSent and Skip-thoughts, with cues relating to Verb-Ob seemingly encoded by InferSent, but cues relating to Mod-Noun seeming to be absent. Mod-Noun also yields the largest margin of difference between GenSen and Skip-thoughts. Since Skip-thoughts is one objective of GenSen, this suggests that the additional GenSen objectives provide an edge particularly for the finer-grained information needed for Mod-Noun .

BERT and RoBERTa emerge soundly as the strongest encoders of anomaly information, with RoBERTa also consistently outperforming BERT. While this is in line with patterns of downstream task performance from these models, it is noteworthy that these models also show superior perfor-

mance on these anomaly-detection tasks, as it is not obvious that encoding anomaly information would be relevant for these models' pre-training objectives, or for the most common NLU tasks on which they are typically evaluated.

## 6 Investigation on Generality of Anomaly Encoding

The above experiments suggest that embeddings from many of these encoders contain signal enabling detection of the presence of syntactic anomalies. However, these results cannot tell us whether these embeddings encode awareness that an 'error' is present per se-the classifiers may simply have learned to detect properties associated with a given anomaly, e.g., agreement mismatches, or occurrence of modifiers after nouns. In this sense, the above experiments serve as finer-grained tests of levels of hierarchical information available in these embeddings, but still do not test awareness of the notion of anomaly in general.

In this section we take a closer look at anomaly awareness per se, by testing the extent to which the sensitivities identified above are specific to individual anomalies, or reflective of a more abstract 'error' signal that would apply across anomalies. We explore this question by testing transfer performance between different anomaly types.

Transfer Results While in Section 5 we focused on examining anomaly-specific sensitivity in our new tasks-testing variation along fine-grained syntactic hierarchical distinctions and in a wordcontrolled setting-for examining generality of anomaly encoding it is worthwhile to take into account a broader range of anomaly types and datasets. For this reason we examine transfer between each of our generated probing tasks, as well as transfer to our tasks from established datasets: SOMO , BShift , and CoordInv from Conneau et al. (2018), and the CoLA task (Warstadt et al., 2019).

Fig. 2 shows transfer results from each dataset to each of our tasks. For ease of comparison, we also show the test result achieved when training on the same anomaly (the non-transfer result) in the white bars. We see that the majority of encoders show a marked drop in performance relative to the original non-transfer accuracy, and in fact most of the transfer results are approximately at chance performance. This suggests that the embeddings from these models encode information supporting detection of these anomalies, but the signals that

enable this detection are anomaly-specific. That is, they may encode some syntactic/semantic signal supporting detection of specific anomalies, but there is no indication that they encode a general awareness that 'there is an anomaly'.

The notable exceptions to this poor transfer performance are the transformer-based models, BERT and RoBERTa, which by stark contrast to the RNNbased encoders, show non-trivial transfer performance across all four of our generated tasks, regardless of the anomaly that the classifier is trained on. This suggests that to a much greater extent than any of the other encoders, BERT and RoBERTa may encode a more general 'error' signal, allowing for generalization across anomalies. Importantly, BERT and RoBERTa do also show some performance drop from non-transfer to transfer settingsso while they may encode a more generalized 'error' signal, this is likely in combination with encoding of anomaly-specific information that further aids performance on a given anomaly.

We note that transfer performance from CoLA is typically comparable to, or occasionally better than, training on the Conneau et al. tasks-despite the fact that CoLA has much smaller training data. CoLA is also the only task that contains a variety of anomalies for the model to learn from, rather than a single anomaly type as in all other datasets. This may enable faster, more generalizable learning on this small dataset-but of course, this would only be possible if a generalized anomaly signal is available in the embeddings. Following this reasoning, we also test whether jointly training on multiple anomalies improves transfer performance. The results of these multi-task transfer experiments can be found in Appendix Tables 7-8. These transfer results show overall a small decrease in performance relative to the one-to-one transfer, suggesting that training on single types of anomalies is not creating any major disadvantage for transfer performance. It may also indicate that mixed types of higher-level oddity in natural occurring anomalies from CoLA is not trivial to simulate by stacking together data with single type of anomalies as we do here.

The Conneau et al. task that most often shows the best transfer to our tasks (especially Mod-Noun and Verb-Ob ) is BShift . This is sensible, given that that task involves detecting a switch in word order within a bigram. Given this similarity, we can expect to see some transfer from this task even in the absence of generalized anomaly encoding.

Figure 2: Results on transfer settings, by test tasks. Different patterns/colors represent different training tasks. Results for non-transfer settings (same training and test tasks) are shown in white bars.