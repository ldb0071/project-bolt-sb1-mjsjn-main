Page 8

We can imagine various ways in which objectives involving prediction of words in context, as used by BERT and RoBERTa, could encourage learning of a generalized notion of syntactic anomaly. It may be the case that oddities occur together, in which case they could be mutually predictive and therefore of value for optimizing a prediction-based objective. More generally, anomalous sentences are likely identifiable as less probable, or more difficult to generate coherent predictions for. This relationship between anomaly and sentence probability raises a related question: Is this a problem for our conclusions here? Could models simply be identifying anomalous sentences as less probable, without any actual notion of syntactic anomaly? In NLP models, assessment of text probabilities is closely related to assessment of text naturalness and acceptability. For this reason, teasing apart general sensitivity to probability

versus genuine awareness of syntactic grammaticality phenomena is a recurring challenge when testing syntactic knowledge in language modelsand these things are similarly potentially entangled in our analyses here. To an extent this entanglement is inevitable and unproblematic: we necessarily expect syntactic anomalies to lower the probability of a string, and we can expect some awareness of syntactic anomaly to be important for assigning lower probability to an ungrammatical string. We can imagine, for instance, a situation in which a language model has no sensitivity to what constitutes good versus anomalous syntax, and thus assigns probability solely on the basis of word co-occurrence or other unrelated indicators of naturalness. In this sense, although it is not difficult to imagine how the close relationship between anomaly and sentence probability could be an explanation for findings that suggest anomaly awareness in these models, this does not change the fact that model representations may end up with genuine, detectable encoding of generalized anomaly information as a byproduct of probabilistic training-and this genuine anomaly encoding may be what we are detecting with our tests here. However, future work can examine further the relationship between syntactic anomaly and model perplexities, to explore whether these embeddings could show signs of anomaly sensitivity while in fact exclusively encoding confounding probabilistic information unrelated to syntactic anomaly.

Our content-word-only analysis provides one source of evidence on the relative importance of genuine syntactic sensitivity in success on our syntactic-related anomaly tasks. This test aims to tease apart the extent to which success on our tasks requires processing of finer-grained syntactic information, versus the extent to which models can succeed based on more superficial content word position information. We find in this analysis that most encoders do benefit from the finer-grained syntactic information provided by function words, supporting an important role for more advanced syntactic sensitivity in these tasks-however, we also find that substantial proportions of the observed detection accuracy can indeed be achieved with content words alone. To the best of our knowledge, we are the first to report this finding. This leaves us with two key takeaways. First, to an extent there is good reason to believe that a reasonable amount of genuine syntactic sensitivity is involved in the highest

levels of success on our anomaly tasks. Second, success on syntactic anomaly tasks can also be nontrivially inflated by use of more superficial cues. That is to say, as usual, these datasets have a habit of enabling learning from simpler cues than are intended. This takeaway highlights a need for caution in interpreting detection of reordering anomalies as evidence of deeper syntactic encoding per se (Conneau et al., 2018)-especially in probing datasets that use naturally-occurring data without exerting controls for confounding cues.

While these results have shed light on potential encoding of generalized syntactic anomaly knowledge in pre-trained models, there are many further questions to pursue with respect to these models' handling and understanding of such anomalies. We will leave for future work the problem of understanding in greater detail how model training may contribute to encoding of a generalized awareness of anomalies in text, how a genuine notion of syntactic anomaly could be further disentangled from general probability sensitivity, and how one could exploit models' awareness of anomaly for improving model robustness on downstream pipelines.

## 9 Conclusion

We have undertaken a direct study of anomaly encoding in sentence embeddings, finding impacts of hierarchical differences in anomaly type, but finding evidence of generalized anomaly encoding only in BERT and RoBERTa. Follow-up analyses support the conclusion that these embeddings encode a combination of generalized and anomaly-specific cues in these embeddings, with models appearing to leverage both finer-grained and coarser-grained information for anomaly detection. These results contribute to our understanding of the nature of encoding of linguistic input in embeddings from recent models. Future work can further explore the relationship between naturalness-oriented training and cultivation of abstract anomaly awareness, and how these insights can be leveraged for more robust and human-like processing of language inputs.

## Acknowledgments

We would like to thank several anonymous reviewers for helpful comments and suggestions that have led to improvements on earlier versions of this paper. We would also like to thank the members of the University of Chicago CompLing Lab, for valuable discussions on this work.

## References

Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. 2016. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. arXiv preprint arXiv:1608.04207 .

Matteo Alleman, Jonathan Mamou, Miguel A Del Rio, Hanlin Tang, Yoon Kim, and SueYeon Chung. 2021. Syntactic perturbations reveal representational correlates of hierarchical phrase structure in pretrained language models. arXiv preprint arXiv:2104.07578 .

Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. 2017a. What do neural machine translation models learn about morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 861-872.