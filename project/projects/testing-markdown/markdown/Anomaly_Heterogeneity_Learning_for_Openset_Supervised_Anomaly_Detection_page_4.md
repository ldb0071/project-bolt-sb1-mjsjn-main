Page 4

One main challenge of learning the underlying intricate abnormalities is the lack of training data that illustrate different possible anomaly distributions. Our HADG component is designed to address this challenge, in which we partition the normal examples into different clusters and associate each of the normal clusters with randomly sampled anomaly examples to create diverse anomaly distributions. The resulting distributions differ from each other in terms of normal patterns and/or abnormal patterns. Specifically, HADG generates T sets of training anomaly distribution data, T = {D i } T i =1 , with each D i = X n,i ∪ X a,i , where X n,i ⊂ X n and X a,i ⊂ X a . To simulate an anomaly distribution of good quality, X n,i should represent one main normal pattern. To this end, HADG adopts a clustering approach to partition X n into C clusters, and then randomly samples one of these C normal clusters as X n,i . On the other hand, to ensure the diversity of anomalies in each D i , X a,i is randomly drawn from X a and pseudo anomalies generated by popular anomaly generation methods [22, 60, 63].

Further, HADG utilizes those training data to create open-set detection and validation datasets for enabling the training of our model in a surrogate OSAD environment. Particularly, for each D i , HADG splits it into two disjoint subsets, i.e ., D i = {D s i , D q i } , which correspond to support and query sets respectively, with the support set D s i = X s n,i ∪X s a,i used to train our base model ϕ i and the query set

Figure 2. Overview of our approach AHL . Its HADG component first generates T heterogeneous anomaly distribution datasets from the training set, T = {D i } T i =1 , each of which includes a support set and open-set query set, i.e ., D i = {D s i , D q i } . It then utilizes them to learn T heterogeneous AD models { ϕ i } T i =1 in a simulated open-set environment and synthesizes these heterogeneous anomaly models into a unified AD model g ( · ) by a collaborative differential learning (CDL). Different ϕ i learn anomaly distributions of various quality, so we also devise a model ψ ( · ) to assign an importance score to each ϕ i to enhance the CDL component.

<!-- image -->

D q i = X q n,i ∪X q a,i used to validate its open-set performance. To guarantee the openness in the validation/query set D q i , we perform sampling in a way to ensure that X s n,i and X q n,i are two different normal clusters, and X s a,i and X q a,i do not overlap with each other, i.e ., X s a,i ∩ X q a,i = ∅ .

## 3.3. Collaborative Differentiable Learning of the Anomaly Heterogeneity

Our CDL component aims to first learn heterogeneous anomaly distributions hidden in T = {D i } T i =1 by using T base model ϕ i and then utilize these models to collaboratively optimize the unified detection model g in an end-toend manner. CDL is presented in detail as follows.

## Learning T Heterogeneous Anomaly Distributions.

Wefirst train T base models { ϕ i } T i =1 to respectively capture heterogeneous anomaly distributions in {D i } T i =1 , with each ϕ i optimized using the following loss:

L ϕ i = |D s i | ∑ j =1 ℓ dev ( ϕ i ( x j ; θ i ) , y j ) , (1)