Page 45

Many methods combine Transformer model or its attention mechanism with other methods for anomaly detection tasks, which often complement the advantages of the Transformer, thus enhancing the overall performance of the model. These methods are collectively referred to as hybrid models in this paper. Mori et al. [124] used the attention mechanism in Transformer for reference and applied it to RNN for anomaly noise detection. They used the model on multiple missing frames of a given log-scale MEL spectrum to compute the reconstruction errors of predicted and deleted frames as anomaly scores. TADDY [125] combines the Vanilla Transformer with a discriminator in GAN to detect anomalies in dynamic graphs. TADDY mainly addresses two key difficulties in dynamic graph anomaly detection: the lack of informative encoding for unattributed nodes and the difficulty of learning to discriminate knowledge from coupled spatial-temporal dynamic graphs. TADDY designs a comprehensive node encoding that is inspired by the Position Encoding of Transformer model. It can simultaneously extract global spatial, local spatial, and temporal information, and integrate learnable mapping functions to help the framework automatically extract information encoding end-to-end. TADDY uses Transformer to learn spatial-temporal knowledge,

adopts an edge-based substructure sampling method, takes cross-time contextual information as input, and then uses an attention mechanism to extract spatial-temporal coupling information. Xu et al. [47] applied the minimax optimization strategy to Transformer. They alternately stacked anomalous attention blocks and feed-forward layers, which facilitated learning the underlying associations from deep multi-level features, and proposed anomalous attention modules with two branching structures.

Liu et al. [126] combined heterogeneous information networks with Transformer for anomaly detection in smart contracts. They first extracted features to construct HIN and applied them to smart contracts. Then, they obtained the relationship matrix through the meta path learned by Transformer and used it as the input of CNN. Finally, they used node embedding for classification tasks. They adopted Doc2Vec to preprocess and convert the codes into vectors by using account features and code features as node attributes. Transformer is mainly used for encoding and decoding, converting the input data into output embedding. Experiments demonstrate the feasibility of Transformer for blockchain security applications. DCT-GAN [49] uses the Transformer's encoder block to extract features in time series with additional GP and Lipschitz constraint methods. However, DCT-GAN does not fully exploit the performance of Transformer. It needs to first downscale into single-dimension data through PCA when processing MTS data, which can be very well solved by MHA in Transformer.

Intra [127] is a self-supervised approach by performing the Inpainting task, i.e., covering certain regions of the image and then recovering them. Similar to other schemes used for image anomaly detection, Intra also separates the anomaly detection tasks into image-level fine-grained (anomaly detection) and pixel-level fine-grained (anomaly location). Intra uses both global and local Position Encoding because the global position information of image blocks does not need to be considered for texture class images but for some other types of images. Intra further introduces the MLP method for nonlinear dimensionality reduction because the authors found that the attention weights calculated by Vanilla Transformer were almost equivalent due to the similar features of different image blocks. This method is called Multi-head feature self-attention (MFSA). Although this scheme improves the accuracy of the model, it further increases the number of parameters in Transformer and prolongs the training time. GTA [54] is sampled by the GumbelSoftmax method to solve the problem that the BPTT algorithm could not be used due to the non-differentiable problem of discrete data sampling in classification distribution. They used Influence Propagation Convolution and Transformer's architecture to model time dependencies. Hierarchical extended convolution can effectively model sequences, set the multiscale dilation size, and explore the temporal context modeling process with different sequence lengths and receptive fields. GTA re-modified Transformer's MHA and proposed a method called multi-branch attention. Multi-branch attention consists of two attention branches, a convolution branch for extracting information from a restricted neighborhood and another MHA branch for capturing long-range dependencies, and a multi-branch mixing strategy is proposed, which combines pairwise tokens with global learning attention.

Wang et al. [34] combined BERT and VAE to detect anomalies in log sequences. They used the Fast Gradient Method (FGM) to perturb the embedding layer of BERT model, generated the semantic features, and reduced the distance between the semantic features generated by log sequences before and after the perturbation of the embedding layer (a special method to enhance the robustness of the model). In addition, Contrastive Learning (making the semantic features generated by normal and anomaly log sequences have a larger gap and a longer distance in the semantic space) and VAE are used to extract statistical results to obtain statistical features. The statistical features and semantic features are combined to obtain enhanced features to train the model. Finally, MLP is used for anomaly detection tasks. However, this method configures different hyperparameters for different datasets to achieve optimal results, which has led to questions about the universality of the model. Li et al. [128] proposed MFVT, a hybrid model combining FusionNet (essentially CNN structure) and ViT for anomalous traffic detection tasks. They fed the preprocessed data into FusionNet and ViT for training, iterated through Max Iteration, and finally obtained the trained model for evaluation using the Confusion Matrix. Their experiments illustrate that the combination of ViT and FusionNet can effectively address the challenges posed by unbalanced datasets, while also reducing the demand for sample data resources during training.