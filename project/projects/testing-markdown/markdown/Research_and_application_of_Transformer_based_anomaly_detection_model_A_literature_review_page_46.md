Page 46

Zhang et al. [129] applied Transformer's MHA to the Adversarial Learning OCC (ALOCC) model. MHA can effectively increase the difference between IC (Inlier class) and OC (Outlier class) samples, and solve the training instability problem of ALOCC. Gundawar et al. [130] also noticed the excellent performance of MHA. They combined MHSA with LSTM and introduced Non-Parametric Dynamic Thresholding to classify certain points as anomaly thresholds, to realize anomaly detection of Spacecraft Telemetry. They also added improvements to the sin and cos Position Encoding functions of the Vanilla Transformer. By introducing the Time2Vec model [131], they expressed the K + 1 input time series as Fourier series. Logsy [132] is a method combining the self-attention mechanism with auxiliary data to improve log vector representation and perform unstructured log anomaly detection tasks. Logsy modifies the objective function through hyperspherical decision boundaries to achieve compact data representation and distance-based anomaly scoring. The author also used NLTK instead of existing log parsers to preprocess logs. Pereira et al. [133] applied the Bi-LSTM model based on an attention mechanism to the task of anomaly detection of energy time series data. They adopted VAE and VASM mechanisms to introduce attention to the model. To enhance the robustness of the model, they also added noise to the input.

Yuan et al. [134] combined ViT with U-Net and GAN, introducing TransAnomaly for video anomaly detection. TransAnomaly utilizes U-Net to encode spatial information and Transformer encoder to encode temporal information. It predicts future frames and calculates the difference with ground truth for anomaly detection. The Adapter of TransLog [63] has a lightweight structure, consisting of a simple projection layer inserted in the middle of Transformer layer. When adjusting the model on downstream tasks, only the Adapter parameters are updated and the weight of the pre-trained model is frozen, which greatly reduces the trainable parameters of the model, but achieves considerable performance for transfer learning tasks. Zhang et al.

[135] combined Transformer with VAE for MTS anomaly detection tasks. They applied this hybrid model to a nonlinear state space, which can reduce the computational complexity, allow parallelization, and provide interpretable insights.

MT-RVAE [55] improves the position encoding based on the Vanilla Transformer by using a global temporal encoding to add time series and period information to the data. This enhancement enables the model to capture long-term dependencies more effectively. Additionally, MT-RVAE introduces a multi-scale feature fusion algorithm for time series data. By integrating features from multiple time scales, the algorithm compensates for detailed information lost during the upsampling process, resulting in a more robust feature representation. Wang et al. proposed AnoDFDNet [136], a fusion model combining convolution and ViT for image anomaly detection of high-speed trains. They argued that the pre-trained models cannot detect 'invisible' images, so AnoDFDNet does not employ the pre-training mechanism. AnoDFDNet can better distinguish the feature differences between anomaly images and normal images by detecting the divergence between two images taken at different times in the same area. The RT-SemiVAE model proposed by Chen et al. [137] is designed for semi-supervised anomaly detection and localization in multivariate time series data. They achieved this by learning the long-term dependence of the data using a parallel multihead attention mechanism in the Transformer. Additionally, they utilized LSTM to capture shortterm dependence. The introduction of parallel computing significantly reduces model training time.

Tian et al. [138] combined Transformer with MAE model [139] and proposed a hybrid model called MemMC-MAE. The encoder part of MemMC-MAE incorporates a memory-enhanced self-attention operator, while the decoder utilizes a multi-level cross-attention mechanism. This combination allows for the correlation between normal patterns stored in the encoder memory and multiple normal patterns in the image to be effectively captured. By leveraging this correlation, MemMC-MAE can accurately reconstruct anomalies, resulting in high reconstruction errors. To address the issue of low anomaly image reconstruction error, the anomaly score in MemMC-MAE adopts the multi-scale structural similarity (MSSSIM) metric. Ma et al. [140] first combined Transformer with federated learning techniques to perform privacy-preserving anomaly detection in cloud manufacturing. To avoid the conflict between Transformer and federated learning protocols, they designed a new collaborative learning protocol. Specifically, each edge device has a local encoder composed of Transformer structure, which extracts important anomaly data feature representations and uploads these encoded features with differential privacy noise to the cloud. The cloud contains a decoder with a MLP structure, which can distinguish between normal and anomalous features uploaded by edge devices. Further, the author used Gaussian noise to score the uploaded features instead of the raw data to detect anomalies, thus enhancing the privacy-protection ability of the framework and reducing communication overhead during training. However, the author indicated that the current model combining Transformer model and federated learning still has many shortcomings. For example, Transformer focuses too much on extracting more important features, which leads to a lower overall convergence speed of the model, and the distributed training protocol further limits the convergence speed. However, it is undeniable that the results