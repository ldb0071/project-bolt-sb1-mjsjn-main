Page 6

Training and Inference . During training, the feature mapping network f is shared and jointly trained by all the

four heads g s , g p , g r and g n . These four heads are independent from each other, and so their parameters are not shared and independently optimized. A loss function called deviation loss [35, 37] is used to implement the loss function â„“ in all our heads by default, as it enables generally more stable and effective performance than other loss functions such as cross entropy loss or focal loss (see Appendix C.2 ). During inference, given a test image, we sum of all the scores from the abnormality learning heads ( g s , g p and g r ) and minus the score from the normality head g n to obtain its anomaly score.

## 4. Experiments

Datasets Many studies evaluate their models on synthetic anomaly detection datasets converted from popular image classification benchmarks, such as MNIST [25], Fashion-MNIST [64], CIFAR-10 [24], using one-vs-all or one-vs-one protocols. This conversion results in clearly disparate anomalies from normal samples. However, anomalies and normal samples in real-world applications, such as industrial defect inspection and lesion detection in medical images, typically have only subtle/small difference. Motivated by this, following [26, 35, 65], we focus on datasets with natural anomalies rather than one-vs-all/onevs-one based synthetic anomalies. Particularly, nine diverse datasets with real anomalies are used in our experiments, including five industrial defect inspection datasets: MVTec AD [3], AITEX [50], SDD [53], ELPV [9] and Optical [63], in which we aim to inspect defective image samples; one planetary exploration dataset: Mastcam [22] in which we aim to identify geologically-interesting/novel images taken by Mars exploration rovers; and three medical image datasets for detecting lesions on different organs: BrainMRI [47], HeadCT [47] and Hyper-Kvasir [5]. These datasets are popular benchmarks in the respective research domains and recently emerging as important benchmarks for anomaly detection [4, 19, 35, 47, 65] (see Appendix A for detailed introduction of these datasets).

## 4.1. Implementation Details

DRA uses ResNet-18 as the feature learning backbone. All its heads are jointly trained using 30 epochs, with 20 iterations per epoch and a batch size of 48. Adam is used for the parameter optimization using an initial learning rate 10 -3 with a weight decay of 10 -2 . The topK MIL in DRA is the same as that in DevNet [35], i.e ., K in the topK MIL is set to 10% of the number of all scores per score map. N r = 5 is used by default in the residual anomaly learning (see Sec. 4.6). The pseudo abnormality learning uses CutMix [67] to create pseudo anomaly samples on all datasets except the three medical datasets, on which DRA uses external data from another medical dataset LAG [27] as the pseudo anomaly source (see Sec. 4.6).

Our model DRA is compared to five recent and closely related state-of-the-art (SotA) methods, including MLEP [30], deviation network (DevNet) [35, 37], SAOE (combining data augmentation-based Synthetic Anomalies [26, 32, 54] with Outlier Exposure [18, 42]), unsupervised anomaly detector KDAD [47], and focal loss-driven classifier (FLOS) [28] (See Appendix C.1 for comparison with two other methods [45,62]). MLEP and DevNet address the same open-set AD problem as ours. KDAD is a recent unsupervised AD method that works on normal training data only. It is commonly assumed that unsupervised detectors are more preferable than the supervised ones in detecting unseen anomalies, as the latter may bias towards the seen anomalies. Motivated by this, KDAD is used as a baseline. The implementation of DevNet and KDAD is taken from their authors. MLEP is adapted to the image task with the same setting as DRA. SAOE utilizes pseudo anomalies from both data augmentation-based and outlier exposurebased methods, outperforming the individuals that use one of these anomaly creation methods. FLOS is an imbalanced classifier trained with focal loss. For a fair comparison, all competing methods use the same network backbone ( i.e ., ResNet-18) as DRA except KDAD that requires its own special network architecture to perform training and inference. Further implementation details of DRA and its competing methods are provided in Appendix B .

## 4.2. Experiment Protocols

We use the following two experiment protocols:

General setting simulates a general scenario of open-set AD, where the given anomaly examples are a few samples randomly drawn from all possible anomaly classes in the test set per dataset. These sampled anomalies are then removed from the test data. This is to replicate real-world applications where we cannot determine which anomaly classes are known and how many anomaly classes the given anomaly examples span. Thus, the datasets can contain both seen and unseen anomaly classes, or only the seen anomaly classes, depending on the underlying complexity of the applications ( e.g ., the number of all possible anomaly classes).

Hard setting is designed to exclusively evaluate the performance of the models in detecting unseen anomaly classes, which is the very key challenge in open-set AD. To this end, the anomaly example sampling is limited to be drawn from one single anomaly class only, and all anomaly samples in this anomaly class are removed from the test set to ensure that the test set contains only unseen anomaly classes. Note that this setting is only applicable to datasets with no less than two anomaly classes.