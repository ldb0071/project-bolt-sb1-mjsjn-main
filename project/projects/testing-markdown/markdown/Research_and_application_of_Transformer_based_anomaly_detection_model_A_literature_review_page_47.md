Page 47

strongly demonstrate the feasibility of combining Transformer with federated learning and provide a new research idea for the field of privacy protection anomaly detection.

Bi-Transformer Anomaly Detection method [141] first introduces the Binary Transformer architecture, which extracts temporal data correlation features from both the meta sequence and time-step sequence dimensions. It enhances detection performance through techniques such as generative adversarial training and MAML. This Binary Transformer architecture exhibits strong generalization capabilities and achieves highprecision detection on multiple time series datasets. Lakha et al. [142] combined Transformer with GNN for anomaly detection in Cybersecurity events. They first used a GNN to produce representations of each event, which encode information about their neighboring events in an unsupervised manner. They then utilized complex features, such as command arguments that exhibit significant variation, which cannot be directly used as features in typical machine learning algorithms.

Kong et al. [143] also combined Transformer with GAN for anomaly detection tasks in multivariate time series data. They modified the Transformer architecture and proposed the active distortion transformer (ADT) to capture temporal dependence and anomalous features by leveraging prior knowledge of overall associations. AnoFed proposed by Raza et al. [144], combines the Transformer-based Autoencoder and VAE with Support Vector Data Description (SVDD) in a federated setting for anomaly detection in Electrocardiogram (ECG) images. They focused on the computational cost, making AnoFed efficient and lightweight, allowing it to be deployed on resourceconstrained edge computing devices. Li et al. [145] introduced the Memory-Token Transformer (MTT) to boost the reconstruction performance on normal frames. MTT takes the feature map and the memory module as input. A Transformer is then used to model the internal relationship between different memory tokens, projecting the final token to the size of the original feature map.

Qin et al. [146] combined the Transformer model with signal decomposition. They provided a multi-view embedding method to capture temporal and correlational features of signals. They also designed a frequency attention module to extract periodic oscillation features. Finally, the signals were decomposed into seasonal, trend, and remainder components for further separate representation. The DADF [147] model proposes, for the first time, the combination of Dual-Attention Transformer, Convolutional Block, and Discriminative flow. It integrates self-attention and memorial-attention mechanisms, incorporating sequential and normality associations. This integration enables the model to capture high-level global and local structural semantics of the inspected object. This approach innovatively gets rid of the limitations of previous reconstruction-based or distance-based anomaly detection methods, which tend to have high misclassification rates. By utilizing normalizing flow to learn a discriminative model over the joint distribution of the original features and two reconstructed features, it obtains the normality likelihood. This enables the simultaneous detection of local structural defects and global logical defects in anomaly detection tasks. RDAD [148] combines the ideas of GAN and Transformer, composed of a reconstructive subnetwork and a discriminative subnetwork. The reconstructive subnetwork learns to reconstruct anomalous images without anomalies. In the discriminative subnetwork, the anomalous images and the reconstructed images obtained in

the reconstructive subnetwork are concatenated as input. The reconstructed images assist the discriminative subnetwork in better inferring the anomaly of the input samples. RDAD further incorporates the squeeze-and-excitation block to enhance the sensitivity of relevant features by assigning attention to feature channels.