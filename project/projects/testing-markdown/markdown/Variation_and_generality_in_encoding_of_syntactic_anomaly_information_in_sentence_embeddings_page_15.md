Page 15

|             | Exclusive in Mod-Noun                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|-------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| BERT        | The buildings here were all a lovely misty gray , which gave them a dreamlike quality . There are terrible slums in London Daisy , places you 'd never want to visit .                                                                                                                                                                                                                                                                                                                                                 |
| RoBERTa     | Suddenly , his senses sharpened and he felt less inebriated . All the charts are in drawers below the table .                                                                                                                                                                                                                                                                                                                                                                                                          |
| observation | Most of the samples involve a construction which is 'seemingly a noun followed by a modifier format'. For BERT, the samples seem to involve multiple adjectives in a row, where the final word is more frequently to be an adjective generally, and is more clear following the Mod-Noun -specific detection rules. For RoBERTa, e.g., 'drawers below the table', 'drawers' actually belongs to another prepositional phrase 'in drawers' which is parallel to the followed by prepositional phrase 'below the table'. |
|             | Exclusive in Verb-Ob                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| BERT        | Slowly , the gatehouse rose . Through their windows , thick candles spread throughout flickered softly .                                                                                                                                                                                                                                                                                                                                                                                                               |
| RoBERTa     | A satisfactory rate of exchange I feel . The entire column stretched back almost as far as the eye could see .                                                                                                                                                                                                                                                                                                                                                                                                         |
| observation | A clear pattern across both encoders: the error samples involve fronting such as prepositional phrase-fronting or object-fronting, or involve constructions end with a verb/verb phrase (thus not with a standard SVO structure).                                                                                                                                                                                                                                                                                      |
|             | Exclusive in SubN-ObN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| BERT        | The object changed as he spoke . The bases were wide , and as the buildings climbed into the sky , they became narrower and branched off to connect to other buildings .                                                                                                                                                                                                                                                                                                                                               |
| RoBERTa     | That joint will help you sleep . The stealth assassin never belonged , but the reason will shatter his every conviction .                                                                                                                                                                                                                                                                                                                                                                                              |
| observation | For both encoders, the subject word of the sampled sentence is always an uncommon subject word.                                                                                                                                                                                                                                                                                                                                                                                                                        |
|             | Exclusive in Agree-Shift                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| BERT        | Not even the vendors who stood at their little shops or at their carts and called out their specials cared that I was there . The Tiger Man , still awake , regarded her with groggy eyes .                                                                                                                                                                                                                                                                                                                            |
| RoBERTa     | My brows went up . A humorless laugh escaped his mouth and all I could do was stand mute , my heart breaking .                                                                                                                                                                                                                                                                                                                                                                                                         |

Table 6: Error rate on Subj-Num data, with a total size of 10,000 sentences (test set). The top rows show the results when the training on original tasks, while the bottom rows show the results when training on contentword-only tasks.

|            | original          | original          | original          | original          |
|------------|-------------------|-------------------|-------------------|-------------------|
| train task | Mod-Noun          | Verb-Ob           | SubN-ObN          | Agree-Shift       |
| BERT       | 4.9%              | 2.61%             | 4.81%             | 31.98%            |
| RoBERTa    | 7.73%             | 3.6%              | 7.83%             | 22.13%            |
|            | content-word-only | content-word-only | content-word-only | content-word-only |
| train task | Mod-Noun          | Verb-Ob           | SubN-ObN          | Agree-Shift       |
| BERT       | 1.82%             | 3.75%             | 2.0%              | 22.86%            |
| RoBERTa    | 0.96%             | 1.64%             | 1.54%             | 4.92%             |

Probing data generation We use the premise sentences from the train, dev-matched, devmismatched datasets of MultiNLI, with repeats dis-

## carded according to the promptID. 12 13

We adopt an approach that we refer to as 'exhaustive' perturbation: modifying all instances of a given structure within a sentence, to ensure that sentences have internal structural consistency-e.g., a perturbed sentence in Mod-Noun will not contain both 'modifier+noun' and 'noun+modifier' structures-thus avoiding inconsistency serving as an extra signal for detection. 14

For each task, we use training data of 71k sentences, and dev and test data of 8.9k sentences.

For the SubN-ObN and the Mod-Noun tasks, 15

Table 7: Transfer results of multi-one transferring among our generated tasks with consistent training size. The amount of training size of multi-task training is consistent with one-one transferring. The columns of ∆ show how much the multi-one transferring improves or drops from the best one-one result. The improvements (positive ∆ values) are bolded.

| Encoder/Train task   | multi-task ( Verb-Ob + SubN-ObN + Agree-Shift )   | multi-task ( Verb-Ob + SubN-ObN + Agree-Shift )   | multi-task ( Mod-Noun + SubN-ObN + Agree-Shift )   | multi-task ( Mod-Noun + SubN-ObN + Agree-Shift )   | multi-task ( Mod-Noun + Verb-Ob + Agree-Shift   | )      | multi-task ( Mod-Noun + Verb-Ob + SubN-ObN )   | multi-task ( Mod-Noun + Verb-Ob + SubN-ObN )   |
|----------------------|---------------------------------------------------|---------------------------------------------------|----------------------------------------------------|----------------------------------------------------|-------------------------------------------------|--------|------------------------------------------------|------------------------------------------------|
|                      | Mod-Noun                                          | ∆                                                 | Verb-Ob                                            | ∆                                                  | SubN-ObN                                        | ∆      | Agree-Shift                                    | ∆                                              |
| BoW                  | 50.0                                              | 0.0                                               | 50.0                                               | 0.0                                                | 50.0                                            | 0.0    | 50.0                                           | 0.0                                            |
| InferSent            | 50.011                                            | 0.011                                             | 54.61                                              | 0.012                                              | 51.369                                          | 0.92   | 54.337                                         | -0.1                                           |
| Skip-thoughts        | 53.298                                            | -0.83                                             | 63.022                                             | 6.001                                              | 51.357                                          | -0.46  | 55.289                                         | 0.034                                          |
| GenSen               | 55.531                                            | -1.772                                            | 60.61                                              | 2.456                                              | 52.984                                          | -0.213 | 54.729                                         | 1.401                                          |
| BERT                 | 73.996                                            | -0.101                                            | 83.614                                             | -0.919                                             | 72.616                                          | 0.09   | 71.257                                         | 0.941                                          |
| Multi-task en-en     | 53.455                                            | 0.123                                             | 54.587                                             | -2.804                                             | 52.984                                          | -0.247 | 51.154                                         | 0.258                                          |
| Multi-task en-fr     | 52.165                                            | -0.797                                            | 53.41                                              | -0.28                                              | 52.244                                          | -1.054 | 52.824                                         | -0.258                                         |
| Multi-task en-de     | 52.008                                            | -0.381                                            | 54.419                                             | 0.179                                              | 52.333                                          | -0.73  | 52.387                                         | -0.863                                         |

Table 8: Transfer tasks jointly trained on multi-task learning with all of Conneau et al. tasks, tested on each of our generated tasks, with consistent training size.

|                    | multi-task ( SOMO + BShift + CoordInv )   | multi-task ( SOMO + BShift + CoordInv )   | multi-task ( SOMO + BShift + CoordInv )   | multi-task ( SOMO + BShift + CoordInv )   | multi-task ( SOMO + BShift + CoordInv )   | multi-task ( SOMO + BShift + CoordInv )   | multi-task ( SOMO + BShift + CoordInv )   | multi-task ( SOMO + BShift + CoordInv )   |
|--------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|
| Encoder/Train task | Mod-Noun                                  | Mod-Noun                                  | Verb-Ob                                   | Verb-Ob                                   | SubN-ObN                                  | SubN-ObN                                  | Agree-Shift                               | Agree-Shift                               |
|                    | acc                                       | ∆                                         | acc                                       | ∆                                         | acc                                       | ∆                                         | acc                                       | ∆                                         |
| BOW                | 50.0                                      | 0.0                                       | 50.033                                    | 0.033                                     | 50.017                                    | 0.017                                     | 50.0                                      | 0.0                                       |
| Infersen           | 50.067                                    | -0.833                                    | 50.217                                    | -2.316                                    | 50.1                                      | -1.2                                      | 49.933                                    | -0.5                                      |
| Skip-thoughts      | 57.767                                    | -2.666                                    | 57.733                                    | -5.05                                     | 51.3                                      | -0.85                                     | 50.383                                    | -0.4                                      |
| GenSen             | 58.533                                    | -4.75                                     | 58.867                                    | -4.75                                     | 50.533                                    | -1.834                                    | 50.783                                    | -1.617                                    |
| BERT               | 69.883                                    | -3.967                                    | 73.683                                    | -5.1                                      | 69.317                                    | -0.75                                     | 62.367                                    | -1.55                                     |
| Multi-task en-en   | 53.283                                    | 0.283                                     | 54.0                                      | -0.533                                    | 52.617                                    | 0.134                                     | 51.017                                    | 0.084                                     |
| Multi-task en-fr   | 50.767                                    | -0.933                                    | 52.5                                      | -5.467                                    | 51.85                                     | -0.45                                     | 51.05                                     | 0.117                                     |
| Multi-task en-de   | 49.7                                      | -3.4                                      | 50.167                                    | -5.3                                      | 50.3                                      | -1.167                                    | 50.033                                    | -0.5                                      |

sometimes the case might arise that the resulting perturbed sentences are still normal or acceptable, but perhaps somewhat stranger or less probable to occur in the wild, e.g., 'man bites dog'. However, this should be a rare case, as the original sentences are long enough to involve adequate context to distinguish normal from perturbed examples.