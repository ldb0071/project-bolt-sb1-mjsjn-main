Page 44

The author's final evaluation results show that although the pre-trained BERT models are moderately robust to different OOD detection tasks, there is still room for further research and development. Podolskiy et al. took Hendrycks' work one step further and proposed an OOD detection scheme using Mahalanobis distance in BERT [121]. Through the fine-tuned BERT model, they could construct homogeneous representations of in-domain utterances and reveal geometric differences from out-of-domain utterances. which are easily captured by the Mahalanobis distance. While the finetuned BERT can allow a task-friendly reshaping of the embedding space. However, the authors argued that this method still suffers from its detection limitations. Firstly, it depends on the geometric features of the embedding space. For example, if the embedder is used as a classification model and overfitting at the same time, the geometric features of the embedding space may be destroyed. In addition, the biggest challenge of this approach is the semantically similar utterances, one of which is ID and the other is OOD. Xu et al. [58] argued that previous models extract features from a certain layer in BERT, but this approach has performance limitations because the features of each layer in BERT model are different. If the features of all layers in BERT model are extracted directly, the efficiency is too low. If some feature dimension aggregation methods are used, such as Max pooling, information is sacrificed for efficiency. Therefore, the author used Mahalanobis distance to extract the potential representations of all layers, and automatically determine which layers' features are important. Also, they used two methods, In-domain masked language modeling (IMLM) and Binary Classification with Auxiliary Datasets (BCAD) to fine-tune BERT within the domain. Experimental results show that pre-trained BERT can produce better feature representation. However, the simple aggregation of hierarchical structure is not very effective, while the fine-tuning approach further improves the overall performance of BERT.

## 4.11 Anomaly detection based on other models

In addition to the above methods, some researchers have proposed other variants of Transformer for different types of anomaly detection tasks. Although the methods themselves may have defects, these models have brought pioneering thinking to this research field. Network Transformer [61] first uses a packet analyzer to group packets based on their source and destination addresses, uses Transformer to obtain an embedded representation, and then uses an aggregator to extract a series of hierarchical network features representing the traffic graph. By predicting n future packets given several packet sequences, and training the anomaly detection algorithms at different abstraction levels (global, node, edge), it can facilitate anomaly detection and

information extraction at different fine-grained levels, and enhance the learning ability of the system. Network Transformer can be applied to three different anomaly detection algorithms, namely LOF, OCSVM, and Autoencoder. Lee et al. [122] investigated the performance of different neural network models including Transformer on the Virtual Network Function Chains anomaly detection task. The proposed model consists of a feature mapping layer, an encoder layer, a readout layer, and a classifier layer. At the encoder level, they compared several different models such as Transformer, Uni-RNN, Bi-RNN, etc. At the readout level, they tested several different functions including max , mean , and self -attention . They tested in two scenarios (i.e., web hosting service scenario and login authentication scenario), and considered the performance of joint training, i.e., training Transformer with several datasets at the same time. Experimental results fully demonstrated the robustness and superiority of Transformer. Skeleton-Transformer [123] is a Transformer model combining MSA and Temporal Convolution Layer (TCL) for video anomaly detection tasks. MSA module captures the long-term dependencies of arbitrary paired pose components in spatial and temporal dimensions from different perspectives, while TCL focuses on local time information. Finally, the error between the predicted posture component and the corresponding expected value is used as the anomaly score. However, the disadvantage of this method is that it has very strict requirements on the datasets because the 2D skeleton trajectory extraction in the pre-processing stage must rely on high-quality video. Transformer for the Data Access Semantics (Trans-DAS) model in UCAD [60] is improved based on Vanilla Transformer. The attention block of Trans-DAS uses a new masking mechanism, which connects an operation with its bidirectional operation context. The masking mechanism prevents inferring the semantics of the operation directly from the operation itself and allows Trans-DAS to capture the contextual intent of the operation based on its bidirectional context.

## 4.12 Anomaly detection based on hybrid models