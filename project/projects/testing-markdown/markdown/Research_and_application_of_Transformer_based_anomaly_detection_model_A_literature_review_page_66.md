Page 66

## 7.1 Problems of existing research methods

## 7.1.1 Pre-trained model VS Non-pre-trained model

From the current research, researchers have disagreed on whether Transformer model needs to be pre-trained. Some researchers believe that Transformer itself has the defects of many parameters, slow training speed, and excessive resource consumption. However, most parameters of Transformer can be frozen through pre-training, and only a few parameters need to be fine-tuned according to the actual task during the formal training stage. As mentioned in the previous section, some researchers have designed their fine-tuning schemes, such as Adapter [63]. However, some researchers have also started training the model with only initialized parameters for different anomaly detection tasks, and even fine-tuned the hyperparameters according to different test datasets, to achieve higher performance on different tasks.

In this paper, we argue that to solve this problem, it should depend on the specific task requirements. For some anomaly detection tasks that are easy to perform

transfer learning and have similar features, such as log anomaly detection and unstructured text anomaly detection, etc., a pre-trained Transformer should be used. Through the pre-training stage, the model can learn in advance some common features of domain-related tasks, such as log templates. In the formal training stage, the model only needs to be fine-tuned according to the actual task requirements, thus greatly improving the training efficiency of the model. In addition, the pre-training stage also helps the model obtain more prior information and improve the model's decisionmaking performance. Pre-trained models can also perform transfer learning strategies to adapt to more anomaly detection tasks within the same domain. For example, Bozorgtabar et al [157]. proposed a Masked Image Modeling (MIM) strategy, namely Attention-conditioned Patch Masking (APMask) for ViT-based pretraining by masking non-salient image patches that help the model to capture the local semantics while preserving the crucial structure associated with the foreground object. They further harnessed the self-attention map extracted from the Transformer to mask non-salient image patches without destroying the crucial structure associated with the foreground object. Subsequently, the pre-trained model is fine-tuned to detect and localize simulated anomalies generated under the guidance of the Transformer's self-attention map. However, pre-trained models must be supported by high-quality datasets to achieve significant results. For researchers, how to collect massive data suitable for model pre-training and how to choose the appropriate transfer learning strategy are the key issues they have to consider. The pre-training step should not be performed for those anomaly detection tasks with significant feature variations, or feature uniqueness and specificity, i.e., wafer fault anomaly detection, pedestrian trajectory anomaly detection, etc. In this case, if the model performs the pre-training phase, it will be interfered by the pre-training parameters during the formal decision-making task, and the model performance will be reduced due to the large difference between the data features at the pre-training stage and actual tasks. Google Brain team has also explored this issue on the ImageNet dataset [158].