Page 7

## 3.2 Semi-supervised learning

Semi-supervised learning is suitable for training tasks with large amounts of data and high labeling costs. The usual approach of a semi-supervised paradigm is to train the deep learning network by labeling only a portion of training data. For anomaly detection tasks, semi-supervised learning methods typically involve training the model by adding only normal samples to the dataset. With partial data labeling, semi-supervised learning can usually achieve better performance than unsupervised learning, but the features extracted from latent layers may not be the actual anomalies and are prone to overfitting problems. Manolache et al. [43] proposed DATE, a Transformer-based model for text anomaly detection tasks. They tested both semi-supervised training and unsupervised training conditions in the experimental stage. Experimental results show that Transformer is highly generalizable and has excellent performance in a variety of test environments. Zhao et al. [44] proposed Trine, a system log anomaly detection method using a semi-supervised paradigm. Trine uses only normal samples for training, adopts minimax strategies to train generators and discriminators, and performs anomaly detection on system logs. VT-ADL [45] uses a similar paradigm for anomaly detection (image-level fine-grained) and anomaly location tasks (pixel-level fine-grained).

## 3.3 Unsupervised learning

For anomaly detection, unsupervised learning usually refers to the case where it is difficult to obtain labels for anomalous data. The core of an unsupervised anomaly detection task is to distinguish between normal samples and abnormal samples based on the latent space of feature distribution. In this case, there is an underlying assumption that normal cases occur more frequently than anomalies, otherwise the model would have a high false positive rate. Therefore, in a sense, the unbalanced distribution of data can be considered a prerequisite for unsupervised anomaly detection. At present, the majority of studies exploring the application of Transformer in anomaly detection predominantly revolve around unsupervised models, with a primary emphasis on tackling binary classification problems. There is little research on using unsupervised methods to perform multi-class classification anomaly detection tasks. However, unsupervised paradigms are very sensitive to abnormal data noise,

and face the challenge of learning the inherent commonalities of data in a complex high-dimensional space. Their performance is also inferior to supervised and semisupervised methods. Therefore, for unsupervised learning methods, the priority is to ensure the stability of network training and to obtain higher anomaly detection accuracy.