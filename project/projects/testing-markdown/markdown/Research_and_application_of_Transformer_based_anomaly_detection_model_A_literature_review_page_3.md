Page 3

With the diversification of anomaly detection application domains, variants of mainstream neural networks have been proposed to achieve better performance in domain-specific anomaly detection tasks. However, current datasets tend to have quantities of positive samples (normal samples) and a very small number of negative samples (anomalous samples), resulting in serious data imbalance [11]. This also leads to the fact that most methods based on neural networks and deep learning adopt unsupervised learning approaches for model training. Therefore, before Transformer was applied to the anomaly detection task, the industry generally considered Variational Autoencoder (VAE) [12] and GAN, which are capable of distribution fitting, were the best methods for anomaly detection. VAE is used to minimize the log-likelihood lower bound on the data, while GAN is to achieve a balance between the generator and the discriminator. Although the GAN network [10] has shown superior performance in the task of anomaly detection, it also suffers from training instability. (i.e., difficulty in entering Nash equilibrium) To this end, many variants of GAN-based neural networks have also been proposed, such as WGAN [13].

With Transformer [14] being proposed in 2017, its unique attention mechanism with excellent contextual feature extraction performance makes it not only competent for NLP tasks but also for a wide range of transfer learning tasks. Google released BERT [15] in 2019, an important variant of Transformer. BERT introduces Bi-directional operations based on Vanilla Transformer and uses the pre-training mechanism to further enhance model performance and reduce training costs. BERT model has also achieved state-of-the-art (SOTA) results in 11 NLP tasks such as Named Entity Recognition (NER), Machine Translation (MT), etc. Transformer essentially changes the linear operation structure of the previous RNNs (i.e. LSTM, GRU, etc.), and realizes parallel operation through pure attention mechanism, which greatly improves the operation efficiency and has further contextual memory ability.

Therefore, Transformer model is well suited for anomaly detection tasks with strong contextual information associations such as serialized data. In related studies, researchers have employed various approaches for anomaly detection. Some utilize the Vanilla Transformer model, while others directly adopt the BERT model [16]. Additionally, some researchers modify the Transformer structure, such as using Vision Transformer (ViT) [17], and others integrate Transformer with auxiliary methods, as seen in TranAD [18]. TranAD integrates the generative adversarial training approach from GAN into the Transformer model through refactoring and applying it to the multivariate time series data (MTS) anomaly detection tasks. This idea has also been adopted by Adformer[19].

In this comprehensive review, we will provide an in-depth analysis of the theoretical advancements and application progress of Transformers in anomaly detection tasks over the past seven years. Furthermore, we will identify the existing open issues, research gaps, and future trends in Transformer-based anomaly detection tasks. To the best of our knowledge, this will be the first work to comprehensively summarize the research progress of Transformers in the field of anomaly detection, offering valuable insights for future investigations. This helps the reader to gain key insights into relevant research and provides illuminating thoughts and genuine open opportunities on some of the difficulties and challenges in Transformer-based anomaly detection.

The rest of the paper is organized as follows: Section 2 provides a comprehensive overview and analysis on the existing definitions of anomaly detection concepts from various research sources. Section 3 delves into the interrelationships between different learning paradigms, offering a consolidated summary. Section 4 presents a systematic categorization and summary of the operational principles of Transformer-related models, along with their application perspectives in the realm of anomaly detection. Section 5 conducts an examination and classification of relevant research based on different datatypes. Section 6 critically evaluates the evaluation indexes and datasets employed in current studies, highlighting the strengths and weaknesses of existing index systems and widely adopted open-access datasets across diverse application scenarios. Section 7 outlines the overarching challenges of anomaly detection, specifically emphasizing the core issues that warrant consideration when utilizing Transformers, while also discussing future research trends within this domain. Section 8 provides a concise summary of our work.