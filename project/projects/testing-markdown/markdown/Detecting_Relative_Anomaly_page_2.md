Page 2

s ( x i , x j ) = exp( -d ( x i , x j ) 2 /γ ) , (1)

which is symmetric in its arguments. The parameter γ controls the degree of localization, meaning how far one observation can lie from another observation for the two to still be considered similar. When γ = ∞ , all observations are equally similar to x i , and when γ ↓ 0 only x i is similar to itself. More localization is needed when the data come from a complicated distribution. The resulting matrix of similarities, S , holds the edge weights in the similarity graph. In methods that apply the 'kernel trick,' such as the support vector machine and kernel principal components analysis, such a similarity matrix is called the kernel matrix.

Common choices for the distance between two real data points, d ( x i , x j ) , are Euclidean ( L 2 ) and Manhattan ( L 1 ) distance. Both of these distance measures assume that each dimension of the data has been appropriately normalized. Euclidean distance has the advantage of being rotation invariant, and the order of the resulting distances typically remains meaningful even in high dimension [5]. Furthermore, data points often approximately lie in a lower-dimensional subspace; then Euclidean distance calculations are effectively carried out in the lower-dimensional subspace. For very high-dimensional problems, Manhattan distance may be preferred over Euclidean distance [6]. However, if the data truly cover the high-dimensional space, that means that the system components are barely correlated, even after feature selection and feature engineering. Then a multivariate anomaly analysis may add only little value as compared to running separate univariate analyses. If variables are measured on a nominal or ordinal scale, they may be converted into numerical data using dummy variables, or specialized distance measures for that scale level can be used; for a reference, see [7, Chapter 14].

B. A random walk approach, its relationships with other methods, and problems in the presence of frequently occurring anomalies

The approach of [2] proposes to take a random walk on the similarity graph, and to label an observation as anomalous when the stationary probability of the random walk at that observation is low. For cases where the similarity matrix is not irreducible and aperiodic, random restarts are introduced in the random walk, like it was proposed as part of the PageRank algorithm [3]. In the case that the similarity matrix is irreducible and aperiodic, which we will assume in to following to keep technical discussions at a minimum, the matrix of transition probabilities in the graph is simply the similarity matrix normalized by row,

P = [diag( S1 )] -1 S , (2)

where 1 is a column vector of ones. The vector of stationary (unnormalized) probabilities, p , follows from the stationarity condition P T p = p as the dominant lefteigenvector of P by the Perron-Frobenius theorem; see [8] for a reference.

We now show that the approach of [2] is closely related to both a density-based and a distance-based approach to anomaly detection. To see the connection with density-based anomaly detection, consider the case when S is symmetric; then the dominant left-eigenvector of P = [diag( S1 )] -1 S is, up to scaling, S1 . This follows from plugging in S1 for p in P T p = p , and using that P T = S T [diag( S1 )] -1 , with S T = S , which yields the true statement S [diag( S1 )] -1 S1 = S1 . We see that the stationary probability at observation x i is proportional to its (weighted) vertex degree VD( x i ) in the similarity graph, where

VD( x i ) · · = ( S1 ) i = n ∑ j =1 s ( x i , x j ) . (3)

Expression (3) is proportional to a kernel density estimate with Gaussian kernel, whose kernel covariance matrix is diagonal with all diagonal elements equaling γ/ 2 . As a density estimate, VD( x i ) is typically misspecified, because the kernel matrix is not tuned to fit the particular data generating process. This may actually be desired in anomaly detection problems where a low density observation close to a very typical system state does not make for an interesting anomaly. However, the close connection with kernel density estimation suggests that if anomalous system states occur too frequently, they may not be labeled correctly as anomalies, even if they are for from the most typical system states.

To also see the connection with distance-based anomaly detection, consider a directed k nearest neighbor graph instead of a fully connected similarity graph. Here the ( i, j ) th element of S takes value s ( x i , x j ) if x j is in the set N k ( x i ) , which contains the k nearest neighbors of x i , and it is zero otherwise. The resulting similarity matrix is a sparse approximation of the full similarity matrix. The additional tuning parameter k controls the degree of localization. Localization via the k nearest neighbor graph is also used in spectral clustering, manifold learning, and local multidimensional scaling; for a reference, see [7, Chapter 14]. Consider a linear expansion of the radial kernel function, defined in Equation (1), around some distance level v > 0 . Then VD( x i ) is approximately an affine decreasing function of the average distance to the k nearest neighbors:

VD approx ( x i ) = k exp( -v 2 /γ )(1 + 2 v 2 /γ ) (4) -2( v exp( -v 2 /γ )) /γ ∑ j : x j ∈N k ( x i ) d ( x i , x j ) .

This effectively eliminates the dependency on the kernel parameter γ . Using the average distance to the k nearest neighbors as a measure of anomaly was suggested in both [9] and [10]. However, for relative anomalies, the average distance to the k nearest neighbors can be small, and what is an anomalous system state may not be considered anomalous by the anomaly detection model.