Page 7

As labeled anomalies are difficult to obtain due to their rareness and unknowingness, in both settings we use only very limited labeled anomalies, i.e ., with the number of the given anomaly examples respectively fixed to one and ten.

Table 1. AUC results (mean±std) on nine real-world AD datasets under the general setting. The first 15 datasets are data subsets of MVTec AD whose results are the averaged results over these subsets. The supervised methods are trained using one or ten random anomaly examples, with the best results in red and the second-best in blue . KDAD is treated as a baseline. | C | is the number of anomaly classes.

| Dataset      | | C |   | Baseline    | One Training Anomaly Example   | One Training Anomaly Example   | One Training Anomaly Example   | One Training Anomaly Example   | One Training Anomaly Example   | Ten Training Anomaly Examples   | Ten Training Anomaly Examples   | Ten Training Anomaly Examples   | Ten Training Anomaly Examples   | Ten Training Anomaly Examples   |
|--------------|---------|-------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|
|              |         | KDAD        | DevNet                         | FLOS                           | SAOE                           | MLEP                           | DRA (Ours)                     | DevNet                          | FLOS                            | SAOE                            | MLEP                            | DRA (Ours)                      |
| Carpet       | 5       | 0.774±0.005 | 0.746±0.076                    | 0.755±0.026                    | 0.766 ±0.098                   | 0.701±0.091                    | 0.859 ±0.023                   | 0.867 ±0.040                    | 0.780±0.009                     | 0.755±0.136                     | 0.781±0.049                     | 0.940 ±0.027                    |
| Grid         | 5       | 0.749±0.017 | 0.891±0.040                    | 0.871±0.076                    | 0.921 ±0.032                   | 0.839±0.028                    | 0.972 ±0.011                   | 0.967±0.021                     | 0.966±0.005                     | 0.952±0.011                     | 0.980 ±0.009                    | 0.987 ±0.009                    |
| Leather      | 5       | 0.948±0.005 | 0.873±0.026                    | 0.791±0.057                    | 0.996 ±0.007                   | 0.781±0.020                    | 0.989 ±0.005                   | 0.999 ±0.001                    | 0.993±0.004                     | 1.000 ±0.000                    | 0.813±0.158                     | 1.000 ±0.000                    |
| Tile         | 5       | 0.911±0.010 | 0.752±0.038                    | 0.787±0.038                    | 0.935 ±0.034                   | 0.927±0.036                    | 0.965 ±0.015                   | 0.987±0.005                     | 0.952±0.010                     | 0.944±0.013                     | 0.988 ±0.009                    | 0.994 ±0.006                    |
| Wood         | 5       | 0.940±0.004 | 0.900±0.068                    | 0.927±0.065                    | 0.948 ±0.009                   | 0.660±0.142                    | 0.985 ±0.011                   | 0.999 ±0.001                    | 1.000 ±0.000                    | 0.976±0.031                     | 0.999 ±0.002                    | 0.998±0.001                     |
| Bottle       | 3       | 0.992±0.002 | 0.976±0.006                    | 0.975±0.023                    | 0.989 ±0.019                   | 0.927±0.090                    | 1.000 ±0.000                   | 0.993±0.008                     | 0.995±0.002                     | 0.998 ±0.003                    | 0.981±0.004                     | 1.000 ±0.000                    |
| Capsule      | 5       | 0.775±0.019 | 0.564±0.032                    | 0.666 ±0.020                   | 0.611±0.109                    | 0.558±0.075                    | 0.631 ±0.056                   | 0.865±0.057                     | 0.902 ±0.017                    | 0.850±0.054                     | 0.818±0.063                     | 0.935 ±0.022                    |
| Pill         | 7       | 0.824±0.006 | 0.769 ±0.017                   | 0.745±0.064                    | 0.652±0.078                    | 0.656±0.061                    | 0.832 ±0.034                   | 0.866±0.038                     | 0.929 ±0.012                    | 0.872±0.049                     | 0.845±0.048                     | 0.904 ±0.024                    |
| Transistor   | 4       | 0.805±0.013 | 0.722 ±0.032                   | 0.709 ±0.041                   | 0.680±0.182                    | 0.695±0.124                    | 0.668±0.068                    | 0.924 ±0.027                    | 0.862±0.037                     | 0.860±0.053                     | 0.927 ±0.043                    | 0.915±0.025                     |
| Zipper       | 7       | 0.927±0.018 | 0.922±0.018                    | 0.885±0.033                    | 0.970 ±0.033                   | 0.856±0.086                    | 0.984 ±0.016                   | 0.990±0.009                     | 0.990±0.008                     | 0.995 ±0.004                    | 0.965±0.002                     | 1.000 ±0.000                    |
| Cable        | 8       | 0.880±0.002 | 0.783±0.058                    | 0.790±0.039                    | 0.819 ±0.060                   | 0.688±0.017                    | 0.876 ±0.012                   | 0.892 ±0.020                    | 0.890±0.063                     | 0.862±0.022                     | 0.857±0.062                     | 0.909 ±0.011                    |
| Hazelnut     | 4       | 0.984±0.001 | 0.979 ±0.010                   | 0.976±0.021                    | 0.961±0.042                    | 0.704±0.090                    | 0.977 ±0.030                   | 1.000 ±0.000                    | 1.000 ±0.000                    | 1.000 ±0.000                    | 1.000 ±0.000                    | 1.000 ±0.000                    |
| Metal nut    | 4       | 0.743±0.013 | 0.876±0.007                    | 0.930 ±0.022                   | 0.922±0.033                    | 0.878±0.038                    | 0.948 ±0.046                   | 0.991 ±0.006                    | 0.984±0.004                     | 0.976±0.013                     | 0.974±0.009                     | 0.997 ±0.002                    |
| Screw        | 5       | 0.805±0.021 | 0.399±0.187                    | 0.337±0.091                    | 0.653±0.074                    | 0.675 ±0.294                   | 0.903 ±0.064                   | 0.970±0.015                     | 0.940±0.017                     | 0.975 ±0.023                    | 0.899±0.039                     | 0.977 ±0.009                    |
| Toothbrush   | 1       | 0.863±0.029 | 0.753 ±0.027                   | 0.731 ±0.028                   | 0.686±0.110                    | 0.617±0.058                    | 0.650±0.029                    | 0.860±0.066                     | 0.900 ±0.008                    | 0.865 ±0.062                    | 0.783±0.048                     | 0.826±0.021                     |
| MVTec AD     | -       | 0.861±0.009 | 0.794±0.014                    | 0.792±0.014                    | 0.834 ±0.007                   | 0.744±0.019                    | 0.883 ±0.008                   | 0.945 ±0.004                    | 0.939±0.007                     | 0.926±0.010                     | 0.907±0.005                     | 0.959 ±0.003                    |
| AITEX        | 12      | 0.576±0.002 | 0.598±0.070                    | 0.538±0.073                    | 0.675 ±0.094                   | 0.564±0.055                    | 0.692 ±0.124                   | 0.887 ±0.013                    | 0.841±0.049                     | 0.874±0.024                     | 0.867±0.037                     | 0.893 ±0.017                    |
| SDD          | 1       | 0.888±0.005 | 0.881 ±0.009                   | 0.840±0.043                    | 0.781±0.009                    | 0.811±0.045                    | 0.859 ±0.014                   | 0.988 ±0.006                    | 0.967±0.018                     | 0.955±0.020                     | 0.983±0.013                     | 0.991 ±0.005                    |
| ELPV         | 2       | 0.744±0.001 | 0.514±0.076                    | 0.457±0.056                    | 0.635 ±0.092                   | 0.578±0.062                    | 0.675 ±0.024                   | 0.846 ±0.022                    | 0.818±0.032                     | 0.793±0.047                     | 0.794±0.047                     | 0.845 ±0.013                    |
| Optical      | 1       | 0.579±0.002 | 0.523±0.003                    | 0.518±0.003                    | 0.815 ±0.014                   | 0.516±0.009                    | 0.888 ±0.012                   | 0.782±0.065                     | 0.720±0.055                     | 0.941 ±0.013                    | 0.740±0.039                     | 0.965 ±0.006                    |
| Mastcam      | 11      | 0.642±0.007 | 0.595±0.016                    | 0.542±0.017                    | 0.662 ±0.018                   | 0.625±0.045                    | 0.692 ±0.058                   | 0.790±0.021                     | 0.703±0.029                     | 0.810 ±0.029                    | 0.798±0.026                     | 0.848 ±0.008                    |
| BrainMRI     | 1       | 0.733±0.016 | 0.694 ±0.004                   | 0.693±0.036                    | 0.531±0.060                    | 0.632±0.017                    | 0.744 ±0.004                   | 0.958±0.012                     | 0.955±0.011                     | 0.900±0.041                     | 0.959 ±0.011                    | 0.970 ±0.003                    |
| HeadCT       | 1       | 0.793±0.017 | 0.742±0.076                    | 0.698±0.092                    | 0.597±0.022                    | 0.758 ±0.038                   | 0.796 ±0.105                   | 0.982 ±0.009                    | 0.971±0.004                     | 0.935±0.021                     | 0.972 ±0.014                    | 0.972 ±0.002                    |
| Hyper-Kvasir | 4       | 0.401±0.002 | 0.653±0.037                    | 0.668 ±0.004                   | 0.498±0.100                    | 0.445±0.040                    | 0.690 ±0.017                   | 0.829 ±0.018                    | 0.773±0.029                     | 0.666±0.050                     | 0.600±0.069                     | 0.834 ±0.004                    |

The popular performance metric, Area Under ROC Curve (AUC), is used. Each model yields an anomaly ranking, and its AUC is calculated based on the ranking. All reported AUCs are averaged results over three independent runs.

## 4.3. Results under the General Setting

Tab. 7 shows the comparison results under the general setting protocol. Below we discuss the results in details.

Application Domain Perspective . Despite the datasets from diverse application domains, including industrial defect inspection, rover-based planetary exploration and medical image analysis, our model achieves the best AUC performance on across nearly all of the datasets, i.e ., eight (seven) out of nine datasets in the one-shot (ten-shot) setting, with the second-best results on the other datasets. On challenging datasets, such as MVTec AD, AITEX, Mastcam and Hyper-Kvasir, where a larger number of possible anomaly classes is presented, our model obtains consistently better AUC results, increasing by up to 5% AUC.

Sample Efficiency . The reduction of training anomaly examples generally decreases the performance of all the supervised models. Compared to the competing detectors, our model shows better sample efficiency in that i) with reduced anomaly examples, our model has a much smaller decrease of AUC, i.e ., an average of 15.1% AUC decrease across the nine datasets, which is much better than DevNet (22.3%), FLOS (21.6%), SAOE (19.7%), and MLEP (21.6%), and ii) our model trained with one anomaly example can largely outperform the strong competing methods trained with ten anomaly examples, such as DevNet, FLOS and MLEP on Optical, and SAOE and MLEP on Hyper-Kvasir.

Comparison to Unsupervised Baseline . Compared to

the unsupervised model KDAD, our model and other supervised models demonstrate consistently better performance when using ten training anomaly examples ( i.e ., less openset scenarios). In more open-set scenarios where only one anomaly example is used, our method is the only model that is still clearly better than KDAD on most datasets, even on challenging datasets which have many anomaly classes, such as MVTec AD, AITEX, and Mastcam.

## 4.4. Results under the Hard Setting